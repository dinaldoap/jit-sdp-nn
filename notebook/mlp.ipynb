{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JIT-SDP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dinaldoap/jit-sdp-nn/blob/master/notebook/mlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXwvNEAQE6NO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import random\n",
        "from scipy.stats import mstats"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAupgTDfmJ_I",
        "colab_type": "code",
        "outputId": "af855e7f-78c1-4beb-c508-672aa7668380",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/dinaldoap/jit-sdp-data/master/jenkins.csv')\n",
        "df.head()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fix</th>\n",
              "      <th>ns</th>\n",
              "      <th>nd</th>\n",
              "      <th>nf</th>\n",
              "      <th>entropy</th>\n",
              "      <th>la</th>\n",
              "      <th>ld</th>\n",
              "      <th>lt</th>\n",
              "      <th>ndev</th>\n",
              "      <th>age</th>\n",
              "      <th>nuc</th>\n",
              "      <th>exp</th>\n",
              "      <th>rexp</th>\n",
              "      <th>sexp</th>\n",
              "      <th>author_date_unix_timestamp</th>\n",
              "      <th>classification</th>\n",
              "      <th>contains_bug</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>False</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.641604</td>\n",
              "      <td>9.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>426.428571</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.000093</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5171.0</td>\n",
              "      <td>30.227271</td>\n",
              "      <td>1472.714286</td>\n",
              "      <td>1555326371</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>False</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.750000</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>426.428571</td>\n",
              "      <td>100.0</td>\n",
              "      <td>6.314775</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5170.0</td>\n",
              "      <td>29.227271</td>\n",
              "      <td>1471.714286</td>\n",
              "      <td>1555326363</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>False</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.906580</td>\n",
              "      <td>15.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>96.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.034722</td>\n",
              "      <td>2.0</td>\n",
              "      <td>629.0</td>\n",
              "      <td>14.828373</td>\n",
              "      <td>414.000000</td>\n",
              "      <td>1554971763</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>False</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.058824</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1554969774</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>False</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.662506</td>\n",
              "      <td>14.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>67.000000</td>\n",
              "      <td>6.0</td>\n",
              "      <td>21.280683</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.058824</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1554967752</td>\n",
              "      <td>Feature Addition</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     fix   ns   nd  ...  author_date_unix_timestamp    classification  contains_bug\n",
              "0  False  7.0  7.0  ...                  1555326371              None         False\n",
              "1  False  7.0  7.0  ...                  1555326363              None         False\n",
              "2  False  1.0  1.0  ...                  1554971763              None         False\n",
              "3  False  1.0  1.0  ...                  1554969774              None         False\n",
              "4  False  1.0  2.0  ...                  1554967752  Feature Addition         False\n",
              "\n",
              "[5 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alizaxdPD3El",
        "colab_type": "code",
        "outputId": "914403b2-3858-4e82-add8-f1b99be2118f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "df = df.sample(frac=1)\n",
        "label_col = 'contains_bug'\n",
        "features_cols = ['fix', 'ns', 'nd', 'nf', 'entropy', 'la', 'ld', 'lt', 'ndev', 'age', 'nuc', 'exp', 'rexp', 'sexp', 'classification']\n",
        "x = df[features_cols]\n",
        "x['fix'] = x['fix'].astype('int')\n",
        "df_classification = pd.get_dummies(x, columns=['classification'])\n",
        "x = pd.concat([x, df_classification], axis='columns')\n",
        "x = x.drop(['classification'], axis='columns')\n",
        "x = x.values\n",
        "y = df[label_col]\n",
        "y = y.astype('category')\n",
        "y = y.cat.codes\n",
        "y = y.values"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBD1-tp2ExzQ",
        "colab_type": "code",
        "outputId": "198f1e33-10c6-4014-b620-f92ba798a4bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "print(x[:3])\n",
        "print(y[:3])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 1.00000000e+00 1.14300000e+03\n",
            "  2.90000000e+01 2.23151620e+00 1.00000000e+00 1.35000000e+02\n",
            "  1.53895529e+01 9.00000000e+01 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.14300000e+03 2.90000000e+01 2.23151620e+00\n",
            "  1.00000000e+00 1.35000000e+02 1.53895529e+01 9.00000000e+01\n",
            "  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 3.00000000e+00 3.00000000e+00\n",
            "  1.55665671e+00 4.00000000e+00 3.00000000e+00 3.90666667e+02\n",
            "  6.00000000e+00 1.75112037e+01 2.00000000e+00 3.00000000e+00\n",
            "  1.33333333e+00 2.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  3.00000000e+00 3.00000000e+00 1.55665671e+00 4.00000000e+00\n",
            "  3.00000000e+00 3.90666667e+02 6.00000000e+00 1.75112037e+01\n",
            "  2.00000000e+00 3.00000000e+00 1.33333333e+00 2.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 2.00000000e+00 2.00000000e+00 2.00000000e+00\n",
            "  6.19382195e-01 8.00000000e+00 5.00000000e+00 1.78050000e+03\n",
            "  3.50000000e+01 4.74590278e+00 2.00000000e+00 1.90000000e+01\n",
            "  1.80000000e+01 9.50000000e+00 0.00000000e+00 2.00000000e+00\n",
            "  2.00000000e+00 2.00000000e+00 6.19382195e-01 8.00000000e+00\n",
            "  5.00000000e+00 1.78050000e+03 3.50000000e+01 4.74590278e+00\n",
            "  2.00000000e+00 1.90000000e+01 1.80000000e+01 9.50000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "[0 1 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwCn1uNTMjfG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_index = int( len(x) * 0.8 )\n",
        "test_index = int( len(x) * 0.9 )\n",
        "x_train, y_train = x[:val_index], y[:val_index]\n",
        "x_val, y_val = x[val_index:test_index], y[val_index:test_index]\n",
        "x_test, y_test = x[test_index:], y[test_index:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSqnoapE5Poq",
        "colab_type": "code",
        "outputId": "3464300a-9bcd-4d48-c604-688ed303b530",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(x_train[:3])\n",
        "print(x_val[:3])\n",
        "print(x_test[:3])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 1.00000000e+00 1.14300000e+03\n",
            "  2.90000000e+01 2.23151620e+00 1.00000000e+00 1.35000000e+02\n",
            "  1.53895529e+01 9.00000000e+01 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.14300000e+03 2.90000000e+01 2.23151620e+00\n",
            "  1.00000000e+00 1.35000000e+02 1.53895529e+01 9.00000000e+01\n",
            "  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 3.00000000e+00 3.00000000e+00\n",
            "  1.55665671e+00 4.00000000e+00 3.00000000e+00 3.90666667e+02\n",
            "  6.00000000e+00 1.75112037e+01 2.00000000e+00 3.00000000e+00\n",
            "  1.33333333e+00 2.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  3.00000000e+00 3.00000000e+00 1.55665671e+00 4.00000000e+00\n",
            "  3.00000000e+00 3.90666667e+02 6.00000000e+00 1.75112037e+01\n",
            "  2.00000000e+00 3.00000000e+00 1.33333333e+00 2.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 2.00000000e+00 2.00000000e+00 2.00000000e+00\n",
            "  6.19382195e-01 8.00000000e+00 5.00000000e+00 1.78050000e+03\n",
            "  3.50000000e+01 4.74590278e+00 2.00000000e+00 1.90000000e+01\n",
            "  1.80000000e+01 9.50000000e+00 0.00000000e+00 2.00000000e+00\n",
            "  2.00000000e+00 2.00000000e+00 6.19382195e-01 8.00000000e+00\n",
            "  5.00000000e+00 1.78050000e+03 3.50000000e+01 4.74590278e+00\n",
            "  2.00000000e+00 1.90000000e+01 1.80000000e+01 9.50000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "[[  1.           1.           1.           1.           0.\n",
            "   20.          23.         736.           2.           1.52216435\n",
            "    1.          29.           3.32140629   6.           1.\n",
            "    1.           1.           1.           0.          20.\n",
            "   23.         736.           2.           1.52216435   1.\n",
            "   29.           3.32140629   6.           1.           0.\n",
            "    0.           0.           0.           0.        ]\n",
            " [  0.           1.           7.          14.           3.5136063\n",
            "  241.          13.          15.07142857   3.          13.40131118\n",
            "    2.         216.          11.73830683 158.           0.\n",
            "    1.           7.          14.           3.5136063  241.\n",
            "   13.          15.07142857   3.          13.40131118   2.\n",
            "  216.          11.73830683 158.           0.           1.\n",
            "    0.           0.           0.           0.        ]\n",
            " [  0.           1.           1.           1.           0.\n",
            "    1.           1.         282.           1.           4.93840278\n",
            "    1.         182.          49.82305466 146.           0.\n",
            "    1.           1.           1.           0.           1.\n",
            "    1.         282.           1.           4.93840278   1.\n",
            "  182.          49.82305466 146.           0.           0.\n",
            "    0.           1.           0.           0.        ]]\n",
            "[[0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 2.00000000e+00 1.00000000e+00 2.04000000e+02\n",
            "  3.00000000e+00 2.14729236e+02 1.00000000e+00 4.20000000e+01\n",
            "  1.97798946e+00 3.40000000e+01 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 2.00000000e+00\n",
            "  1.00000000e+00 2.04000000e+02 3.00000000e+00 2.14729236e+02\n",
            "  1.00000000e+00 4.20000000e+01 1.97798946e+00 3.40000000e+01\n",
            "  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 1.20000000e+01 1.00000000e+00 5.80000000e+01\n",
            "  1.00000000e+00 2.48842593e-03 1.00000000e+00 9.00000000e+02\n",
            "  9.40811490e+01 5.15000000e+02 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 1.20000000e+01\n",
            "  1.00000000e+00 5.80000000e+01 1.00000000e+00 2.48842593e-03\n",
            "  1.00000000e+00 9.00000000e+02 9.40811490e+01 5.15000000e+02\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 2.00000000e+00 6.00000000e+00 9.00000000e+00\n",
            "  1.19727526e+00 1.05200000e+03 8.91000000e+02 5.56222222e+02\n",
            "  1.20000000e+01 4.57037590e+01 8.00000000e+00 5.70600000e+03\n",
            "  1.85269793e+02 2.58850000e+03 0.00000000e+00 2.00000000e+00\n",
            "  6.00000000e+00 9.00000000e+00 1.19727526e+00 1.05200000e+03\n",
            "  8.91000000e+02 5.56222222e+02 1.20000000e+01 4.57037590e+01\n",
            "  8.00000000e+00 5.70600000e+03 1.85269793e+02 2.58850000e+03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46STsbti3AmT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean_train = x_train.mean(axis=0)\n",
        "std_train = x_train.std(axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5vvbrrK3PES",
        "colab_type": "code",
        "outputId": "bb2651f8-c9c7-4a4b-b2ce-7cca56bbb87e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "print(mean_train)\n",
        "print(std_train)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2.80165503e-01 1.60000000e+00 2.49252651e+00 3.93167830e+00\n",
            " 7.29343523e-01 5.36284458e+01 2.70048616e+01 7.15818862e+02\n",
            " 1.90462891e+01 8.30126323e+01 1.91642100e+00 1.73230111e+03\n",
            " 8.50179057e+01 8.93732072e+02 2.80165503e-01 1.60000000e+00\n",
            " 2.49252651e+00 3.93167830e+00 7.29343523e-01 5.36284458e+01\n",
            " 2.70048616e+01 7.15818862e+02 1.90462891e+01 8.30126323e+01\n",
            " 1.91642100e+00 1.73230111e+03 8.50179057e+01 8.93732072e+02\n",
            " 2.80165503e-01 1.57124386e-01 3.53245410e-02 4.76803724e-01\n",
            " 1.56710628e-02 3.49107836e-02]\n",
            "[4.49079942e-01 1.63179553e+00 6.18039590e+00 2.46665298e+01\n",
            " 1.11449687e+00 7.35022221e+02 6.10443860e+02 1.07988283e+03\n",
            " 2.67969402e+01 2.14765419e+02 7.35582372e+00 1.97484705e+03\n",
            " 8.20515477e+01 1.29884812e+03 4.49079942e-01 1.63179553e+00\n",
            " 6.18039590e+00 2.46665298e+01 1.11449687e+00 7.35022221e+02\n",
            " 6.10443860e+02 1.07988283e+03 2.67969402e+01 2.14765419e+02\n",
            " 7.35582372e+00 1.97484705e+03 8.20515477e+01 1.29884812e+03\n",
            " 4.49079942e-01 3.63918003e-01 1.84598802e-01 4.99461643e-01\n",
            " 1.24199358e-01 1.83553863e-01]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2IoFA5h425Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = (x_train - mean_train) / std_train\n",
        "x_val = (x_val - mean_train) / std_train\n",
        "x_test = (x_test - mean_train) / std_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1AoKNoU5V1w",
        "colab_type": "code",
        "outputId": "3e129686-553b-4838-adfd-7fa03040a766",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        }
      },
      "source": [
        "print(x_train[:3])\n",
        "print(x_val[:3])\n",
        "print(x_test[:3])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.62386555 -0.36769313 -0.24149367 -0.11885248 -0.65441505 -0.07160116\n",
            "  -0.04259992  0.39558101  0.37144953 -0.37613651 -0.12458442 -0.8088227\n",
            "  -0.8485928  -0.61880374 -0.62386555 -0.36769313 -0.24149367 -0.11885248\n",
            "  -0.65441505 -0.07160116 -0.04259992  0.39558101  0.37144953 -0.37613651\n",
            "  -0.12458442 -0.8088227  -0.8485928  -0.61880374 -0.62386555  2.31611409\n",
            "  -0.19135845 -0.95463532 -0.12617668 -0.19019367]\n",
            " [-0.62386555 -0.36769313  0.08211019 -0.03777095  0.74231988 -0.06751965\n",
            "  -0.03932362 -0.30109951 -0.48685742 -0.30499057  0.01136229 -0.87566331\n",
            "  -1.01990242 -0.68655608 -0.62386555 -0.36769313  0.08211019 -0.03777095\n",
            "   0.74231988 -0.06751965 -0.03932362 -0.30109951 -0.48685742 -0.30499057\n",
            "   0.01136229 -0.87566331 -1.01990242 -0.68655608 -0.62386555 -0.43175766\n",
            "  -0.19135845  1.04752043 -0.12617668 -0.19019367]\n",
            " [-0.62386555  0.24512875 -0.07969174 -0.07831172 -0.09866455 -0.06207764\n",
            "  -0.03604731  0.98592284  0.59535569 -0.36442892  0.01136229 -0.86756142\n",
            "  -0.81677808 -0.68078173 -0.62386555  0.24512875 -0.07969174 -0.07831172\n",
            "  -0.09866455 -0.06207764 -0.03604731  0.98592284  0.59535569 -0.36442892\n",
            "   0.01136229 -0.86756142 -0.81677808 -0.68078173 -0.62386555 -0.43175766\n",
            "  -0.19135845  1.04752043 -0.12617668 -0.19019367]]\n",
            "[[ 1.60290948 -0.36769313 -0.24149367 -0.11885248 -0.65441505 -0.0457516\n",
            "  -0.00656057  0.01868827 -0.63612819 -0.37943943 -0.12458442 -0.86249774\n",
            "  -0.99567286 -0.68347642  1.60290948 -0.36769313 -0.24149367 -0.11885248\n",
            "  -0.65441505 -0.0457516  -0.00656057  0.01868827 -0.63612819 -0.37943943\n",
            "  -0.12458442 -0.86249774 -0.99567286 -0.68347642  1.60290948 -0.43175766\n",
            "  -0.19135845 -0.95463532 -0.12617668 -0.19019367]\n",
            " [-0.62386555 -0.36769313  0.72931792  0.40817747  2.49822396  0.25491958\n",
            "  -0.0229421  -0.64891062 -0.5988105  -0.32412723  0.01136229 -0.76780686\n",
            "  -0.89309222 -0.56644966 -0.62386555 -0.36769313  0.72931792  0.40817747\n",
            "   2.49822396  0.25491958 -0.0229421  -0.64891062 -0.5988105  -0.32412723\n",
            "   0.01136229 -0.76780686 -0.89309222 -0.56644966 -0.62386555  2.31611409\n",
            "  -0.19135845 -0.95463532 -0.12617668 -0.19019367]\n",
            " [-0.62386555 -0.36769313 -0.24149367 -0.11885248 -0.65441505 -0.07160116\n",
            "  -0.04259992 -0.40172772 -0.67344588 -0.36353259 -0.12458442 -0.78502338\n",
            "  -0.42893586 -0.57568861 -0.62386555 -0.36769313 -0.24149367 -0.11885248\n",
            "  -0.65441505 -0.07160116 -0.04259992 -0.40172772 -0.67344588 -0.36353259\n",
            "  -0.12458442 -0.78502338 -0.42893586 -0.57568861 -0.62386555 -0.43175766\n",
            "  -0.19135845  1.04752043 -0.12617668 -0.19019367]]\n",
            "[[-0.62386555 -0.36769313 -0.24149367 -0.11885248 -0.65441505 -0.07024066\n",
            "  -0.04259992 -0.47395778 -0.5988105   0.61330453 -0.12458442 -0.85591495\n",
            "  -1.0120457  -0.66191886 -0.62386555 -0.36769313 -0.24149367 -0.11885248\n",
            "  -0.65441505 -0.07024066 -0.04259992 -0.47395778 -0.5988105   0.61330453\n",
            "  -0.12458442 -0.85591495 -1.0120457  -0.66191886 -0.62386555  2.31611409\n",
            "  -0.19135845 -0.95463532 -0.12617668 -0.19019367]\n",
            " [-0.62386555 -0.36769313 -0.24149367 -0.11885248 -0.65441505 -0.05663563\n",
            "  -0.04259992 -0.60915763 -0.67344588 -0.38651541 -0.12458442 -0.42145092\n",
            "   0.11045792 -0.29159073 -0.62386555 -0.36769313 -0.24149367 -0.11885248\n",
            "  -0.65441505 -0.05663563 -0.04259992 -0.60915763 -0.67344588 -0.38651541\n",
            "  -0.12458442 -0.42145092  0.11045792 -0.29159073 -0.62386555 -0.43175766\n",
            "   5.22579479 -0.95463532 -0.12617668 -0.19019367]\n",
            " [-0.62386555  0.24512875  0.56751599  0.20547364  0.41985917  1.35828758\n",
            "   1.4153556  -0.1477907  -0.26295126 -0.17371918  0.82704252  2.01215527\n",
            "   1.22181592  1.30482379 -0.62386555  0.24512875  0.56751599  0.20547364\n",
            "   0.41985917  1.35828758  1.4153556  -0.1477907  -0.26295126 -0.17371918\n",
            "   0.82704252  2.01215527  1.22181592  1.30482379 -0.62386555 -0.43175766\n",
            "  -0.19135845  1.04752043 -0.12617668 -0.19019367]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNcuSWBQp2Ua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_sampler(y):\n",
        "  _, counts = np.unique(y, return_counts=True)\n",
        "  n_samples = len(y)\n",
        "  class_weights = n_samples / counts\n",
        "  weights = class_weights[y]\n",
        "  return data.WeightedRandomSampler(weights=weights, num_samples=n_samples, replacement=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fh6O6Nz6uC51",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sampler = create_sampler(y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iP6L6GN22_bR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, y_train = torch.from_numpy(x_train), torch.from_numpy(y_train)\n",
        "x_val, y_val = torch.from_numpy(x_val), torch.from_numpy(y_val)\n",
        "x_test, y_test = torch.from_numpy(x_test), torch.from_numpy(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XnnrzRjL3IP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = data.TensorDataset(x_train, y_train)\n",
        "val_dataset = data.TensorDataset(x_val, y_val)\n",
        "test_dataset = data.TensorDataset(x_test, y_test)\n",
        "\n",
        "train_dataloader = data.DataLoader(train_dataset, batch_size=512, sampler=sampler)\n",
        "val_dataloader = data.DataLoader(val_dataset, batch_size=32)\n",
        "test_dataloader = data.DataLoader(test_dataset, batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHoY9-JcORo-",
        "colab_type": "code",
        "outputId": "76a685cb-5b15-4321-fb12-46b39dbcad29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "source": [
        "inputs, targets = next(iter(train_dataloader))\n",
        "print(inputs[:3])\n",
        "print(targets[:3])\n",
        "\n",
        "inputs, targets = next(iter(val_dataloader))\n",
        "print(inputs[:3])\n",
        "print(targets[:3])\n",
        "\n",
        "inputs, targets = next(iter(test_dataloader))\n",
        "print(inputs[:3])\n",
        "print(targets[:3])"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 1.6029,  0.2451,  0.0821, -0.0378, -0.1389, -0.0077, -0.0426,  0.9299,\n",
            "          2.0507, -0.3655,  0.1473, -0.8645, -1.0287, -0.6819,  1.6029,  0.2451,\n",
            "          0.0821, -0.0378, -0.1389, -0.0077, -0.0426,  0.9299,  2.0507, -0.3655,\n",
            "          0.1473, -0.8645, -1.0287, -0.6819,  1.6029, -0.4318, -0.1914, -0.9546,\n",
            "         -0.1262, -0.1902],\n",
            "        [-0.6239, -0.3677, -0.2415, -0.1189, -0.6544, -0.0417, -0.0442, -0.6388,\n",
            "         -0.6734, -0.3865, -0.1246, -0.3865,  0.5921, -0.2577, -0.6239, -0.3677,\n",
            "         -0.2415, -0.1189, -0.6544, -0.0417, -0.0442, -0.6388, -0.6734, -0.3865,\n",
            "         -0.1246, -0.3865,  0.5921, -0.2577, -0.6239,  2.3161, -0.1914, -0.9546,\n",
            "         -0.1262, -0.1902],\n",
            "        [-0.6239, -0.3677, -0.0797, -0.0783,  0.2335,  0.0250, -0.0393, -0.3304,\n",
            "         -0.3003,  0.1343, -0.1246, -0.8012, -0.7033, -0.5972, -0.6239, -0.3677,\n",
            "         -0.0797, -0.0783,  0.2335,  0.0250, -0.0393, -0.3304, -0.3003,  0.1343,\n",
            "         -0.1246, -0.8012, -0.7033, -0.5972, -0.6239, -0.4318, -0.1914,  1.0475,\n",
            "         -0.1262, -0.1902]], dtype=torch.float64)\n",
            "tensor([0, 0, 1], dtype=torch.int8)\n",
            "tensor([[ 1.6029, -0.3677, -0.2415, -0.1189, -0.6544, -0.0458, -0.0066,  0.0187,\n",
            "         -0.6361, -0.3794, -0.1246, -0.8625, -0.9957, -0.6835,  1.6029, -0.3677,\n",
            "         -0.2415, -0.1189, -0.6544, -0.0458, -0.0066,  0.0187, -0.6361, -0.3794,\n",
            "         -0.1246, -0.8625, -0.9957, -0.6835,  1.6029, -0.4318, -0.1914, -0.9546,\n",
            "         -0.1262, -0.1902],\n",
            "        [-0.6239, -0.3677,  0.7293,  0.4082,  2.4982,  0.2549, -0.0229, -0.6489,\n",
            "         -0.5988, -0.3241,  0.0114, -0.7678, -0.8931, -0.5664, -0.6239, -0.3677,\n",
            "          0.7293,  0.4082,  2.4982,  0.2549, -0.0229, -0.6489, -0.5988, -0.3241,\n",
            "          0.0114, -0.7678, -0.8931, -0.5664, -0.6239,  2.3161, -0.1914, -0.9546,\n",
            "         -0.1262, -0.1902],\n",
            "        [-0.6239, -0.3677, -0.2415, -0.1189, -0.6544, -0.0716, -0.0426, -0.4017,\n",
            "         -0.6734, -0.3635, -0.1246, -0.7850, -0.4289, -0.5757, -0.6239, -0.3677,\n",
            "         -0.2415, -0.1189, -0.6544, -0.0716, -0.0426, -0.4017, -0.6734, -0.3635,\n",
            "         -0.1246, -0.7850, -0.4289, -0.5757, -0.6239, -0.4318, -0.1914,  1.0475,\n",
            "         -0.1262, -0.1902]], dtype=torch.float64)\n",
            "tensor([0, 0, 0], dtype=torch.int8)\n",
            "tensor([[-0.6239, -0.3677, -0.2415, -0.1189, -0.6544, -0.0702, -0.0426, -0.4740,\n",
            "         -0.5988,  0.6133, -0.1246, -0.8559, -1.0120, -0.6619, -0.6239, -0.3677,\n",
            "         -0.2415, -0.1189, -0.6544, -0.0702, -0.0426, -0.4740, -0.5988,  0.6133,\n",
            "         -0.1246, -0.8559, -1.0120, -0.6619, -0.6239,  2.3161, -0.1914, -0.9546,\n",
            "         -0.1262, -0.1902],\n",
            "        [-0.6239, -0.3677, -0.2415, -0.1189, -0.6544, -0.0566, -0.0426, -0.6092,\n",
            "         -0.6734, -0.3865, -0.1246, -0.4215,  0.1105, -0.2916, -0.6239, -0.3677,\n",
            "         -0.2415, -0.1189, -0.6544, -0.0566, -0.0426, -0.6092, -0.6734, -0.3865,\n",
            "         -0.1246, -0.4215,  0.1105, -0.2916, -0.6239, -0.4318,  5.2258, -0.9546,\n",
            "         -0.1262, -0.1902],\n",
            "        [-0.6239,  0.2451,  0.5675,  0.2055,  0.4199,  1.3583,  1.4154, -0.1478,\n",
            "         -0.2630, -0.1737,  0.8270,  2.0122,  1.2218,  1.3048, -0.6239,  0.2451,\n",
            "          0.5675,  0.2055,  0.4199,  1.3583,  1.4154, -0.1478, -0.2630, -0.1737,\n",
            "          0.8270,  2.0122,  1.2218,  1.3048, -0.6239, -0.4318, -0.1914,  1.0475,\n",
            "         -0.1262, -0.1902]], dtype=torch.float64)\n",
            "tensor([0, 0, 1], dtype=torch.int8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-GctbL8E1oA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Classifier(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, drop_prob, epoch=None, val_loss=None):\n",
        "    super(Classifier, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.drop_prob = drop_prob\n",
        "    self.epoch = epoch\n",
        "    self.val_loss = val_loss\n",
        "    self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "    self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "    self.fcout = nn.Linear(hidden_size, 1)\n",
        "    self.dropout = nn.Dropout(drop_prob)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = torch.relu(self.fc1(x))\n",
        "    x = self.dropout(x)\n",
        "    x = torch.relu(self.fc2(x))\n",
        "    x = self.dropout(x)\n",
        "    x = torch.sigmoid(self.fcout(x))\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inpSePkGLTIs",
        "colab_type": "code",
        "outputId": "560b7f1c-a7f6-46ea-9219-0c41d84f5358",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "classifier = Classifier(input_size=x.shape[1], hidden_size=128, drop_prob=0.2)\n",
        "classifier"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Classifier(\n",
              "  (fc1): Linear(in_features=34, out_features=128, bias=True)\n",
              "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (fcout): Linear(in_features=128, out_features=1, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsT5eIrNoXAh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = 'classifier.cpt'\n",
        "def save_classifier(classifier):\n",
        "  checkpoint = {\n",
        "      'input_size': classifier.input_size,\n",
        "      'hidden_size': classifier.hidden_size,\n",
        "      'drop_prob': classifier.drop_prob,\n",
        "      'val_loss': classifier.val_loss,\n",
        "      'epoch': classifier.epoch,\n",
        "      'state_dict': classifier.state_dict()\n",
        "  }\n",
        "  with open(filename, 'wb') as f:\n",
        "    torch.save(checkpoint, f)\n",
        "\n",
        "def load_classifier():\n",
        "  with open(filename, 'rb') as f:\n",
        "    checkpoint = torch.load(f)\n",
        "  classifier = Classifier(checkpoint['input_size'], checkpoint['hidden_size'], checkpoint['drop_prob'], checkpoint['epoch'], checkpoint['val_loss'])\n",
        "  classifier.load_state_dict(checkpoint['state_dict'])\n",
        "  return classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LX_eofKUq0ok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_classifier(classifier)\n",
        "classifier = load_classifier()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PR8s5ZA5rX-V",
        "colab_type": "code",
        "outputId": "acbd1a11-4fff-44e4-c22e-71b06b56e4e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  classifier = classifier.cuda()\n",
        "  inputs = inputs.cuda()\n",
        "\n",
        "classifier(inputs[:3].float())"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4493],\n",
              "        [0.5016],\n",
              "        [0.4605]], device='cuda:0', grad_fn=<SigmoidBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arYOr5lMLYy7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(params=classifier.parameters(), lr=0.003)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFy03p96Lza1",
        "colab_type": "code",
        "outputId": "eae97fae-6b34-4c66-aba8-380328888f80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_loss = 0\n",
        "epochs = 200\n",
        "for epoch in range(epochs):\n",
        "  classifier.train()\n",
        "  for inputs, targets in train_dataloader:\n",
        "    if torch.cuda.is_available():\n",
        "      inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "    outputs = classifier(inputs.float())\n",
        "    loss = criterion(outputs.squeeze(), targets.float())\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    classifier.eval()\n",
        "    val_loss = 0\n",
        "    for inputs, targets in val_dataloader:\n",
        "      if torch.cuda.is_available():\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "      outputs = classifier(inputs.float())\n",
        "      loss = criterion(outputs.squeeze(), targets.float())\n",
        "      val_loss += loss.item()\n",
        "\n",
        "    train_loss = train_loss / len(train_dataloader)\n",
        "    val_loss = val_loss / len(val_dataloader)\n",
        "    print('Epoch: {}, Train loss: {}, Val loss: {}'.format(epoch, train_loss, val_loss))\n",
        "\n",
        "    if classifier.val_loss is None or val_loss < classifier.val_loss:\n",
        "      classifier.epoch = epoch\n",
        "      classifier.val_loss = val_loss\n",
        "      save_classifier(classifier)\n",
        "\n",
        "classifier.epoch = epoch\n",
        "classifier.val_loss = val_loss"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Train loss: 0.610739645205046, Val loss: 0.5140360621245283\n",
            "Epoch: 1, Train loss: 0.5691520774793757, Val loss: 0.5345292483505449\n",
            "Epoch: 2, Train loss: 0.551976387188184, Val loss: 0.5141056219213888\n",
            "Epoch: 3, Train loss: 0.537418377359848, Val loss: 0.516100591734836\n",
            "Epoch: 4, Train loss: 0.528453981718477, Val loss: 0.48866499333005203\n",
            "Epoch: 5, Train loss: 0.5226798066925861, Val loss: 0.5028612276441172\n",
            "Epoch: 6, Train loss: 0.5191937779371986, Val loss: 0.5018313609455761\n",
            "Epoch: 7, Train loss: 0.5120950806428075, Val loss: 0.49240878420440776\n",
            "Epoch: 8, Train loss: 0.511282972255007, Val loss: 0.526810391953117\n",
            "Epoch: 9, Train loss: 0.5063042575789819, Val loss: 0.5109318961438379\n",
            "Epoch: 10, Train loss: 0.5044644230151212, Val loss: 0.4914055815652797\n",
            "Epoch: 11, Train loss: 0.49918433175156, Val loss: 0.4932374773841155\n",
            "Epoch: 12, Train loss: 0.4970249366722791, Val loss: 0.475483096351749\n",
            "Epoch: 13, Train loss: 0.4917126152388942, Val loss: 0.48199496418237686\n",
            "Epoch: 14, Train loss: 0.49473613540716527, Val loss: 0.4878148173815326\n",
            "Epoch: 15, Train loss: 0.49279094370843746, Val loss: 0.5208107833015291\n",
            "Epoch: 16, Train loss: 0.48635036005070376, Val loss: 0.4736696062119384\n",
            "Epoch: 17, Train loss: 0.48263172968790474, Val loss: 0.4873753664524932\n",
            "Epoch: 18, Train loss: 0.4854529763168842, Val loss: 0.4618311638110562\n",
            "Epoch: 19, Train loss: 0.48733970513706343, Val loss: 0.5057380187668299\n",
            "Epoch: 20, Train loss: 0.48527004451886735, Val loss: 0.4808354691455239\n",
            "Epoch: 21, Train loss: 0.4803567965645894, Val loss: 0.4697711589305024\n",
            "Epoch: 22, Train loss: 0.4776230073806991, Val loss: 0.48039061693768753\n",
            "Epoch: 23, Train loss: 0.4745662712131607, Val loss: 0.5169820828657401\n",
            "Epoch: 24, Train loss: 0.4849782465923111, Val loss: 0.47793996883066076\n",
            "Epoch: 25, Train loss: 0.4727122325620847, Val loss: 0.4693006175129037\n",
            "Epoch: 26, Train loss: 0.4681994761785964, Val loss: 0.4797063214998496\n",
            "Epoch: 27, Train loss: 0.46830518660252846, Val loss: 0.4972213145933653\n",
            "Epoch: 28, Train loss: 0.4679535043389404, Val loss: 0.48850618734171514\n",
            "Epoch: 29, Train loss: 0.4619496261136333, Val loss: 0.4594075087653963\n",
            "Epoch: 30, Train loss: 0.4684843737176024, Val loss: 0.4539179562738067\n",
            "Epoch: 31, Train loss: 0.46162528745513987, Val loss: 0.48618125484177943\n",
            "Epoch: 32, Train loss: 0.4670070475005743, Val loss: 0.46479921748763636\n",
            "Epoch: 33, Train loss: 0.46490218860450444, Val loss: 0.4779980837514526\n",
            "Epoch: 34, Train loss: 0.46186973379958696, Val loss: 0.5024873320209352\n",
            "Epoch: 35, Train loss: 0.4628868405565121, Val loss: 0.48386238358522715\n",
            "Epoch: 36, Train loss: 0.4594514872017255, Val loss: 0.47525070490021454\n",
            "Epoch: 37, Train loss: 0.4616612789541555, Val loss: 0.47053061268831553\n",
            "Epoch: 38, Train loss: 0.46019880237681315, Val loss: 0.47625095397233963\n",
            "Epoch: 39, Train loss: 0.46086191850338654, Val loss: 0.4520093606490838\n",
            "Epoch: 40, Train loss: 0.4563143370228468, Val loss: 0.4873561286612561\n",
            "Epoch: 41, Train loss: 0.45937463671930384, Val loss: 0.46552640317302\n",
            "Epoch: 42, Train loss: 0.45185460778468733, Val loss: 0.48664858701981994\n",
            "Epoch: 43, Train loss: 0.4578207666237088, Val loss: 0.44149913952538844\n",
            "Epoch: 44, Train loss: 0.4503561735010956, Val loss: 0.5172617282522353\n",
            "Epoch: 45, Train loss: 0.458688192931855, Val loss: 0.45629433699344335\n",
            "Epoch: 46, Train loss: 0.45650583017729995, Val loss: 0.4539593454254301\n",
            "Epoch: 47, Train loss: 0.45314631402201383, Val loss: 0.44692567696696833\n",
            "Epoch: 48, Train loss: 0.4541863784820644, Val loss: 0.4647002059378122\n",
            "Epoch: 49, Train loss: 0.45283652759861215, Val loss: 0.47872393656718104\n",
            "Epoch: 50, Train loss: 0.4524184727923053, Val loss: 0.46565387123509455\n",
            "Epoch: 51, Train loss: 0.4432240990281523, Val loss: 0.439148943283056\n",
            "Epoch: 52, Train loss: 0.4555187789167239, Val loss: 0.47707416274045644\n",
            "Epoch: 53, Train loss: 0.4539513461620276, Val loss: 0.46235626661463786\n",
            "Epoch: 54, Train loss: 0.44533322169286815, Val loss: 0.46920531439153773\n",
            "Epoch: 55, Train loss: 0.44622384898335865, Val loss: 0.44979646566667053\n",
            "Epoch: 56, Train loss: 0.44205692786722617, Val loss: 0.4647484892293027\n",
            "Epoch: 57, Train loss: 0.4462866752761365, Val loss: 0.4901806492554514\n",
            "Epoch: 58, Train loss: 0.44226078837957755, Val loss: 0.45051450360762446\n",
            "Epoch: 59, Train loss: 0.4412690475974812, Val loss: 0.5113009826133126\n",
            "Epoch: 60, Train loss: 0.4432144786584647, Val loss: 0.44820163281340347\n",
            "Epoch: 61, Train loss: 0.4415520610664714, Val loss: 0.4454673458086817\n",
            "Epoch: 62, Train loss: 0.44196422132027363, Val loss: 0.48936518556193304\n",
            "Epoch: 63, Train loss: 0.43997954178070087, Val loss: 0.47982073418403925\n",
            "Epoch: 64, Train loss: 0.44193546836005365, Val loss: 0.45729020630058487\n",
            "Epoch: 65, Train loss: 0.4487971699719959, Val loss: 0.45817814296797704\n",
            "Epoch: 66, Train loss: 0.43883622564894414, Val loss: 0.46823420061876897\n",
            "Epoch: 67, Train loss: 0.4325389605546266, Val loss: 0.4916309028079635\n",
            "Epoch: 68, Train loss: 0.44617778465879665, Val loss: 0.4542550179911287\n",
            "Epoch: 69, Train loss: 0.44802771676154846, Val loss: 0.4766062078507323\n",
            "Epoch: 70, Train loss: 0.44288055720783426, Val loss: 0.47524504559604747\n",
            "Epoch: 71, Train loss: 0.43060686207206034, Val loss: 0.46773528699812134\n",
            "Epoch: 72, Train loss: 0.43650920977100754, Val loss: 0.45173082010526405\n",
            "Epoch: 73, Train loss: 0.433125729307246, Val loss: 0.49062753233470413\n",
            "Epoch: 74, Train loss: 0.4350347807779833, Val loss: 0.46244379427087934\n",
            "Epoch: 75, Train loss: 0.4275189496727756, Val loss: 0.4797684813016339\n",
            "Epoch: 76, Train loss: 0.42362937450840793, Val loss: 0.46831766986533213\n",
            "Epoch: 77, Train loss: 0.4334562719183101, Val loss: 0.4831659327212133\n",
            "Epoch: 78, Train loss: 0.43314088567389974, Val loss: 0.4697376007312222\n",
            "Epoch: 79, Train loss: 0.4286304103659679, Val loss: 0.4806231044624981\n",
            "Epoch: 80, Train loss: 0.4238185403831358, Val loss: 0.4487840701874934\n",
            "Epoch: 81, Train loss: 0.42849001711420737, Val loss: 0.4666522831508988\n",
            "Epoch: 82, Train loss: 0.43849859515455014, Val loss: 0.4509255552762433\n",
            "Epoch: 83, Train loss: 0.4300511003271599, Val loss: 0.47581885911916433\n",
            "Epoch: 84, Train loss: 0.43207842613713743, Val loss: 0.4743355657709272\n",
            "Epoch: 85, Train loss: 0.4322479525431878, Val loss: 0.4731721101622832\n",
            "Epoch: 86, Train loss: 0.42144591834258843, Val loss: 0.4608599543571472\n",
            "Epoch: 87, Train loss: 0.43053078526627386, Val loss: 0.4597456423859847\n",
            "Epoch: 88, Train loss: 0.41920137715819933, Val loss: 0.4853634449996446\n",
            "Epoch: 89, Train loss: 0.42437579059613467, Val loss: 0.44633852278715686\n",
            "Epoch: 90, Train loss: 0.41734371578066304, Val loss: 0.5078688416826097\n",
            "Epoch: 91, Train loss: 0.42915887842977485, Val loss: 0.4589174025152859\n",
            "Epoch: 92, Train loss: 0.4294599415467508, Val loss: 0.4439990614589892\n",
            "Epoch: 93, Train loss: 0.42274581319827353, Val loss: 0.48076604620406505\n",
            "Epoch: 94, Train loss: 0.42438645632819616, Val loss: 0.4521884082963592\n",
            "Epoch: 95, Train loss: 0.41664473829346693, Val loss: 0.47160021686240244\n",
            "Epoch: 96, Train loss: 0.4165020597131832, Val loss: 0.4921475170474303\n",
            "Epoch: 97, Train loss: 0.42308663619231696, Val loss: 0.44634604571681274\n",
            "Epoch: 98, Train loss: 0.42367108304580875, Val loss: 0.46899034984801946\n",
            "Epoch: 99, Train loss: 0.41830114984702665, Val loss: 0.48085409520488037\n",
            "Epoch: 100, Train loss: 0.42239169971571455, Val loss: 0.45115717657302556\n",
            "Epoch: 101, Train loss: 0.4161871236919379, Val loss: 0.46115695332226\n",
            "Epoch: 102, Train loss: 0.42029759016570034, Val loss: 0.4677867034548207\n",
            "Epoch: 103, Train loss: 0.41442934847895946, Val loss: 0.47807266680817856\n",
            "Epoch: 104, Train loss: 0.4139658890212379, Val loss: 0.46387544548825216\n",
            "Epoch: 105, Train loss: 0.41172706161163564, Val loss: 0.45283372817855133\n",
            "Epoch: 106, Train loss: 0.41605541399545376, Val loss: 0.470762647296253\n",
            "Epoch: 107, Train loss: 0.4171988729561178, Val loss: 0.46414702817013387\n",
            "Epoch: 108, Train loss: 0.41676905767939804, Val loss: 0.44383719053707626\n",
            "Epoch: 109, Train loss: 0.41119269870269254, Val loss: 0.47511641994902964\n",
            "Epoch: 110, Train loss: 0.408972906789902, Val loss: 0.4553882997286947\n",
            "Epoch: 111, Train loss: 0.41418813999602994, Val loss: 0.47379600884098755\n",
            "Epoch: 112, Train loss: 0.4194478571247923, Val loss: 0.45619028257696254\n",
            "Epoch: 113, Train loss: 0.4172989040608779, Val loss: 0.4874228415520568\n",
            "Epoch: 114, Train loss: 0.417843233362613, Val loss: 0.4973352657336938\n",
            "Epoch: 115, Train loss: 0.4110201843165457, Val loss: 0.4637851315109353\n",
            "Epoch: 116, Train loss: 0.4097668480130222, Val loss: 0.48188811697457967\n",
            "Epoch: 117, Train loss: 0.41788035696863135, Val loss: 0.48263179510831833\n",
            "Epoch: 118, Train loss: 0.4123922437602279, Val loss: 0.4513487723705016\n",
            "Epoch: 119, Train loss: 0.4150752915368111, Val loss: 0.45183510607794714\n",
            "Epoch: 120, Train loss: 0.41345546327511895, Val loss: 0.4782129377126694\n",
            "Epoch: 121, Train loss: 0.4165605324094966, Val loss: 0.45453527295275736\n",
            "Epoch: 122, Train loss: 0.40647487660663556, Val loss: 0.4554056442881885\n",
            "Epoch: 123, Train loss: 0.4091713605796081, Val loss: 0.47970328362364517\n",
            "Epoch: 124, Train loss: 0.4073929336470334, Val loss: 0.47354207501599666\n",
            "Epoch: 125, Train loss: 0.4105018263501085, Val loss: 0.47142766416072845\n",
            "Epoch: 126, Train loss: 0.4082275767217659, Val loss: 0.4678311187185739\n",
            "Epoch: 127, Train loss: 0.4086486771417568, Val loss: 0.4685894870444348\n",
            "Epoch: 128, Train loss: 0.4011553198483767, Val loss: 0.4542804576064411\n",
            "Epoch: 129, Train loss: 0.4112121164024587, Val loss: 0.4584348205673067\n",
            "Epoch: 130, Train loss: 0.4089737087467802, Val loss: 0.4775959610154754\n",
            "Epoch: 131, Train loss: 0.40645496743284687, Val loss: 0.4763276922075372\n",
            "Epoch: 132, Train loss: 0.41232030138043013, Val loss: 0.46157756487005636\n",
            "Epoch: 133, Train loss: 0.40395329462453655, Val loss: 0.48514339641520854\n",
            "Epoch: 134, Train loss: 0.40547885263880223, Val loss: 0.46423763977853877\n",
            "Epoch: 135, Train loss: 0.4054692439251093, Val loss: 0.47683697311501755\n",
            "Epoch: 136, Train loss: 0.4113113630550288, Val loss: 0.45645490837724584\n",
            "Epoch: 137, Train loss: 0.4084901716853892, Val loss: 0.4644499819137548\n",
            "Epoch: 138, Train loss: 0.40309702306082823, Val loss: 0.47698539417040975\n",
            "Epoch: 139, Train loss: 0.41328424030989175, Val loss: 0.4563860301124422\n",
            "Epoch: 140, Train loss: 0.40360789780408923, Val loss: 0.4688107238004082\n",
            "Epoch: 141, Train loss: 0.4054468868292713, Val loss: 0.4908490435857522\n",
            "Epoch: 142, Train loss: 0.4018959763753763, Val loss: 0.45180314583213704\n",
            "Epoch: 143, Train loss: 0.40028051107365925, Val loss: 0.47300548184859126\n",
            "Epoch: 144, Train loss: 0.40672366100686663, Val loss: 0.4592971001800738\n",
            "Epoch: 145, Train loss: 0.4030948739507057, Val loss: 0.46460192258420746\n",
            "Epoch: 146, Train loss: 0.39551077746837354, Val loss: 0.48828607523127604\n",
            "Epoch: 147, Train loss: 0.40616744089113227, Val loss: 0.45760615522924225\n",
            "Epoch: 148, Train loss: 0.40423958927078896, Val loss: 0.46757172969611066\n",
            "Epoch: 149, Train loss: 0.4004504263478632, Val loss: 0.46218427153010117\n",
            "Epoch: 150, Train loss: 0.40688758821884297, Val loss: 0.479648168934019\n",
            "Epoch: 151, Train loss: 0.3928216797039173, Val loss: 0.49289214258131225\n",
            "Epoch: 152, Train loss: 0.40211243718843, Val loss: 0.4635128923936894\n",
            "Epoch: 153, Train loss: 0.4005087915454989, Val loss: 0.46504360025650576\n",
            "Epoch: 154, Train loss: 0.39747364035665644, Val loss: 0.471820914823758\n",
            "Epoch: 155, Train loss: 0.3938232513141788, Val loss: 0.4745933323314315\n",
            "Epoch: 156, Train loss: 0.3970061481832207, Val loss: 0.45220276517303365\n",
            "Epoch: 157, Train loss: 0.3992664592852685, Val loss: 0.4648797017963309\n",
            "Epoch: 158, Train loss: 0.39627036158816464, Val loss: 0.4845784549650393\n",
            "Epoch: 159, Train loss: 0.3869412728842352, Val loss: 0.4954844387738328\n",
            "Epoch: 160, Train loss: 0.3912024032355826, Val loss: 0.47173897373048884\n",
            "Epoch: 161, Train loss: 0.40892817035913853, Val loss: 0.49548278552921193\n",
            "Epoch: 162, Train loss: 0.39225333104415955, Val loss: 0.49790700172123153\n",
            "Epoch: 163, Train loss: 0.398407408848636, Val loss: 0.47681564210276856\n",
            "Epoch: 164, Train loss: 0.3915769852372437, Val loss: 0.4870273349316497\n",
            "Epoch: 165, Train loss: 0.38663169767033745, Val loss: 0.47695440602929967\n",
            "Epoch: 166, Train loss: 0.3954213991043786, Val loss: 0.46210350762856633\n",
            "Epoch: 167, Train loss: 0.3986141638125187, Val loss: 0.4628756230599002\n",
            "Epoch: 168, Train loss: 0.39468859830489594, Val loss: 0.4826909711486415\n",
            "Epoch: 169, Train loss: 0.39397584037796207, Val loss: 0.4910146515620382\n",
            "Epoch: 170, Train loss: 0.4024582619303192, Val loss: 0.4708956734914529\n",
            "Epoch: 171, Train loss: 0.3854977397407192, Val loss: 0.4794305315927455\n",
            "Epoch: 172, Train loss: 0.3947332506856508, Val loss: 0.46713685989379883\n",
            "Epoch: 173, Train loss: 0.39659638861156654, Val loss: 0.48223883305725296\n",
            "Epoch: 174, Train loss: 0.38618108124569245, Val loss: 0.4764795873902346\n",
            "Epoch: 175, Train loss: 0.3931789475289968, Val loss: 0.4645537326210423\n",
            "Epoch: 176, Train loss: 0.38911485849139976, Val loss: 0.46089774095698405\n",
            "Epoch: 177, Train loss: 0.3883662945811994, Val loss: 0.46840292881978185\n",
            "Epoch: 178, Train loss: 0.3879074026272617, Val loss: 0.4847771094033593\n",
            "Epoch: 179, Train loss: 0.38588005975967415, Val loss: 0.46725694443050186\n",
            "Epoch: 180, Train loss: 0.39482169968937725, Val loss: 0.4768484019135174\n",
            "Epoch: 181, Train loss: 0.39355166952417997, Val loss: 0.49979980995780543\n",
            "Epoch: 182, Train loss: 0.39118943303316484, Val loss: 0.4793631885396807\n",
            "Epoch: 183, Train loss: 0.39231342173707323, Val loss: 0.46138120168133784\n",
            "Epoch: 184, Train loss: 0.3854672995306661, Val loss: 0.48369154510529416\n",
            "Epoch: 185, Train loss: 0.3859278954097514, Val loss: 0.47982307013712433\n",
            "Epoch: 186, Train loss: 0.3856953949704214, Val loss: 0.4768823181328021\n",
            "Epoch: 187, Train loss: 0.3902122142309014, Val loss: 0.4724738374352455\n",
            "Epoch: 188, Train loss: 0.39057377546083005, Val loss: 0.472514770925045\n",
            "Epoch: 189, Train loss: 0.39440657840722776, Val loss: 0.47908520110343633\n",
            "Epoch: 190, Train loss: 0.38535988622238604, Val loss: 0.463024196067923\n",
            "Epoch: 191, Train loss: 0.3955272342796206, Val loss: 0.46286895126104355\n",
            "Epoch: 192, Train loss: 0.38579643140245706, Val loss: 0.48077528217905446\n",
            "Epoch: 193, Train loss: 0.38319996636426484, Val loss: 0.45902006818275704\n",
            "Epoch: 194, Train loss: 0.38687301875240865, Val loss: 0.48582663779195984\n",
            "Epoch: 195, Train loss: 0.3836547013963229, Val loss: 0.4743776654726581\n",
            "Epoch: 196, Train loss: 0.3846805994365429, Val loss: 0.48189718158621536\n",
            "Epoch: 197, Train loss: 0.3885372234139887, Val loss: 0.48062755520406525\n",
            "Epoch: 198, Train loss: 0.3821517557239168, Val loss: 0.4799030654524502\n",
            "Epoch: 199, Train loss: 0.38430031279287746, Val loss: 0.4947412159097822\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXw0R-Tkucgw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_recalls(targets, predictions):\n",
        "  n_classes = 2\n",
        "  confusion_matrix, _, _ = np.histogram2d(targets.detach().cpu().numpy(), predictions.squeeze().detach().cpu().numpy(), bins=n_classes)\n",
        "  return np.diag(confusion_matrix) / (np.sum(confusion_matrix, axis=1))\n",
        "\n",
        "def gmean(classifier, dataloader):\n",
        "  with torch.no_grad():\n",
        "    classifier.eval()\n",
        "    recalls = np.zeros((2))\n",
        "    for inputs, targets in dataloader:\n",
        "      if torch.cuda.is_available():\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "      \n",
        "      outputs = classifier(inputs.float())\n",
        "      predictions = torch.round(outputs).int()\n",
        "      recalls += calculate_recalls(targets, predictions)\n",
        "\n",
        "    recalls = recalls / len(dataloader)\n",
        "    return mstats.gmean(recalls)    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlrNw0bEqAeh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(classifier):\n",
        "  if torch.cuda.is_available():\n",
        "    classifier = classifier.cuda()\n",
        "  train_accuracy = gmean(classifier, train_dataloader)\n",
        "  test_accuracy = gmean(classifier, test_dataloader)\n",
        "  print('Epoch: {}'.format(classifier.epoch))\n",
        "  print('Train g-mean: {}'.format(train_accuracy.item()))\n",
        "  print('Test g-mean: {}'.format(test_accuracy.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFoBTASdvqRv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "509e0efb-11e6-4aa1-8c54-98435fc312f0"
      },
      "source": [
        "print('Best classifier')\n",
        "evaluate(load_classifier())"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best classifier\n",
            "Epoch: 51\n",
            "Train g-mean: 0.794108949293009\n",
            "Test g-mean: 0.7376259936221754\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "918KM5F07hFl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "a710db9b-c6ae-43a6-915d-fbbd260fe7e9"
      },
      "source": [
        "print('Last classifier')\n",
        "evaluate(classifier)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Last classifier\n",
            "Epoch: 199\n",
            "Train g-mean: 0.8423115451462121\n",
            "Test g-mean: 0.7433104830683602\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_t4Gh43GYWl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}