{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JIT-SDP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dinaldoap/jit-sdp-nn/blob/master/notebook/mlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXwvNEAQE6NO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import random\n",
        "from scipy.stats import mstats"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAupgTDfmJ_I",
        "colab_type": "code",
        "outputId": "51eea8c7-3eec-486a-c889-e6a8bf739f26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/dinaldoap/jit-sdp-data/master/jenkins.csv')\n",
        "df.head()"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fix</th>\n",
              "      <th>ns</th>\n",
              "      <th>nd</th>\n",
              "      <th>nf</th>\n",
              "      <th>entropy</th>\n",
              "      <th>la</th>\n",
              "      <th>ld</th>\n",
              "      <th>lt</th>\n",
              "      <th>ndev</th>\n",
              "      <th>age</th>\n",
              "      <th>nuc</th>\n",
              "      <th>exp</th>\n",
              "      <th>rexp</th>\n",
              "      <th>sexp</th>\n",
              "      <th>author_date_unix_timestamp</th>\n",
              "      <th>classification</th>\n",
              "      <th>contains_bug</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>False</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.641604</td>\n",
              "      <td>9.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>426.428571</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.000093</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5171.0</td>\n",
              "      <td>30.227271</td>\n",
              "      <td>1472.714286</td>\n",
              "      <td>1555326371</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>False</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.750000</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>426.428571</td>\n",
              "      <td>100.0</td>\n",
              "      <td>6.314775</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5170.0</td>\n",
              "      <td>29.227271</td>\n",
              "      <td>1471.714286</td>\n",
              "      <td>1555326363</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>False</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.906580</td>\n",
              "      <td>15.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>96.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.034722</td>\n",
              "      <td>2.0</td>\n",
              "      <td>629.0</td>\n",
              "      <td>14.828373</td>\n",
              "      <td>414.000000</td>\n",
              "      <td>1554971763</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>False</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.058824</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1554969774</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>False</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.662506</td>\n",
              "      <td>14.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>67.000000</td>\n",
              "      <td>6.0</td>\n",
              "      <td>21.280683</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.058824</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1554967752</td>\n",
              "      <td>Feature Addition</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     fix   ns   nd  ...  author_date_unix_timestamp    classification  contains_bug\n",
              "0  False  7.0  7.0  ...                  1555326371              None         False\n",
              "1  False  7.0  7.0  ...                  1555326363              None         False\n",
              "2  False  1.0  1.0  ...                  1554971763              None         False\n",
              "3  False  1.0  1.0  ...                  1554969774              None         False\n",
              "4  False  1.0  2.0  ...                  1554967752  Feature Addition         False\n",
              "\n",
              "[5 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alizaxdPD3El",
        "colab_type": "code",
        "outputId": "384cda6d-c0e6-4c3a-da2a-e4befd9f4dfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "#df = df.sample(frac=1)\n",
        "label_col = 'contains_bug'\n",
        "features_cols = ['fix', 'ns', 'nd', 'nf', 'entropy', 'la', 'ld', 'lt', 'ndev', 'age', 'nuc', 'exp', 'rexp', 'sexp', 'classification']\n",
        "x = df[features_cols]\n",
        "x['fix'] = x['fix'].astype('int')\n",
        "df_classification = pd.get_dummies(x, columns=['classification'])\n",
        "x = pd.concat([x, df_classification], axis='columns')\n",
        "x = x.drop(['classification'], axis='columns')\n",
        "x = x.values\n",
        "y = df[label_col]\n",
        "y = y.astype('category')\n",
        "y = y.cat.codes\n",
        "y = y.values"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBD1-tp2ExzQ",
        "colab_type": "code",
        "outputId": "6993bb65-0dac-401c-a4f9-57546e178626",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "print(x[:3])\n",
        "print(y[:3])"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.00000000e+00 7.00000000e+00 7.00000000e+00 7.00000000e+00\n",
            "  2.64160417e+00 9.00000000e+00 9.00000000e+00 4.26428571e+02\n",
            "  1.00000000e+02 9.25925926e-05 1.00000000e+00 5.17100000e+03\n",
            "  3.02272705e+01 1.47271429e+03 0.00000000e+00 7.00000000e+00\n",
            "  7.00000000e+00 7.00000000e+00 2.64160417e+00 9.00000000e+00\n",
            "  9.00000000e+00 4.26428571e+02 1.00000000e+02 9.25925926e-05\n",
            "  1.00000000e+00 5.17100000e+03 3.02272705e+01 1.47271429e+03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 7.00000000e+00 7.00000000e+00 7.00000000e+00\n",
            "  2.75000000e+00 8.00000000e+00 8.00000000e+00 4.26428571e+02\n",
            "  1.00000000e+02 6.31477513e+00 2.00000000e+00 5.17000000e+03\n",
            "  2.92272705e+01 1.47171429e+03 0.00000000e+00 7.00000000e+00\n",
            "  7.00000000e+00 7.00000000e+00 2.75000000e+00 8.00000000e+00\n",
            "  8.00000000e+00 4.26428571e+02 1.00000000e+02 6.31477513e+00\n",
            "  2.00000000e+00 5.17000000e+03 2.92272705e+01 1.47171429e+03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 1.00000000e+00 2.00000000e+00\n",
            "  9.06579548e-01 1.50000000e+01 4.40000000e+01 9.60000000e+01\n",
            "  4.00000000e+00 3.47222222e-02 2.00000000e+00 6.29000000e+02\n",
            "  1.48283729e+01 4.14000000e+02 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 2.00000000e+00 9.06579548e-01 1.50000000e+01\n",
            "  4.40000000e+01 9.60000000e+01 4.00000000e+00 3.47222222e-02\n",
            "  2.00000000e+00 6.29000000e+02 1.48283729e+01 4.14000000e+02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "[0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwCn1uNTMjfG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_index = int( len(x) * 0.8 )\n",
        "test_index = int( len(x) * 0.9 )\n",
        "x_train, y_train = x[:val_index], y[:val_index]\n",
        "x_val, y_val = x[val_index:test_index], y[val_index:test_index]\n",
        "x_test, y_test = x[test_index:], y[test_index:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSqnoapE5Poq",
        "colab_type": "code",
        "outputId": "7f681ca2-76a0-4bd9-cba2-0b35e39b0e04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(x_train[:3])\n",
        "print(x_val[:3])\n",
        "print(x_test[:3])"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.00000000e+00 7.00000000e+00 7.00000000e+00 7.00000000e+00\n",
            "  2.64160417e+00 9.00000000e+00 9.00000000e+00 4.26428571e+02\n",
            "  1.00000000e+02 9.25925926e-05 1.00000000e+00 5.17100000e+03\n",
            "  3.02272705e+01 1.47271429e+03 0.00000000e+00 7.00000000e+00\n",
            "  7.00000000e+00 7.00000000e+00 2.64160417e+00 9.00000000e+00\n",
            "  9.00000000e+00 4.26428571e+02 1.00000000e+02 9.25925926e-05\n",
            "  1.00000000e+00 5.17100000e+03 3.02272705e+01 1.47271429e+03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 7.00000000e+00 7.00000000e+00 7.00000000e+00\n",
            "  2.75000000e+00 8.00000000e+00 8.00000000e+00 4.26428571e+02\n",
            "  1.00000000e+02 6.31477513e+00 2.00000000e+00 5.17000000e+03\n",
            "  2.92272705e+01 1.47171429e+03 0.00000000e+00 7.00000000e+00\n",
            "  7.00000000e+00 7.00000000e+00 2.75000000e+00 8.00000000e+00\n",
            "  8.00000000e+00 4.26428571e+02 1.00000000e+02 6.31477513e+00\n",
            "  2.00000000e+00 5.17000000e+03 2.92272705e+01 1.47171429e+03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 1.00000000e+00 2.00000000e+00\n",
            "  9.06579548e-01 1.50000000e+01 4.40000000e+01 9.60000000e+01\n",
            "  4.00000000e+00 3.47222222e-02 2.00000000e+00 6.29000000e+02\n",
            "  1.48283729e+01 4.14000000e+02 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 2.00000000e+00 9.06579548e-01 1.50000000e+01\n",
            "  4.40000000e+01 9.60000000e+01 4.00000000e+00 3.47222222e-02\n",
            "  2.00000000e+00 6.29000000e+02 1.48283729e+01 4.14000000e+02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "[[0.00000000e+00 8.00000000e+00 8.00000000e+00 8.00000000e+00\n",
            "  2.73215889e+00 1.10000000e+01 1.10000000e+01 1.58625000e+02\n",
            "  5.00000000e+00 1.96759259e-04 1.00000000e+00 4.17300000e+03\n",
            "  2.45570735e+02 8.19125000e+02 0.00000000e+00 8.00000000e+00\n",
            "  8.00000000e+00 8.00000000e+00 2.73215889e+00 1.10000000e+01\n",
            "  1.10000000e+01 1.58625000e+02 5.00000000e+00 1.96759259e-04\n",
            "  1.00000000e+00 4.17300000e+03 2.45570735e+02 8.19125000e+02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 8.00000000e+00 8.00000000e+00 8.00000000e+00\n",
            "  2.73215889e+00 1.10000000e+01 1.10000000e+01 1.58625000e+02\n",
            "  5.00000000e+00 4.29604022e+00 5.00000000e+00 4.17200000e+03\n",
            "  2.44570735e+02 8.18125000e+02 0.00000000e+00 8.00000000e+00\n",
            "  8.00000000e+00 8.00000000e+00 2.73215889e+00 1.10000000e+01\n",
            "  1.10000000e+01 1.58625000e+02 5.00000000e+00 4.29604022e+00\n",
            "  5.00000000e+00 4.17200000e+03 2.44570735e+02 8.18125000e+02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 1.00000000e+00 1.07000000e+02\n",
            "  1.00000000e+00 5.29513889e-01 1.00000000e+00 4.17100000e+03\n",
            "  2.43572058e+02 8.40000000e+01 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.07000000e+02 1.00000000e+00 5.29513889e-01\n",
            "  1.00000000e+00 4.17100000e+03 2.43572058e+02 8.40000000e+01\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "[[0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 1.00000000e+00 2.31000000e+02\n",
            "  2.00000000e+00 8.17129630e-03 1.00000000e+00 2.29000000e+03\n",
            "  2.35138387e+02 1.96900000e+03 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 2.31000000e+02 2.00000000e+00 8.17129630e-03\n",
            "  1.00000000e+00 2.29000000e+03 2.35138387e+02 1.96900000e+03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 3.00000000e+00 4.00000000e+00\n",
            "  1.92417435e+00 1.00000000e+01 4.00000000e+00 7.95000000e+01\n",
            "  2.00000000e+00 7.59004630e-01 4.00000000e+00 2.28900000e+03\n",
            "  2.34145689e+02 1.96800000e+03 0.00000000e+00 1.00000000e+00\n",
            "  3.00000000e+00 4.00000000e+00 1.92417435e+00 1.00000000e+01\n",
            "  4.00000000e+00 7.95000000e+01 2.00000000e+00 7.59004630e-01\n",
            "  4.00000000e+00 2.28900000e+03 2.34145689e+02 1.96800000e+03\n",
            "  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 5.00000000e+00 4.00000000e+00 2.10000000e+01\n",
            "  1.00000000e+00 5.92943287e+00 1.00000000e+00 2.28800000e+03\n",
            "  2.33161190e+02 1.96700000e+03 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 5.00000000e+00\n",
            "  4.00000000e+00 2.10000000e+01 1.00000000e+00 5.92943287e+00\n",
            "  1.00000000e+00 2.28800000e+03 2.33161190e+02 1.96700000e+03\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46STsbti3AmT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean_train = x_train.mean(axis=0)\n",
        "std_train = x_train.std(axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5vvbrrK3PES",
        "colab_type": "code",
        "outputId": "88ef6040-c723-44e8-9cc8-07bf644a8354",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "print(mean_train)\n",
        "print(std_train)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2.90871477e-01 1.61432635e+00 2.60858547e+00 4.18908715e+00\n",
            " 7.43390526e-01 5.00800621e+01 2.96331523e+01 8.11968929e+02\n",
            " 2.32963538e+01 9.76994760e+01 2.02296354e+00 1.73095790e+03\n",
            " 6.53773667e+01 8.28714959e+02 2.90871477e-01 1.61432635e+00\n",
            " 2.60858547e+00 4.18908715e+00 7.43390526e-01 5.00800621e+01\n",
            " 2.96331523e+01 8.11968929e+02 2.32963538e+01 9.76994760e+01\n",
            " 2.02296354e+00 1.73095790e+03 6.53773667e+01 8.28714959e+02\n",
            " 2.90871477e-01 1.47142488e-01 3.46521852e-02 4.71373157e-01\n",
            " 1.51538660e-02 4.08068270e-02]\n",
            "[4.54164354e-01 1.62736223e+00 6.93052365e+00 2.56616350e+01\n",
            " 1.13236779e+00 5.86250473e+02 6.19852440e+02 1.15236538e+03\n",
            " 2.84287684e+01 2.37802272e+02 7.67653474e+00 2.11471132e+03\n",
            " 6.96820390e+01 1.34456679e+03 4.54164354e-01 1.62736223e+00\n",
            " 6.93052365e+00 2.56616350e+01 1.13236779e+00 5.86250473e+02\n",
            " 6.19852440e+02 1.15236538e+03 2.84287684e+01 2.37802272e+02\n",
            " 7.67653474e+00 2.11471132e+03 6.96820390e+01 1.34456679e+03\n",
            " 4.54164354e-01 3.54247902e-01 1.82897270e-01 4.99179831e-01\n",
            " 1.22164751e-01 1.97842437e-01]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2IoFA5h425Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = (x_train - mean_train) / std_train\n",
        "x_val = (x_val - mean_train) / std_train\n",
        "x_test = (x_test - mean_train) / std_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1AoKNoU5V1w",
        "colab_type": "code",
        "outputId": "d3a19775-a4e9-429b-e852-c7586e362471",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(x_train[:3])\n",
        "print(x_val[:3])\n",
        "print(x_test[:3])"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-6.40454219e-01  3.30944983e+00  6.33633872e-01  1.09537559e-01\n",
            "   1.67632253e+00 -7.00725440e-02 -3.32872003e-02 -3.34564335e-01\n",
            "   2.69809951e+00 -4.10842935e-01 -1.33258504e-01  1.62671948e+00\n",
            "  -5.04435529e-01  4.78964178e-01 -6.40454219e-01  3.30944983e+00\n",
            "   6.33633872e-01  1.09537559e-01  1.67632253e+00 -7.00725440e-02\n",
            "  -3.32872003e-02 -3.34564335e-01  2.69809951e+00 -4.10842935e-01\n",
            "  -1.33258504e-01  1.62671948e+00 -5.04435529e-01  4.78964178e-01\n",
            "  -6.40454219e-01 -4.15365869e-01 -1.89462561e-01  1.05899079e+00\n",
            "  -1.24044505e-01 -2.06259221e-01]\n",
            " [-6.40454219e-01  3.30944983e+00  6.33633872e-01  1.09537559e-01\n",
            "   1.77204746e+00 -7.17782996e-02 -3.49004875e-02 -3.34564335e-01\n",
            "   2.69809951e+00 -3.84288594e-01 -2.99139370e-03  1.62624660e+00\n",
            "  -5.18786429e-01  4.78220444e-01 -6.40454219e-01  3.30944983e+00\n",
            "   6.33633872e-01  1.09537559e-01  1.77204746e+00 -7.17782996e-02\n",
            "  -3.49004875e-02 -3.34564335e-01  2.69809951e+00 -3.84288594e-01\n",
            "  -2.99139370e-03  1.62624660e+00 -5.18786429e-01  4.78220444e-01\n",
            "  -6.40454219e-01 -4.15365869e-01 -1.89462561e-01  1.05899079e+00\n",
            "  -1.24044505e-01 -2.06259221e-01]\n",
            " [-6.40454219e-01 -3.77498225e-01 -2.32101577e-01 -8.53058331e-02\n",
            "   1.44113090e-01 -5.98380107e-02  2.31778513e-02 -6.21303747e-01\n",
            "  -6.78761509e-01 -4.10697311e-01 -2.99139370e-03 -5.21091408e-01\n",
            "  -7.25423575e-01 -3.08437605e-01 -6.40454219e-01 -3.77498225e-01\n",
            "  -2.32101577e-01 -8.53058331e-02  1.44113090e-01 -5.98380107e-02\n",
            "   2.31778513e-02 -6.21303747e-01 -6.78761509e-01 -4.10697311e-01\n",
            "  -2.99139370e-03 -5.21091408e-01 -7.25423575e-01 -3.08437605e-01\n",
            "  -6.40454219e-01 -4.15365869e-01 -1.89462561e-01  1.05899079e+00\n",
            "  -1.24044505e-01 -2.06259221e-01]]\n",
            "[[-0.64045422  3.92394118  0.77792311  0.14850624  1.75629188 -0.06666103\n",
            "  -0.03006063 -0.566959   -0.64358587 -0.4108425  -0.1332585   1.15478745\n",
            "   2.58593708 -0.00713238 -0.64045422  3.92394118  0.77792311  0.14850624\n",
            "   1.75629188 -0.06666103 -0.03006063 -0.566959   -0.64358587 -0.4108425\n",
            "  -0.1332585   1.15478745  2.58593708 -0.00713238 -0.64045422 -0.41536587\n",
            "  -0.18946256  1.05899079 -0.1240445  -0.20625922]\n",
            " [-0.64045422  3.92394118  0.77792311  0.14850624  1.75629188 -0.06666103\n",
            "  -0.03006063 -0.566959   -0.64358587 -0.39277773  0.38780994  1.15431457\n",
            "   2.57158618 -0.00787611 -0.64045422  3.92394118  0.77792311  0.14850624\n",
            "   1.75629188 -0.06666103 -0.03006063 -0.566959   -0.64358587 -0.39277773\n",
            "   0.38780994  1.15431457  2.57158618 -0.00787611 -0.64045422 -0.41536587\n",
            "  -0.18946256  1.05899079 -0.1240445  -0.20625922]\n",
            " [-0.64045422 -0.37749822 -0.23210158 -0.12427451 -0.65649211 -0.08371859\n",
            "  -0.0461935  -0.61175816 -0.78428842 -0.40861663 -0.1332585   1.1538417\n",
            "   2.55725426 -0.55386982 -0.64045422 -0.37749822 -0.23210158 -0.12427451\n",
            "  -0.65649211 -0.08371859 -0.0461935  -0.61175816 -0.78428842 -0.40861663\n",
            "  -0.1332585   1.1538417   2.55725426 -0.55386982 -0.64045422 -0.41536587\n",
            "  -0.18946256  1.05899079 -0.1240445  -0.20625922]]\n",
            "[[-0.64045422 -0.37749822 -0.23210158 -0.12427451 -0.65649211 -0.08371859\n",
            "  -0.0461935  -0.5041534  -0.74911278 -0.41080896 -0.1332585   0.26435859\n",
            "   2.43622349  0.84806872 -0.64045422 -0.37749822 -0.23210158 -0.12427451\n",
            "  -0.65649211 -0.08371859 -0.0461935  -0.5041534  -0.74911278 -0.41080896\n",
            "  -0.1332585   0.26435859  2.43622349  0.84806872 -0.64045422 -0.41536587\n",
            "  -0.18946256  1.05899079 -0.1240445  -0.20625922]\n",
            " [-0.64045422 -0.37749822  0.05647691 -0.00736848  1.04275646 -0.06836679\n",
            "  -0.04135364 -0.63562212 -0.74911278 -0.40765158  0.25754283  0.26388571\n",
            "   2.42197738  0.84732499 -0.64045422 -0.37749822  0.05647691 -0.00736848\n",
            "   1.04275646 -0.06836679 -0.04135364 -0.63562212 -0.74911278 -0.40765158\n",
            "   0.25754283  0.26388571  2.42197738  0.84732499 -0.64045422  2.40751606\n",
            "  -0.18946256 -0.94429528 -0.1240445  -0.20625922]\n",
            " [-0.64045422 -0.37749822 -0.23210158 -0.12427451 -0.65649211 -0.07689557\n",
            "  -0.04135364 -0.68638727 -0.78428842 -0.38590903 -0.1332585   0.26341283\n",
            "   2.40784893  0.84658126 -0.64045422 -0.37749822 -0.23210158 -0.12427451\n",
            "  -0.65649211 -0.07689557 -0.04135364 -0.68638727 -0.78428842 -0.38590903\n",
            "  -0.1332585   0.26341283  2.40784893  0.84658126 -0.64045422 -0.41536587\n",
            "   5.27808762 -0.94429528 -0.1240445  -0.20625922]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNcuSWBQp2Ua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_sampler(y):\n",
        "  _, counts = np.unique(y, return_counts=True)\n",
        "  n_samples = len(y)\n",
        "  fading_factor = 0.9999\n",
        "  fading_weights = reversed(range(n_samples))\n",
        "  fading_weights = [fading_factor**x for x in fading_weights]\n",
        "  total_fading_count = np.sum(fading_weights)\n",
        "  bug_fading_count = np.sum(fading_weights * y)\n",
        "  normal_fading_count = total_fading_count - bug_fading_count\n",
        "  class_weights = total_fading_count / [normal_fading_count, bug_fading_count]\n",
        "  print(class_weights)\n",
        "  class_weights = class_weights[y]\n",
        "  weights = class_weights * fading_weights\n",
        "  return data.WeightedRandomSampler(weights=weights, num_samples=n_samples, replacement=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fh6O6Nz6uC51",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sampler = create_sampler(y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iP6L6GN22_bR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, y_train = torch.from_numpy(x_train), torch.from_numpy(y_train)\n",
        "x_val, y_val = torch.from_numpy(x_val), torch.from_numpy(y_val)\n",
        "x_test, y_test = torch.from_numpy(x_test), torch.from_numpy(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XnnrzRjL3IP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = data.TensorDataset(x_train, y_train)\n",
        "val_dataset = data.TensorDataset(x_val, y_val)\n",
        "test_dataset = data.TensorDataset(x_test, y_test)\n",
        "\n",
        "train_dataloader = data.DataLoader(train_dataset, batch_size=512, sampler=sampler)\n",
        "val_dataloader = data.DataLoader(val_dataset, batch_size=32)\n",
        "test_dataloader = data.DataLoader(test_dataset, batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHoY9-JcORo-",
        "colab_type": "code",
        "outputId": "7c983cc3-7cae-494d-c4ca-26060e5104f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "source": [
        "inputs, targets = next(iter(train_dataloader))\n",
        "print(inputs[:3])\n",
        "print(targets[:3])\n",
        "\n",
        "inputs, targets = next(iter(val_dataloader))\n",
        "print(inputs[:3])\n",
        "print(targets[:3])\n",
        "\n",
        "inputs, targets = next(iter(test_dataloader))\n",
        "print(inputs[:3])\n",
        "print(targets[:3])"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 1.5614, -0.3775, -0.2321,  0.4603,  2.8493, -0.0564, -0.0204, -0.6775,\n",
            "         -0.2215, -0.1103, -0.0030, -0.8162, -0.9000, -0.6141,  1.5614, -0.3775,\n",
            "         -0.2321,  0.4603,  2.8493, -0.0564, -0.0204, -0.6775, -0.2215, -0.1103,\n",
            "         -0.0030, -0.8162, -0.9000, -0.6141,  1.5614, -0.4154, -0.1895, -0.9443,\n",
            "         -0.1240, -0.2063],\n",
            "        [-0.6405, -0.3775, -0.2321, -0.1243, -0.6565, -0.0837, -0.0446, -0.5580,\n",
            "         -0.5381, -0.3358, -0.1333, -0.8185, -0.9382, -0.6163, -0.6405, -0.3775,\n",
            "         -0.2321, -0.1243, -0.6565, -0.0837, -0.0446, -0.5580, -0.5381, -0.3358,\n",
            "         -0.1333, -0.8185, -0.9382, -0.6163, -0.6405, -0.4154, -0.1895,  1.0590,\n",
            "         -0.1240, -0.2063],\n",
            "        [ 1.5614,  0.2370, -0.0878, -0.0853, -0.1632, -0.0479, -0.0462,  1.4457,\n",
            "          0.6931, -0.3876, -0.0030, -0.8119, -0.9021, -0.6134,  1.5614,  0.2370,\n",
            "         -0.0878, -0.0853, -0.1632, -0.0479, -0.0462,  1.4457,  0.6931, -0.3876,\n",
            "         -0.0030, -0.8119, -0.9021, -0.6134,  1.5614, -0.4154, -0.1895, -0.9443,\n",
            "         -0.1240, -0.2063]], dtype=torch.float64)\n",
            "tensor([0, 1, 1], dtype=torch.int8)\n",
            "tensor([[-0.6405,  3.9239,  0.7779,  0.1485,  1.7563, -0.0667, -0.0301, -0.5670,\n",
            "         -0.6436, -0.4108, -0.1333,  1.1548,  2.5859, -0.0071, -0.6405,  3.9239,\n",
            "          0.7779,  0.1485,  1.7563, -0.0667, -0.0301, -0.5670, -0.6436, -0.4108,\n",
            "         -0.1333,  1.1548,  2.5859, -0.0071, -0.6405, -0.4154, -0.1895,  1.0590,\n",
            "         -0.1240, -0.2063],\n",
            "        [-0.6405,  3.9239,  0.7779,  0.1485,  1.7563, -0.0667, -0.0301, -0.5670,\n",
            "         -0.6436, -0.3928,  0.3878,  1.1543,  2.5716, -0.0079, -0.6405,  3.9239,\n",
            "          0.7779,  0.1485,  1.7563, -0.0667, -0.0301, -0.5670, -0.6436, -0.3928,\n",
            "          0.3878,  1.1543,  2.5716, -0.0079, -0.6405, -0.4154, -0.1895,  1.0590,\n",
            "         -0.1240, -0.2063],\n",
            "        [-0.6405, -0.3775, -0.2321, -0.1243, -0.6565, -0.0837, -0.0462, -0.6118,\n",
            "         -0.7843, -0.4086, -0.1333,  1.1538,  2.5573, -0.5539, -0.6405, -0.3775,\n",
            "         -0.2321, -0.1243, -0.6565, -0.0837, -0.0462, -0.6118, -0.7843, -0.4086,\n",
            "         -0.1333,  1.1538,  2.5573, -0.5539, -0.6405, -0.4154, -0.1895,  1.0590,\n",
            "         -0.1240, -0.2063]], dtype=torch.float64)\n",
            "tensor([0, 0, 0], dtype=torch.int8)\n",
            "tensor([[-0.6405, -0.3775, -0.2321, -0.1243, -0.6565, -0.0837, -0.0462, -0.5042,\n",
            "         -0.7491, -0.4108, -0.1333,  0.2644,  2.4362,  0.8481, -0.6405, -0.3775,\n",
            "         -0.2321, -0.1243, -0.6565, -0.0837, -0.0462, -0.5042, -0.7491, -0.4108,\n",
            "         -0.1333,  0.2644,  2.4362,  0.8481, -0.6405, -0.4154, -0.1895,  1.0590,\n",
            "         -0.1240, -0.2063],\n",
            "        [-0.6405, -0.3775,  0.0565, -0.0074,  1.0428, -0.0684, -0.0414, -0.6356,\n",
            "         -0.7491, -0.4077,  0.2575,  0.2639,  2.4220,  0.8473, -0.6405, -0.3775,\n",
            "          0.0565, -0.0074,  1.0428, -0.0684, -0.0414, -0.6356, -0.7491, -0.4077,\n",
            "          0.2575,  0.2639,  2.4220,  0.8473, -0.6405,  2.4075, -0.1895, -0.9443,\n",
            "         -0.1240, -0.2063],\n",
            "        [-0.6405, -0.3775, -0.2321, -0.1243, -0.6565, -0.0769, -0.0414, -0.6864,\n",
            "         -0.7843, -0.3859, -0.1333,  0.2634,  2.4078,  0.8466, -0.6405, -0.3775,\n",
            "         -0.2321, -0.1243, -0.6565, -0.0769, -0.0414, -0.6864, -0.7843, -0.3859,\n",
            "         -0.1333,  0.2634,  2.4078,  0.8466, -0.6405, -0.4154,  5.2781, -0.9443,\n",
            "         -0.1240, -0.2063]], dtype=torch.float64)\n",
            "tensor([1, 1, 0], dtype=torch.int8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-GctbL8E1oA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Classifier(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, drop_prob, epoch=None, val_gmean=None):\n",
        "    super(Classifier, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.drop_prob = drop_prob\n",
        "    self.epoch = epoch\n",
        "    self.val_gmean = val_gmean\n",
        "    self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "    self.fcout = nn.Linear(hidden_size, 1)\n",
        "    self.dropout = nn.Dropout(drop_prob)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = torch.relu(self.fc1(x))\n",
        "    x = self.dropout(x)\n",
        "    x = torch.sigmoid(self.fcout(x))\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inpSePkGLTIs",
        "colab_type": "code",
        "outputId": "9531e67d-e27b-4283-c7d5-1adf69904c4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "classifier = Classifier(input_size=x.shape[1], hidden_size=x.shape[1], drop_prob=0.5)\n",
        "classifier"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Classifier(\n",
              "  (fc1): Linear(in_features=34, out_features=34, bias=True)\n",
              "  (fcout): Linear(in_features=34, out_features=1, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsT5eIrNoXAh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = 'classifier.cpt'\n",
        "def save_classifier(classifier):\n",
        "  checkpoint = {\n",
        "      'input_size': classifier.input_size,\n",
        "      'hidden_size': classifier.hidden_size,\n",
        "      'drop_prob': classifier.drop_prob,\n",
        "      'val_gmean': classifier.val_gmean,\n",
        "      'epoch': classifier.epoch,\n",
        "      'state_dict': classifier.state_dict()\n",
        "  }\n",
        "  with open(filename, 'wb') as f:\n",
        "    torch.save(checkpoint, f)\n",
        "\n",
        "def load_classifier():\n",
        "  with open(filename, 'rb') as f:\n",
        "    checkpoint = torch.load(f)\n",
        "  classifier = Classifier(checkpoint['input_size'], checkpoint['hidden_size'], checkpoint['drop_prob'], checkpoint['epoch'], checkpoint['val_gmean'])\n",
        "  classifier.load_state_dict(checkpoint['state_dict'])\n",
        "  return classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LX_eofKUq0ok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_classifier(classifier)\n",
        "classifier = load_classifier()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PR8s5ZA5rX-V",
        "colab_type": "code",
        "outputId": "0c31231a-e219-45ed-b9b7-7f2f050113d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  classifier = classifier.cuda()\n",
        "  inputs = inputs.cuda()\n",
        "\n",
        "classifier(inputs[:3].float())"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4010],\n",
              "        [0.4183],\n",
              "        [0.5323]], device='cuda:0', grad_fn=<SigmoidBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arYOr5lMLYy7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(params=classifier.parameters(), lr=0.003)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXw0R-Tkucgw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_loss(classifier, dataloader):\n",
        "  with torch.no_grad():\n",
        "    classifier.eval()\n",
        "    loss = 0\n",
        "    for inputs, targets in dataloader:\n",
        "      if torch.cuda.is_available():\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "      outputs = classifier(inputs.float())\n",
        "      batch_loss = criterion(outputs.squeeze(), targets.float())\n",
        "      loss += batch_loss.item()\n",
        "\n",
        "    return loss / len(dataloader)\n",
        "\n",
        "def calc_recalls(targets, predictions):\n",
        "  n_classes = 2\n",
        "  confusion_matrix, _, _ = np.histogram2d(targets.detach().cpu().numpy(), predictions.squeeze().detach().cpu().numpy(), bins=n_classes)\n",
        "  recalls = np.diag(confusion_matrix) / ((np.sum(confusion_matrix, axis=1)) + 1e-12)\n",
        "  return recalls\n",
        "\n",
        "def calc_gmean(classifier, dataloader):\n",
        "  with torch.no_grad():\n",
        "    classifier.eval()\n",
        "    recalls = np.zeros((2))    \n",
        "    for inputs, targets in dataloader:\n",
        "      if torch.cuda.is_available():\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "      \n",
        "      outputs = classifier(inputs.float())\n",
        "      predictions = torch.round(outputs).int()\n",
        "      recalls += calc_recalls(targets, predictions)\n",
        "    recalls = recalls / len(dataloader)\n",
        "    return mstats.gmean(recalls), recalls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFy03p96Lza1",
        "colab_type": "code",
        "outputId": "2dad34ed-eb32-454b-aa31-0d1028670603",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_loss = 0\n",
        "epochs = 1000\n",
        "for epoch in range(epochs):\n",
        "  classifier.train()\n",
        "  for inputs, targets in train_dataloader:\n",
        "    if torch.cuda.is_available():\n",
        "      inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "    outputs = classifier(inputs.float())\n",
        "    loss = criterion(outputs.squeeze(), targets.float())\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  train_loss = train_loss / len(train_dataloader)\n",
        "  val_loss = calc_loss(classifier, val_dataloader)\n",
        "  val_gmean, _ = calc_gmean(classifier, val_dataloader)\n",
        "  print('Epoch: {}, Train loss: {}, Val loss: {}, Val g-mean: {}'.format(epoch, train_loss, val_loss, val_gmean))\n",
        "\n",
        "  if classifier.val_gmean is None or val_gmean > classifier.val_gmean:\n",
        "    classifier.epoch = epoch\n",
        "    classifier.val_gmean = val_gmean\n",
        "    save_classifier(classifier)\n",
        "\n",
        "classifier.epoch = epoch\n",
        "classifier.val_gmean = val_gmean"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Train loss: 0.648464770693528, Val loss: 0.5896124353534297, Val g-mean: 0.6679116732806358\n",
            "Epoch: 1, Train loss: 0.6245662609626051, Val loss: 0.5772864379380879, Val g-mean: 0.682601652261799\n",
            "Epoch: 2, Train loss: 0.6084444916036286, Val loss: 0.5856328116435754, Val g-mean: 0.6902046500564137\n",
            "Epoch: 3, Train loss: 0.5988640685766061, Val loss: 0.5685115684019891, Val g-mean: 0.6978068294641718\n",
            "Epoch: 4, Train loss: 0.5904980848230007, Val loss: 0.5879404697763292, Val g-mean: 0.6981276579428864\n",
            "Epoch: 5, Train loss: 0.5835535728879065, Val loss: 0.589929667350493, Val g-mean: 0.6979893798586597\n",
            "Epoch: 6, Train loss: 0.5717597687556747, Val loss: 0.5478033517536364, Val g-mean: 0.6908489920239729\n",
            "Epoch: 7, Train loss: 0.5722346574703706, Val loss: 0.534875039207308, Val g-mean: 0.702254411066916\n",
            "Epoch: 8, Train loss: 0.5690819549734129, Val loss: 0.5374566395031778, Val g-mean: 0.7205438569003801\n",
            "Epoch: 9, Train loss: 0.5650474223342243, Val loss: 0.5095354698990521, Val g-mean: 0.7064865966887809\n",
            "Epoch: 10, Train loss: 0.5606829832333396, Val loss: 0.5168215969675466, Val g-mean: 0.7198828688450362\n",
            "Epoch: 11, Train loss: 0.5563262928743584, Val loss: 0.49774390851196487, Val g-mean: 0.707644374341943\n",
            "Epoch: 12, Train loss: 0.5545154000301619, Val loss: 0.512889529137235, Val g-mean: 0.7103086758828325\n",
            "Epoch: 13, Train loss: 0.5481975818019673, Val loss: 0.5026468850280109, Val g-mean: 0.7145565371279958\n",
            "Epoch: 14, Train loss: 0.548288841373935, Val loss: 0.5034572999728354, Val g-mean: 0.7248318982671883\n",
            "Epoch: 15, Train loss: 0.5469545350609959, Val loss: 0.5228166313547837, Val g-mean: 0.7348965516998638\n",
            "Epoch: 16, Train loss: 0.552867980970988, Val loss: 0.4910878931221209, Val g-mean: 0.728106677316108\n",
            "Epoch: 17, Train loss: 0.5421828816379907, Val loss: 0.5026885216173372, Val g-mean: 0.7240829960833977\n",
            "Epoch: 18, Train loss: 0.5452680631950388, Val loss: 0.478852474375775, Val g-mean: 0.7327446983091469\n",
            "Epoch: 19, Train loss: 0.5397133804844003, Val loss: 0.48354159217131765, Val g-mean: 0.7265447606056408\n",
            "Epoch: 20, Train loss: 0.5453187407043059, Val loss: 0.4856804103443497, Val g-mean: 0.7324587457973355\n",
            "Epoch: 21, Train loss: 0.536786548874322, Val loss: 0.46444914097848694, Val g-mean: 0.7263150597127265\n",
            "Epoch: 22, Train loss: 0.5381911418569073, Val loss: 0.4951377013011983, Val g-mean: 0.7319025391566836\n",
            "Epoch: 23, Train loss: 0.5342398976592954, Val loss: 0.47332871313157837, Val g-mean: 0.729618680127662\n",
            "Epoch: 24, Train loss: 0.5393902716806812, Val loss: 0.46628700235956594, Val g-mean: 0.7279526669283901\n",
            "Epoch: 25, Train loss: 0.5290335622214729, Val loss: 0.48099639678471967, Val g-mean: 0.7256069400057902\n",
            "Epoch: 26, Train loss: 0.5385387090391847, Val loss: 0.5071383228427485, Val g-mean: 0.7435709987274246\n",
            "Epoch: 27, Train loss: 0.5326990551790306, Val loss: 0.47932928957437215, Val g-mean: 0.731808265460026\n",
            "Epoch: 28, Train loss: 0.5265616697884461, Val loss: 0.4647646909089465, Val g-mean: 0.7319352286489512\n",
            "Epoch: 29, Train loss: 0.5319625450927356, Val loss: 0.5077318690325084, Val g-mean: 0.7443919209716182\n",
            "Epoch: 30, Train loss: 0.5334862494514656, Val loss: 0.4749953042911856, Val g-mean: 0.7363842822801211\n",
            "Epoch: 31, Train loss: 0.5302841169898088, Val loss: 0.5001501417473743, Val g-mean: 0.7489765365138293\n",
            "Epoch: 32, Train loss: 0.53251743265219, Val loss: 0.4463639922047916, Val g-mean: 0.7387885136041644\n",
            "Epoch: 33, Train loss: 0.529273797793406, Val loss: 0.4729550343595053, Val g-mean: 0.7287162434741529\n",
            "Epoch: 34, Train loss: 0.5317519657372178, Val loss: 0.46750431586253016, Val g-mean: 0.7428576387364402\n",
            "Epoch: 35, Train loss: 0.5317180989409716, Val loss: 0.45053568639253316, Val g-mean: 0.7364951143592907\n",
            "Epoch: 36, Train loss: 0.5266197643735566, Val loss: 0.4602405813179518, Val g-mean: 0.7361379216972426\n",
            "Epoch: 37, Train loss: 0.5172469436130465, Val loss: 0.4572382518335393, Val g-mean: 0.7312284999151702\n",
            "Epoch: 38, Train loss: 0.5223051140156795, Val loss: 0.47714834856359584, Val g-mean: 0.744934510584324\n",
            "Epoch: 39, Train loss: 0.521095844814253, Val loss: 0.4611190212400336, Val g-mean: 0.7337863752535443\n",
            "Epoch: 40, Train loss: 0.526550228729297, Val loss: 0.4989221405826117, Val g-mean: 0.7425686913530095\n",
            "Epoch: 41, Train loss: 0.5232371663141264, Val loss: 0.479713320143913, Val g-mean: 0.7412830008601603\n",
            "Epoch: 42, Train loss: 0.5268192863126805, Val loss: 0.4825082448752303, Val g-mean: 0.7533423869171373\n",
            "Epoch: 43, Train loss: 0.5198304919183783, Val loss: 0.46674079879334096, Val g-mean: 0.747410331355043\n",
            "Epoch: 44, Train loss: 0.5181946345792019, Val loss: 0.46243653368008764, Val g-mean: 0.7300782895294627\n",
            "Epoch: 45, Train loss: 0.5229743496593305, Val loss: 0.47888702173766334, Val g-mean: 0.7530629179296907\n",
            "Epoch: 46, Train loss: 0.5256708415349755, Val loss: 0.4521504868018, Val g-mean: 0.7381500113894617\n",
            "Epoch: 47, Train loss: 0.5258102734674498, Val loss: 0.46969877948102196, Val g-mean: 0.7404175907396866\n",
            "Epoch: 48, Train loss: 0.5193983073814644, Val loss: 0.46456314623355865, Val g-mean: 0.7507342167623143\n",
            "Epoch: 49, Train loss: 0.5242516682225987, Val loss: 0.4516194850990647, Val g-mean: 0.7307423200429924\n",
            "Epoch: 50, Train loss: 0.5211064854360424, Val loss: 0.4518307957210039, Val g-mean: 0.7449085874865741\n",
            "Epoch: 51, Train loss: 0.5293628535226166, Val loss: 0.4439416339522914, Val g-mean: 0.7429511102128477\n",
            "Epoch: 52, Train loss: 0.5258114799636372, Val loss: 0.47944683424736323, Val g-mean: 0.7540810237549687\n",
            "Epoch: 53, Train loss: 0.5207691526328727, Val loss: 0.4489230056734462, Val g-mean: 0.7344537685885573\n",
            "Epoch: 54, Train loss: 0.5189100551916717, Val loss: 0.46704020723700523, Val g-mean: 0.742747672591275\n",
            "Epoch: 55, Train loss: 0.5195837657397085, Val loss: 0.4523813810787703, Val g-mean: 0.7466191400419149\n",
            "Epoch: 56, Train loss: 0.5159235801632601, Val loss: 0.4395708182924672, Val g-mean: 0.7370660742147284\n",
            "Epoch: 57, Train loss: 0.5229599379299382, Val loss: 0.44360959823978574, Val g-mean: 0.7395608924258038\n",
            "Epoch: 58, Train loss: 0.5167313858678423, Val loss: 0.4505735445571573, Val g-mean: 0.7381408484600248\n",
            "Epoch: 59, Train loss: 0.524658475384405, Val loss: 0.44242714796411364, Val g-mean: 0.7488969634987281\n",
            "Epoch: 60, Train loss: 0.5173420946343241, Val loss: 0.4513528141144075, Val g-mean: 0.7479680849491328\n",
            "Epoch: 61, Train loss: 0.5150878555956208, Val loss: 0.4713577012482442, Val g-mean: 0.7561722621076038\n",
            "Epoch: 62, Train loss: 0.513404859857033, Val loss: 0.4675331241206119, Val g-mean: 0.7483944389736737\n",
            "Epoch: 63, Train loss: 0.5131394325577446, Val loss: 0.45274346576709495, Val g-mean: 0.7470907868637188\n",
            "Epoch: 64, Train loss: 0.5149310930749911, Val loss: 0.4539357149287274, Val g-mean: 0.7523707856147003\n",
            "Epoch: 65, Train loss: 0.5174012900553805, Val loss: 0.4783187673280114, Val g-mean: 0.7476201539833761\n",
            "Epoch: 66, Train loss: 0.5158599798647006, Val loss: 0.43853929187906415, Val g-mean: 0.7498517238049621\n",
            "Epoch: 67, Train loss: 0.5195469343473186, Val loss: 0.45031175370279114, Val g-mean: 0.743003238410951\n",
            "Epoch: 68, Train loss: 0.5144953314249182, Val loss: 0.46772207025634616, Val g-mean: 0.7517496509277812\n",
            "Epoch: 69, Train loss: 0.5119696930934057, Val loss: 0.44877725584726585, Val g-mean: 0.7502861233472503\n",
            "Epoch: 70, Train loss: 0.5160514306496347, Val loss: 0.4607040027254506, Val g-mean: 0.7499216775026278\n",
            "Epoch: 71, Train loss: 0.5104420629286733, Val loss: 0.45139653235673904, Val g-mean: 0.7547125975494386\n",
            "Epoch: 72, Train loss: 0.5129534229561529, Val loss: 0.4527568201485433, Val g-mean: 0.7529529450739553\n",
            "Epoch: 73, Train loss: 0.5141338241333627, Val loss: 0.4342009256544866, Val g-mean: 0.7522666027908632\n",
            "Epoch: 74, Train loss: 0.5163074390204195, Val loss: 0.4379678845013443, Val g-mean: 0.7601295687003434\n",
            "Epoch: 75, Train loss: 0.5116997618845457, Val loss: 0.44836305356339407, Val g-mean: 0.7567754924627298\n",
            "Epoch: 76, Train loss: 0.5174951591496179, Val loss: 0.44749640379297106, Val g-mean: 0.7565266994423614\n",
            "Epoch: 77, Train loss: 0.5108557485655852, Val loss: 0.44836580910180746, Val g-mean: 0.7470363381296521\n",
            "Epoch: 78, Train loss: 0.5140539348729763, Val loss: 0.45739750093535375, Val g-mean: 0.7510004355734932\n",
            "Epoch: 79, Train loss: 0.5156072251122467, Val loss: 0.4374161644985801, Val g-mean: 0.7518663713925388\n",
            "Epoch: 80, Train loss: 0.5133919516470382, Val loss: 0.44843282668214096, Val g-mean: 0.7446864907056134\n",
            "Epoch: 81, Train loss: 0.512955450957455, Val loss: 0.44703760409825727, Val g-mean: 0.760910428157019\n",
            "Epoch: 82, Train loss: 0.5123805974644903, Val loss: 0.4431482694650951, Val g-mean: 0.7550619520037462\n",
            "Epoch: 83, Train loss: 0.5118446231796095, Val loss: 0.44103069917151805, Val g-mean: 0.7485599562229567\n",
            "Epoch: 84, Train loss: 0.5076576421159238, Val loss: 0.4279511241536391, Val g-mean: 0.7483150398240676\n",
            "Epoch: 85, Train loss: 0.5126048819263194, Val loss: 0.45829720344198377, Val g-mean: 0.7581573193843925\n",
            "Epoch: 86, Train loss: 0.5145221075364357, Val loss: 0.4519083443048753, Val g-mean: 0.7489522943064155\n",
            "Epoch: 87, Train loss: 0.5190549548282655, Val loss: 0.44115384001480906, Val g-mean: 0.7649652486792963\n",
            "Epoch: 88, Train loss: 0.5066854249380514, Val loss: 0.4416109980329087, Val g-mean: 0.75375654641053\n",
            "Epoch: 89, Train loss: 0.5134799618756104, Val loss: 0.4802712421668203, Val g-mean: 0.744636982968393\n",
            "Epoch: 90, Train loss: 0.5106106286489005, Val loss: 0.4493585401459744, Val g-mean: 0.7452992902124004\n",
            "Epoch: 91, Train loss: 0.5099279140308317, Val loss: 0.4256189900793527, Val g-mean: 0.7522210304731517\n",
            "Epoch: 92, Train loss: 0.5106021475762517, Val loss: 0.42280841344281245, Val g-mean: 0.7574096492454979\n",
            "Epoch: 93, Train loss: 0.5095415703873111, Val loss: 0.462209815257474, Val g-mean: 0.7544715812294439\n",
            "Epoch: 94, Train loss: 0.51201499055462, Val loss: 0.44474936002179194, Val g-mean: 0.7485062282907206\n",
            "Epoch: 95, Train loss: 0.5111359516127687, Val loss: 0.4473490124862445, Val g-mean: 0.7603895038051462\n",
            "Epoch: 96, Train loss: 0.5042160534004992, Val loss: 0.4332676069124749, Val g-mean: 0.7619465902557716\n",
            "Epoch: 97, Train loss: 0.508796110966594, Val loss: 0.43167827337195996, Val g-mean: 0.7554855761835693\n",
            "Epoch: 98, Train loss: 0.5107987001595473, Val loss: 0.47022465224328797, Val g-mean: 0.7581307613948448\n",
            "Epoch: 99, Train loss: 0.5087061800848434, Val loss: 0.43312573432922363, Val g-mean: 0.7643515323227286\n",
            "Epoch: 100, Train loss: 0.5086383199437956, Val loss: 0.480231074518279, Val g-mean: 0.7566041969431417\n",
            "Epoch: 101, Train loss: 0.5147883028726423, Val loss: 0.4300394744465226, Val g-mean: 0.74348946839552\n",
            "Epoch: 102, Train loss: 0.5019762391347128, Val loss: 0.43683391691822754, Val g-mean: 0.7590469194420684\n",
            "Epoch: 103, Train loss: 0.5008836189239131, Val loss: 0.4469650442662992, Val g-mean: 0.7611460452135302\n",
            "Epoch: 104, Train loss: 0.5125194037675045, Val loss: 0.4490293750637456, Val g-mean: 0.7468167504014448\n",
            "Epoch: 105, Train loss: 0.5079551413517228, Val loss: 0.44168614654948835, Val g-mean: 0.7442726279964579\n",
            "Epoch: 106, Train loss: 0.5060682730059378, Val loss: 0.4398791374344575, Val g-mean: 0.7616765239793879\n",
            "Epoch: 107, Train loss: 0.5095156097521398, Val loss: 0.45916901332767385, Val g-mean: 0.7557949689357392\n",
            "Epoch: 108, Train loss: 0.502236774783673, Val loss: 0.4497427934486615, Val g-mean: 0.7621839616504121\n",
            "Epoch: 109, Train loss: 0.5118730771756315, Val loss: 0.4310951944636671, Val g-mean: 0.762039236256448\n",
            "Epoch: 110, Train loss: 0.5072225405673608, Val loss: 0.4283029913510147, Val g-mean: 0.7635332434099772\n",
            "Epoch: 111, Train loss: 0.5062509102620065, Val loss: 0.4415866446338202, Val g-mean: 0.7482268378931214\n",
            "Epoch: 112, Train loss: 0.5047415547804807, Val loss: 0.44238779517380816, Val g-mean: 0.7488778089569154\n",
            "Epoch: 113, Train loss: 0.5060736765810188, Val loss: 0.44647291067399475, Val g-mean: 0.7474136565128049\n",
            "Epoch: 114, Train loss: 0.5080071638821204, Val loss: 0.4277228316978404, Val g-mean: 0.7530999909039161\n",
            "Epoch: 115, Train loss: 0.5117728661832021, Val loss: 0.4494065289434634, Val g-mean: 0.7500016329970158\n",
            "Epoch: 116, Train loss: 0.5029789372790717, Val loss: 0.45342142232938815, Val g-mean: 0.7583428684446412\n",
            "Epoch: 117, Train loss: 0.5077539310238572, Val loss: 0.43526459426472064, Val g-mean: 0.7596512283116298\n",
            "Epoch: 118, Train loss: 0.5048311648802324, Val loss: 0.44447699346040426, Val g-mean: 0.7533003087099539\n",
            "Epoch: 119, Train loss: 0.5083799213119672, Val loss: 0.415779379245482, Val g-mean: 0.7506728564030065\n",
            "Epoch: 120, Train loss: 0.5019878443133108, Val loss: 0.43479169454229505, Val g-mean: 0.7496466860466953\n",
            "Epoch: 121, Train loss: 0.5108933229092215, Val loss: 0.44161301733631836, Val g-mean: 0.753045203994276\n",
            "Epoch: 122, Train loss: 0.5079979569212477, Val loss: 0.445900826469848, Val g-mean: 0.7496506837133415\n",
            "Epoch: 123, Train loss: 0.5043164310499657, Val loss: 0.44439639464805003, Val g-mean: 0.7502622546501225\n",
            "Epoch: 124, Train loss: 0.5129411333424079, Val loss: 0.4527238725047362, Val g-mean: 0.7631474461310138\n",
            "Epoch: 125, Train loss: 0.5066128132992205, Val loss: 0.4157797305991775, Val g-mean: 0.7547420258384147\n",
            "Epoch: 126, Train loss: 0.5017411929950725, Val loss: 0.4459720234337606, Val g-mean: 0.7566071096224389\n",
            "Epoch: 127, Train loss: 0.5050487547657616, Val loss: 0.4375107053079103, Val g-mean: 0.7514040578392754\n",
            "Epoch: 128, Train loss: 0.5038875993133158, Val loss: 0.44744391131557915, Val g-mean: 0.7547723626118963\n",
            "Epoch: 129, Train loss: 0.5070971808831958, Val loss: 0.4478595346996659, Val g-mean: 0.7554387191332634\n",
            "Epoch: 130, Train loss: 0.5048716435882111, Val loss: 0.44310929645833214, Val g-mean: 0.7489251938331395\n",
            "Epoch: 131, Train loss: 0.5079896828324456, Val loss: 0.4477272523861182, Val g-mean: 0.7506746965007662\n",
            "Epoch: 132, Train loss: 0.5080726156652781, Val loss: 0.4527635327295253, Val g-mean: 0.7576636224704401\n",
            "Epoch: 133, Train loss: 0.5028488487267202, Val loss: 0.4492418379767945, Val g-mean: 0.7596854538039943\n",
            "Epoch: 134, Train loss: 0.5111695257463573, Val loss: 0.4299368234841447, Val g-mean: 0.7610336694858149\n",
            "Epoch: 135, Train loss: 0.5043675680632093, Val loss: 0.41947786588417857, Val g-mean: 0.7599205503624168\n",
            "Epoch: 136, Train loss: 0.5039545033053209, Val loss: 0.46629100567416143, Val g-mean: 0.7473674143369247\n",
            "Epoch: 137, Train loss: 0.5051538207909404, Val loss: 0.43688270979021726, Val g-mean: 0.7517335913214723\n",
            "Epoch: 138, Train loss: 0.5066527555877204, Val loss: 0.4527438377079211, Val g-mean: 0.7632567175822501\n",
            "Epoch: 139, Train loss: 0.5108981237874068, Val loss: 0.4124982953071594, Val g-mean: 0.7622677511973099\n",
            "Epoch: 140, Train loss: 0.5040812675525892, Val loss: 0.4181659890240745, Val g-mean: 0.7590866595903492\n",
            "Epoch: 141, Train loss: 0.5027241437172946, Val loss: 0.43144596701389865, Val g-mean: 0.7637239807436722\n",
            "Epoch: 142, Train loss: 0.5079041547642885, Val loss: 0.44605242323718575, Val g-mean: 0.7547776392859458\n",
            "Epoch: 143, Train loss: 0.5050723250159745, Val loss: 0.43951003175032766, Val g-mean: 0.7573142188016742\n",
            "Epoch: 144, Train loss: 0.5080533490250223, Val loss: 0.4389050040198, Val g-mean: 0.7517292306128078\n",
            "Epoch: 145, Train loss: 0.5084695498819668, Val loss: 0.42563071905782346, Val g-mean: 0.761382007197172\n",
            "Epoch: 146, Train loss: 0.5085605609852899, Val loss: 0.4430587526999022, Val g-mean: 0.7505626607790545\n",
            "Epoch: 147, Train loss: 0.5043540257854389, Val loss: 0.45078892143149124, Val g-mean: 0.7565307682343954\n",
            "Epoch: 148, Train loss: 0.49953981595772173, Val loss: 0.4199683287818181, Val g-mean: 0.7656029066480404\n",
            "Epoch: 149, Train loss: 0.5058241814263917, Val loss: 0.43740310755215195, Val g-mean: 0.7571167455377087\n",
            "Epoch: 150, Train loss: 0.5021922698927769, Val loss: 0.44483515208489016, Val g-mean: 0.7483714596980269\n",
            "Epoch: 151, Train loss: 0.5028646834359694, Val loss: 0.422248208209088, Val g-mean: 0.7547604700365739\n",
            "Epoch: 152, Train loss: 0.5126321259498233, Val loss: 0.41598462803583397, Val g-mean: 0.7625649264661342\n",
            "Epoch: 153, Train loss: 0.5043693379439795, Val loss: 0.4386173420046505, Val g-mean: 0.7494372793453962\n",
            "Epoch: 154, Train loss: 0.5007912121736111, Val loss: 0.420091397668186, Val g-mean: 0.7671733076235766\n",
            "Epoch: 155, Train loss: 0.5067650547741879, Val loss: 0.4331892368040587, Val g-mean: 0.7582980738874485\n",
            "Epoch: 156, Train loss: 0.500145991829192, Val loss: 0.44161325418635416, Val g-mean: 0.7682634962884044\n",
            "Epoch: 157, Train loss: 0.5038620787803321, Val loss: 0.4604521825125343, Val g-mean: 0.7559631477626542\n",
            "Epoch: 158, Train loss: 0.5027437836400876, Val loss: 0.4242654980013245, Val g-mean: 0.7575175807111875\n",
            "Epoch: 159, Train loss: 0.5044978418644902, Val loss: 0.42530147221527603, Val g-mean: 0.7586046739167793\n",
            "Epoch: 160, Train loss: 0.5037043608560287, Val loss: 0.419908009665577, Val g-mean: 0.7668017143402138\n",
            "Epoch: 161, Train loss: 0.5033055361355975, Val loss: 0.44226526821914475, Val g-mean: 0.75931974318279\n",
            "Epoch: 162, Train loss: 0.5064787655970654, Val loss: 0.4380192850765429, Val g-mean: 0.7593259151544324\n",
            "Epoch: 163, Train loss: 0.5100709837515658, Val loss: 0.42016664774794327, Val g-mean: 0.7617216058134708\n",
            "Epoch: 164, Train loss: 0.5025220953629715, Val loss: 0.43298993534163427, Val g-mean: 0.7659477740084409\n",
            "Epoch: 165, Train loss: 0.5073226882261307, Val loss: 0.43704304118689735, Val g-mean: 0.7578053428423801\n",
            "Epoch: 166, Train loss: 0.5040049659924609, Val loss: 0.42928796045874296, Val g-mean: 0.7616838111983179\n",
            "Epoch: 167, Train loss: 0.5037245031713313, Val loss: 0.4233042719332795, Val g-mean: 0.7697150939783461\n",
            "Epoch: 168, Train loss: 0.5020121513137662, Val loss: 0.42555115273908567, Val g-mean: 0.7652049948746359\n",
            "Epoch: 169, Train loss: 0.5073576221296908, Val loss: 0.4466203388414885, Val g-mean: 0.7617626971169253\n",
            "Epoch: 170, Train loss: 0.5004877870266629, Val loss: 0.4406693028776269, Val g-mean: 0.7526253046305312\n",
            "Epoch: 171, Train loss: 0.5037832498680735, Val loss: 0.4467694894655755, Val g-mean: 0.7587160015199497\n",
            "Epoch: 172, Train loss: 0.5010233080704253, Val loss: 0.4503224807742395, Val g-mean: 0.7530660421271962\n",
            "Epoch: 173, Train loss: 0.5063600073684706, Val loss: 0.44111983830991547, Val g-mean: 0.7659187009746256\n",
            "Epoch: 174, Train loss: 0.498726638080858, Val loss: 0.41820133281381505, Val g-mean: 0.7517162620888561\n",
            "Epoch: 175, Train loss: 0.5138558769414869, Val loss: 0.416561517276262, Val g-mean: 0.7611886276455173\n",
            "Epoch: 176, Train loss: 0.5089869609636959, Val loss: 0.43294904557497876, Val g-mean: 0.748286814430048\n",
            "Epoch: 177, Train loss: 0.5092475825003441, Val loss: 0.43130723073294286, Val g-mean: 0.7638901994673684\n",
            "Epoch: 178, Train loss: 0.5068872534743831, Val loss: 0.42752330397304733, Val g-mean: 0.7594170762255776\n",
            "Epoch: 179, Train loss: 0.5119303158069699, Val loss: 0.4410205214823547, Val g-mean: 0.7548633469218565\n",
            "Epoch: 180, Train loss: 0.5141968711194894, Val loss: 0.44723159643380267, Val g-mean: 0.7598672527745346\n",
            "Epoch: 181, Train loss: 0.5014381204361913, Val loss: 0.4425475187599659, Val g-mean: 0.763962077022722\n",
            "Epoch: 182, Train loss: 0.5068205785707298, Val loss: 0.41939910324780566, Val g-mean: 0.7648842873913896\n",
            "Epoch: 183, Train loss: 0.5011861682250721, Val loss: 0.4358384599419017, Val g-mean: 0.7565574476604322\n",
            "Epoch: 184, Train loss: 0.49996670155231004, Val loss: 0.425832045509627, Val g-mean: 0.7669738865444395\n",
            "Epoch: 185, Train loss: 0.5076016506784067, Val loss: 0.4684428264828105, Val g-mean: 0.7501183774809259\n",
            "Epoch: 186, Train loss: 0.510815489504507, Val loss: 0.42930975987722997, Val g-mean: 0.7668503016746432\n",
            "Epoch: 187, Train loss: 0.5004102733630301, Val loss: 0.44207535566467987, Val g-mean: 0.7607153459253627\n",
            "Epoch: 188, Train loss: 0.5009068011372193, Val loss: 0.45085510846815613, Val g-mean: 0.7592362830072306\n",
            "Epoch: 189, Train loss: 0.5039348909706632, Val loss: 0.43480789582980306, Val g-mean: 0.7691898583858437\n",
            "Epoch: 190, Train loss: 0.508032282741056, Val loss: 0.44224850656954867, Val g-mean: 0.749267189742651\n",
            "Epoch: 191, Train loss: 0.4947261886259753, Val loss: 0.44454539175096314, Val g-mean: 0.7579289378349607\n",
            "Epoch: 192, Train loss: 0.4994217198576343, Val loss: 0.4082935621079646, Val g-mean: 0.763625893860899\n",
            "Epoch: 193, Train loss: 0.5010888028601957, Val loss: 0.42777169358573464, Val g-mean: 0.7495720795654712\n",
            "Epoch: 194, Train loss: 0.49923787788461216, Val loss: 0.4209101698116252, Val g-mean: 0.762064936029446\n",
            "Epoch: 195, Train loss: 0.49682608750721613, Val loss: 0.4271817540651874, Val g-mean: 0.7671936313271028\n",
            "Epoch: 196, Train loss: 0.506611021858365, Val loss: 0.4344890052942853, Val g-mean: 0.7702867706776856\n",
            "Epoch: 197, Train loss: 0.49967140985429054, Val loss: 0.4371383690128201, Val g-mean: 0.7557210880931761\n",
            "Epoch: 198, Train loss: 0.5130185809137789, Val loss: 0.4290887184048954, Val g-mean: 0.7552139685786247\n",
            "Epoch: 199, Train loss: 0.5062031613274687, Val loss: 0.41513383133631004, Val g-mean: 0.7598896745601551\n",
            "Epoch: 200, Train loss: 0.5055121736737832, Val loss: 0.4458289415036377, Val g-mean: 0.7622640037309067\n",
            "Epoch: 201, Train loss: 0.5024314866698958, Val loss: 0.44106287077853557, Val g-mean: 0.7559486755959899\n",
            "Epoch: 202, Train loss: 0.5017752860361996, Val loss: 0.4268229805717343, Val g-mean: 0.7493598796131568\n",
            "Epoch: 203, Train loss: 0.5049536147514316, Val loss: 0.43391227624134016, Val g-mean: 0.7592947643829216\n",
            "Epoch: 204, Train loss: 0.4998437626811479, Val loss: 0.42817717339647443, Val g-mean: 0.7609567460607075\n",
            "Epoch: 205, Train loss: 0.5021156147479295, Val loss: 0.40721497057299866, Val g-mean: 0.7612245274555859\n",
            "Epoch: 206, Train loss: 0.5027500887067675, Val loss: 0.4230938147949545, Val g-mean: 0.7619994271513872\n",
            "Epoch: 207, Train loss: 0.5010773638867275, Val loss: 0.45891682980091947, Val g-mean: 0.7580817614650704\n",
            "Epoch: 208, Train loss: 0.5033254861308372, Val loss: 0.46192435332034765, Val g-mean: 0.7553546845398079\n",
            "Epoch: 209, Train loss: 0.5059787268687582, Val loss: 0.44158303463145304, Val g-mean: 0.7503331169565105\n",
            "Epoch: 210, Train loss: 0.5005150804269975, Val loss: 0.4246011743027913, Val g-mean: 0.756547785201624\n",
            "Epoch: 211, Train loss: 0.5069363942516181, Val loss: 0.463046240767366, Val g-mean: 0.7564973034276649\n",
            "Epoch: 212, Train loss: 0.5054796838205434, Val loss: 0.4331471963148368, Val g-mean: 0.7570470324497437\n",
            "Epoch: 213, Train loss: 0.4954375943683777, Val loss: 0.44378629483674703, Val g-mean: 0.7667628591219378\n",
            "Epoch: 214, Train loss: 0.5080300483817154, Val loss: 0.4288961942258634, Val g-mean: 0.7673678652201255\n",
            "Epoch: 215, Train loss: 0.500167702148134, Val loss: 0.4338819114001174, Val g-mean: 0.7623964783742057\n",
            "Epoch: 216, Train loss: 0.5034517701913854, Val loss: 0.45159997398916046, Val g-mean: 0.7511837710848975\n",
            "Epoch: 217, Train loss: 0.5046324582383558, Val loss: 0.4220049622419633, Val g-mean: 0.7676071797995437\n",
            "Epoch: 218, Train loss: 0.5024562372227654, Val loss: 0.44098575824969694, Val g-mean: 0.7666467651814368\n",
            "Epoch: 219, Train loss: 0.49884816584215913, Val loss: 0.4352760071817197, Val g-mean: 0.7528539578769791\n",
            "Epoch: 220, Train loss: 0.49731380363755745, Val loss: 0.42747673725611285, Val g-mean: 0.764997854420632\n",
            "Epoch: 221, Train loss: 0.5041458671083715, Val loss: 0.45789916832980354, Val g-mean: 0.7541017809296575\n",
            "Epoch: 222, Train loss: 0.5014388806066771, Val loss: 0.4321661656232257, Val g-mean: 0.7623291997162864\n",
            "Epoch: 223, Train loss: 0.4995378975973889, Val loss: 0.43505835297860596, Val g-mean: 0.7711267721808905\n",
            "Epoch: 224, Train loss: 0.5041193957519802, Val loss: 0.44001188599749613, Val g-mean: 0.7666784758478088\n",
            "Epoch: 225, Train loss: 0.4992859332064585, Val loss: 0.4516864252325736, Val g-mean: 0.7512384974729966\n",
            "Epoch: 226, Train loss: 0.5029882293738289, Val loss: 0.413882689452485, Val g-mean: 0.7633001276058828\n",
            "Epoch: 227, Train loss: 0.49993030910376224, Val loss: 0.4302041701188213, Val g-mean: 0.7498528632899051\n",
            "Epoch: 228, Train loss: 0.49733474502560426, Val loss: 0.4214837092318033, Val g-mean: 0.7545024117808643\n",
            "Epoch: 229, Train loss: 0.503348277496036, Val loss: 0.4272946762411218, Val g-mean: 0.7555000899780563\n",
            "Epoch: 230, Train loss: 0.4940163003375326, Val loss: 0.43523089646508817, Val g-mean: 0.7660382835988093\n",
            "Epoch: 231, Train loss: 0.49898205803928203, Val loss: 0.41835292232664006, Val g-mean: 0.7686782137726162\n",
            "Epoch: 232, Train loss: 0.5012167766970232, Val loss: 0.4233851491620666, Val g-mean: 0.7614932952903493\n",
            "Epoch: 233, Train loss: 0.4980868510909296, Val loss: 0.41805551965769966, Val g-mean: 0.7684733001931253\n",
            "Epoch: 234, Train loss: 0.50393208341773, Val loss: 0.4425993163726832, Val g-mean: 0.7571854364323195\n",
            "Epoch: 235, Train loss: 0.49629153924996805, Val loss: 0.4293185011728814, Val g-mean: 0.7718897468824886\n",
            "Epoch: 236, Train loss: 0.49487642450233693, Val loss: 0.4434603168384025, Val g-mean: 0.7575175257469062\n",
            "Epoch: 237, Train loss: 0.5022749307393767, Val loss: 0.44676208221598673, Val g-mean: 0.7738777521062908\n",
            "Epoch: 238, Train loss: 0.502166239845752, Val loss: 0.4110945621995549, Val g-mean: 0.75791475368123\n",
            "Epoch: 239, Train loss: 0.50348601446936, Val loss: 0.4295959588336317, Val g-mean: 0.7649003339407442\n",
            "Epoch: 240, Train loss: 0.4994260984309905, Val loss: 0.42848596251324605, Val g-mean: 0.7646736340299454\n",
            "Epoch: 241, Train loss: 0.5075072599546125, Val loss: 0.42876166654260534, Val g-mean: 0.7508675971475944\n",
            "Epoch: 242, Train loss: 0.5025975891192127, Val loss: 0.43483902906116684, Val g-mean: 0.7820709490874402\n",
            "Epoch: 243, Train loss: 0.5004248867124825, Val loss: 0.4305250042755353, Val g-mean: 0.7503105393461384\n",
            "Epoch: 244, Train loss: 0.5069402054989822, Val loss: 0.4415479796497445, Val g-mean: 0.7566445780630678\n",
            "Epoch: 245, Train loss: 0.49266630932961764, Val loss: 0.4337085130575456, Val g-mean: 0.7542570043318279\n",
            "Epoch: 246, Train loss: 0.5058486015977762, Val loss: 0.48260611550588356, Val g-mean: 0.7544243644156512\n",
            "Epoch: 247, Train loss: 0.5068810954726116, Val loss: 0.41938103500165436, Val g-mean: 0.7583233022834207\n",
            "Epoch: 248, Train loss: 0.5033936808376887, Val loss: 0.4211277346077718, Val g-mean: 0.7696191951239242\n",
            "Epoch: 249, Train loss: 0.5051627473085318, Val loss: 0.42629376661620644, Val g-mean: 0.767672583281374\n",
            "Epoch: 250, Train loss: 0.5036284061274225, Val loss: 0.42066032263009173, Val g-mean: 0.7637579170221631\n",
            "Epoch: 251, Train loss: 0.4972448208666718, Val loss: 0.42485933103843737, Val g-mean: 0.7688293049083926\n",
            "Epoch: 252, Train loss: 0.5001724727288068, Val loss: 0.44044095807169614, Val g-mean: 0.757840670257855\n",
            "Epoch: 253, Train loss: 0.49965506025399864, Val loss: 0.4230616061311019, Val g-mean: 0.765347653950289\n",
            "Epoch: 254, Train loss: 0.5050911446486247, Val loss: 0.43508599620116384, Val g-mean: 0.7578239340533113\n",
            "Epoch: 255, Train loss: 0.49885700836535696, Val loss: 0.4071516482845733, Val g-mean: 0.7530541225114528\n",
            "Epoch: 256, Train loss: 0.5020217296049146, Val loss: 0.42879432596658407, Val g-mean: 0.7580263662872156\n",
            "Epoch: 257, Train loss: 0.49735855976888255, Val loss: 0.4389090679193798, Val g-mean: 0.7535874302448653\n",
            "Epoch: 258, Train loss: 0.5065690412124387, Val loss: 0.4507029911405162, Val g-mean: 0.7559787042283028\n",
            "Epoch: 259, Train loss: 0.5017277625412282, Val loss: 0.4220411812788562, Val g-mean: 0.7647632800852273\n",
            "Epoch: 260, Train loss: 0.5029685741119493, Val loss: 0.40586815754833977, Val g-mean: 0.7607623952853426\n",
            "Epoch: 261, Train loss: 0.49707989380404854, Val loss: 0.43227010583014863, Val g-mean: 0.765631951834965\n",
            "Epoch: 262, Train loss: 0.5054109045511916, Val loss: 0.428223372093941, Val g-mean: 0.7616870626119158\n",
            "Epoch: 263, Train loss: 0.5083823955598799, Val loss: 0.4713929009280707, Val g-mean: 0.7582439981050995\n",
            "Epoch: 264, Train loss: 0.5062893915504304, Val loss: 0.4497190337432058, Val g-mean: 0.757849892173963\n",
            "Epoch: 265, Train loss: 0.505948611749709, Val loss: 0.4535442578949426, Val g-mean: 0.7566145128078022\n",
            "Epoch: 266, Train loss: 0.5060835710391011, Val loss: 0.42855839196004364, Val g-mean: 0.7486811861138866\n",
            "Epoch: 267, Train loss: 0.5068026536159991, Val loss: 0.4280405909215149, Val g-mean: 0.7563312024212835\n",
            "Epoch: 268, Train loss: 0.4963740425503392, Val loss: 0.42149787258945015, Val g-mean: 0.7621226532373101\n",
            "Epoch: 269, Train loss: 0.5049837843817476, Val loss: 0.41438437351270724, Val g-mean: 0.7648881954588582\n",
            "Epoch: 270, Train loss: 0.5035978877655299, Val loss: 0.44137010801779597, Val g-mean: 0.7574628301661164\n",
            "Epoch: 271, Train loss: 0.5039330548992064, Val loss: 0.44914709796246727, Val g-mean: 0.7582857693374968\n",
            "Epoch: 272, Train loss: 0.4989152405004499, Val loss: 0.4400608431743948, Val g-mean: 0.7605943346707087\n",
            "Epoch: 273, Train loss: 0.5076870741900144, Val loss: 0.4095323530859069, Val g-mean: 0.7597013253280022\n",
            "Epoch: 274, Train loss: 0.510243345568202, Val loss: 0.4238963225170186, Val g-mean: 0.7629661769996877\n",
            "Epoch: 275, Train loss: 0.4981503311288363, Val loss: 0.4462348810936275, Val g-mean: 0.7534982335587411\n",
            "Epoch: 276, Train loss: 0.5022308288527476, Val loss: 0.4307045305245801, Val g-mean: 0.7631032138808888\n",
            "Epoch: 277, Train loss: 0.49926748416536976, Val loss: 0.43528927274440465, Val g-mean: 0.7584401512404277\n",
            "Epoch: 278, Train loss: 0.5009830130144467, Val loss: 0.4323736964479873, Val g-mean: 0.7655709048085876\n",
            "Epoch: 279, Train loss: 0.5043175992552766, Val loss: 0.43103596783782305, Val g-mean: 0.75645872498226\n",
            "Epoch: 280, Train loss: 0.500044633362336, Val loss: 0.432348515054113, Val g-mean: 0.767609741831526\n",
            "Epoch: 281, Train loss: 0.5019938903789155, Val loss: 0.444701879236259, Val g-mean: 0.7644261855926989\n",
            "Epoch: 282, Train loss: 0.503590747603566, Val loss: 0.43972476571798325, Val g-mean: 0.7618106602442162\n",
            "Epoch: 283, Train loss: 0.5030606761545673, Val loss: 0.4293043880086196, Val g-mean: 0.7626022016439351\n",
            "Epoch: 284, Train loss: 0.4963064336854396, Val loss: 0.44830620445703206, Val g-mean: 0.7525379110941195\n",
            "Epoch: 285, Train loss: 0.49953609115219477, Val loss: 0.4205296963061157, Val g-mean: 0.7574055278312981\n",
            "Epoch: 286, Train loss: 0.507454466507711, Val loss: 0.45034516818429293, Val g-mean: 0.7520448937905462\n",
            "Epoch: 287, Train loss: 0.4959733316690743, Val loss: 0.44657009094953537, Val g-mean: 0.7487067709201902\n",
            "Epoch: 288, Train loss: 0.5060652552712147, Val loss: 0.42350392396512787, Val g-mean: 0.7545811425942136\n",
            "Epoch: 289, Train loss: 0.5051761341687572, Val loss: 0.4298977838142922, Val g-mean: 0.7684953276161448\n",
            "Epoch: 290, Train loss: 0.503676269589277, Val loss: 0.430584612645601, Val g-mean: 0.7546062253999771\n",
            "Epoch: 291, Train loss: 0.5010453267614677, Val loss: 0.4288004326977228, Val g-mean: 0.7516772091703596\n",
            "Epoch: 292, Train loss: 0.5016955045851715, Val loss: 0.43897294037436185, Val g-mean: 0.7499655591551678\n",
            "Epoch: 293, Train loss: 0.502219763000787, Val loss: 0.43228156884249885, Val g-mean: 0.7512112691660069\n",
            "Epoch: 294, Train loss: 0.4976008172777656, Val loss: 0.4394965787467204, Val g-mean: 0.7600369985848924\n",
            "Epoch: 295, Train loss: 0.5050302616753791, Val loss: 0.41952712539779513, Val g-mean: 0.76210963310195\n",
            "Epoch: 296, Train loss: 0.49760446781651657, Val loss: 0.43760055695709427, Val g-mean: 0.7642702248743657\n",
            "Epoch: 297, Train loss: 0.5072268336056451, Val loss: 0.43166280184921463, Val g-mean: 0.756884212156468\n",
            "Epoch: 298, Train loss: 0.5036532034190045, Val loss: 0.43473831603401586, Val g-mean: 0.7586074558798808\n",
            "Epoch: 299, Train loss: 0.503425379462701, Val loss: 0.4251404410522235, Val g-mean: 0.7528838073436352\n",
            "Epoch: 300, Train loss: 0.5081453587367781, Val loss: 0.4297204537219123, Val g-mean: 0.7567086521669703\n",
            "Epoch: 301, Train loss: 0.49992000498728434, Val loss: 0.4482376091182232, Val g-mean: 0.7534709922498128\n",
            "Epoch: 302, Train loss: 0.4977963486884656, Val loss: 0.42295538849736514, Val g-mean: 0.7625929659789069\n",
            "Epoch: 303, Train loss: 0.5081612238224521, Val loss: 0.44831056677197156, Val g-mean: 0.7558027204656388\n",
            "Epoch: 304, Train loss: 0.4945940353602071, Val loss: 0.43326696243725327, Val g-mean: 0.7591672365838844\n",
            "Epoch: 305, Train loss: 0.4996260913791623, Val loss: 0.437285374261831, Val g-mean: 0.7725263343767256\n",
            "Epoch: 306, Train loss: 0.5072186061493722, Val loss: 0.45024389185403524, Val g-mean: 0.7530792685359958\n",
            "Epoch: 307, Train loss: 0.506987062119476, Val loss: 0.4617249299036829, Val g-mean: 0.7563314147303075\n",
            "Epoch: 308, Train loss: 0.5006197909781555, Val loss: 0.43140578446419614, Val g-mean: 0.7454537097179341\n",
            "Epoch: 309, Train loss: 0.4950287826235598, Val loss: 0.41240092602215317, Val g-mean: 0.7587171235224407\n",
            "Epoch: 310, Train loss: 0.499603491570279, Val loss: 0.4398312933350864, Val g-mean: 0.7666903958084165\n",
            "Epoch: 311, Train loss: 0.50315248839546, Val loss: 0.428001742613943, Val g-mean: 0.7671837998175499\n",
            "Epoch: 312, Train loss: 0.506207408840324, Val loss: 0.40942511491869626, Val g-mean: 0.7621890957917788\n",
            "Epoch: 313, Train loss: 0.5030545114512733, Val loss: 0.44374479843597664, Val g-mean: 0.7603595489026076\n",
            "Epoch: 314, Train loss: 0.5058354283608771, Val loss: 0.42683836053076546, Val g-mean: 0.7666534386330524\n",
            "Epoch: 315, Train loss: 0.5073366655206413, Val loss: 0.4538052729085872, Val g-mean: 0.7613311844366819\n",
            "Epoch: 316, Train loss: 0.5105920180241678, Val loss: 0.4203641371507394, Val g-mean: 0.751246593236526\n",
            "Epoch: 317, Train loss: 0.5016817702655217, Val loss: 0.43713096156716347, Val g-mean: 0.7568540362727993\n",
            "Epoch: 318, Train loss: 0.5068069969713798, Val loss: 0.43221825557319743, Val g-mean: 0.756000434495426\n",
            "Epoch: 319, Train loss: 0.5054439485990689, Val loss: 0.40265539112059695, Val g-mean: 0.7603925212522523\n",
            "Epoch: 320, Train loss: 0.4985425257393239, Val loss: 0.4468833012016196, Val g-mean: 0.7549042593561804\n",
            "Epoch: 321, Train loss: 0.49736986762522745, Val loss: 0.4279914674790282, Val g-mean: 0.7538751060731852\n",
            "Epoch: 322, Train loss: 0.49638391749018096, Val loss: 0.44748296216130257, Val g-mean: 0.772195815657805\n",
            "Epoch: 323, Train loss: 0.49755092389122274, Val loss: 0.4216339358765828, Val g-mean: 0.7551938853666902\n",
            "Epoch: 324, Train loss: 0.5019088816032952, Val loss: 0.4286092972677005, Val g-mean: 0.7592569086750762\n",
            "Epoch: 325, Train loss: 0.5017832623013962, Val loss: 0.42923719122221593, Val g-mean: 0.7558802059988655\n",
            "Epoch: 326, Train loss: 0.49282515860986575, Val loss: 0.42708759323546763, Val g-mean: 0.764100581248873\n",
            "Epoch: 327, Train loss: 0.5019653700468288, Val loss: 0.43069782245316, Val g-mean: 0.7626618554822752\n",
            "Epoch: 328, Train loss: 0.4961856947726256, Val loss: 0.4272370554114643, Val g-mean: 0.7587106305464557\n",
            "Epoch: 329, Train loss: 0.5014125949102176, Val loss: 0.4306461397362383, Val g-mean: 0.7694807211382737\n",
            "Epoch: 330, Train loss: 0.5030991174828523, Val loss: 0.4354903780316052, Val g-mean: 0.7691020709537166\n",
            "Epoch: 331, Train loss: 0.5043693719152075, Val loss: 0.43484890382540853, Val g-mean: 0.7652603343838319\n",
            "Epoch: 332, Train loss: 0.5010186385110895, Val loss: 0.4325198909561885, Val g-mean: 0.7641124117434176\n",
            "Epoch: 333, Train loss: 0.49637197572671504, Val loss: 0.4280127535917257, Val g-mean: 0.7636176357694274\n",
            "Epoch: 334, Train loss: 0.49788711157772714, Val loss: 0.437456324304405, Val g-mean: 0.7582437107735776\n",
            "Epoch: 335, Train loss: 0.5046523858286136, Val loss: 0.44193318956776667, Val g-mean: 0.7583422533052815\n",
            "Epoch: 336, Train loss: 0.5081418816697628, Val loss: 0.42214774301177577, Val g-mean: 0.7551229866070123\n",
            "Epoch: 337, Train loss: 0.5059677677835603, Val loss: 0.44250616235168355, Val g-mean: 0.7578245285829536\n",
            "Epoch: 338, Train loss: 0.5112951814016897, Val loss: 0.453145461843202, Val g-mean: 0.7522345063421334\n",
            "Epoch: 339, Train loss: 0.5027926386941974, Val loss: 0.44660494025600583, Val g-mean: 0.7578261661090864\n",
            "Epoch: 340, Train loss: 0.4973782484509168, Val loss: 0.4438536553398559, Val g-mean: 0.7543371158034635\n",
            "Epoch: 341, Train loss: 0.5085643221013839, Val loss: 0.4521023780107498, Val g-mean: 0.7580170076110518\n",
            "Epoch: 342, Train loss: 0.4995672522121174, Val loss: 0.4226302439837079, Val g-mean: 0.7533088479935657\n",
            "Epoch: 343, Train loss: 0.504889184231647, Val loss: 0.42905555155716446, Val g-mean: 0.7514665560404239\n",
            "Epoch: 344, Train loss: 0.49715606146724306, Val loss: 0.42757024459148707, Val g-mean: 0.7584270516745438\n",
            "Epoch: 345, Train loss: 0.5087301576277853, Val loss: 0.4090443782900509, Val g-mean: 0.7577282250605111\n",
            "Epoch: 346, Train loss: 0.5007935656375562, Val loss: 0.4286178211240392, Val g-mean: 0.7671201437840736\n",
            "Epoch: 347, Train loss: 0.502797294325125, Val loss: 0.43396908848693494, Val g-mean: 0.7486579946328792\n",
            "Epoch: 348, Train loss: 0.5082601462714111, Val loss: 0.4204838317083685, Val g-mean: 0.760565387734636\n",
            "Epoch: 349, Train loss: 0.49891932349171614, Val loss: 0.43315405516248, Val g-mean: 0.7548350587336358\n",
            "Epoch: 350, Train loss: 0.499871977468532, Val loss: 0.4307850987503403, Val g-mean: 0.7712650455315777\n",
            "Epoch: 351, Train loss: 0.5046471688909541, Val loss: 0.4158439697011521, Val g-mean: 0.7637915882726135\n",
            "Epoch: 352, Train loss: 0.5053177280744001, Val loss: 0.40118804907328204, Val g-mean: 0.7656588559913059\n",
            "Epoch: 353, Train loss: 0.5017715671698255, Val loss: 0.4363019076225005, Val g-mean: 0.7742639300982017\n",
            "Epoch: 354, Train loss: 0.49301329328051785, Val loss: 0.4391487791742149, Val g-mean: 0.7516108939154834\n",
            "Epoch: 355, Train loss: 0.49109364891089674, Val loss: 0.44243013054916736, Val g-mean: 0.7555772536794021\n",
            "Epoch: 356, Train loss: 0.49741852161759814, Val loss: 0.441472469583938, Val g-mean: 0.7572695896087498\n",
            "Epoch: 357, Train loss: 0.5001389964310419, Val loss: 0.42025154789811686, Val g-mean: 0.7668863927834307\n",
            "Epoch: 358, Train loss: 0.49863633901374405, Val loss: 0.4316376831970717, Val g-mean: 0.750992206940144\n",
            "Epoch: 359, Train loss: 0.5036572618940673, Val loss: 0.43831830079618256, Val g-mean: 0.7593855912881717\n",
            "Epoch: 360, Train loss: 0.5003105820366668, Val loss: 0.42263907076496826, Val g-mean: 0.7646963045959656\n",
            "Epoch: 361, Train loss: 0.5015772419570542, Val loss: 0.426969973076331, Val g-mean: 0.7591834975812642\n",
            "Epoch: 362, Train loss: 0.503561173189624, Val loss: 0.4305940978229046, Val g-mean: 0.7622955859283402\n",
            "Epoch: 363, Train loss: 0.4995040055510492, Val loss: 0.4285220482239598, Val g-mean: 0.7643365033397563\n",
            "Epoch: 364, Train loss: 0.5095794257270586, Val loss: 0.44038551359584455, Val g-mean: 0.7608160148489651\n",
            "Epoch: 365, Train loss: 0.5004420242726471, Val loss: 0.4545431821361968, Val g-mean: 0.7491312273112511\n",
            "Epoch: 366, Train loss: 0.5025145465397513, Val loss: 0.4294122279082474, Val g-mean: 0.761081580334138\n",
            "Epoch: 367, Train loss: 0.5041809514799235, Val loss: 0.4139557257294655, Val g-mean: 0.7602404065818144\n",
            "Epoch: 368, Train loss: 0.4977458703299325, Val loss: 0.43675211777812556, Val g-mean: 0.7656288867525768\n",
            "Epoch: 369, Train loss: 0.5056844023962291, Val loss: 0.4251893709756826, Val g-mean: 0.7551558624310617\n",
            "Epoch: 370, Train loss: 0.5034931167910482, Val loss: 0.4350426742120793, Val g-mean: 0.764396222818721\n",
            "Epoch: 371, Train loss: 0.5044199530622153, Val loss: 0.42315012216567993, Val g-mean: 0.7644969285273372\n",
            "Epoch: 372, Train loss: 0.4980622435984402, Val loss: 0.405437918085801, Val g-mean: 0.7585307830397889\n",
            "Epoch: 373, Train loss: 0.49977397423401576, Val loss: 0.432520844434437, Val g-mean: 0.752169437660923\n",
            "Epoch: 374, Train loss: 0.5034081675769507, Val loss: 0.39686105792459686, Val g-mean: 0.7616393253109136\n",
            "Epoch: 375, Train loss: 0.5025998347113049, Val loss: 0.416148377288329, Val g-mean: 0.7638063919602373\n",
            "Epoch: 376, Train loss: 0.4982678772407391, Val loss: 0.42539647378419576, Val g-mean: 0.7575375871952805\n",
            "Epoch: 377, Train loss: 0.4990057463745214, Val loss: 0.45860863476991653, Val g-mean: 0.7532602450097228\n",
            "Epoch: 378, Train loss: 0.49730992582623357, Val loss: 0.42478364627612264, Val g-mean: 0.7609750741429726\n",
            "Epoch: 379, Train loss: 0.5056212529454274, Val loss: 0.4315099518157934, Val g-mean: 0.753495153762437\n",
            "Epoch: 380, Train loss: 0.49364610639819356, Val loss: 0.4370271059635438, Val g-mean: 0.7470044384091417\n",
            "Epoch: 381, Train loss: 0.4988503861656338, Val loss: 0.4154566467592591, Val g-mean: 0.7680373022867912\n",
            "Epoch: 382, Train loss: 0.5014070855636099, Val loss: 0.44263793841788646, Val g-mean: 0.7526647639770614\n",
            "Epoch: 383, Train loss: 0.4945972694824105, Val loss: 0.43539952369112717, Val g-mean: 0.7556822817087278\n",
            "Epoch: 384, Train loss: 0.5017243580176903, Val loss: 0.4179845993456088, Val g-mean: 0.7657497270061983\n",
            "Epoch: 385, Train loss: 0.4937862901733488, Val loss: 0.4152080432364815, Val g-mean: 0.7605322683980975\n",
            "Epoch: 386, Train loss: 0.500060154024044, Val loss: 0.4267186830310445, Val g-mean: 0.763249139965236\n",
            "Epoch: 387, Train loss: 0.4950054142303446, Val loss: 0.4304729813807889, Val g-mean: 0.7637253882501097\n",
            "Epoch: 388, Train loss: 0.4973086875609849, Val loss: 0.4047559001728108, Val g-mean: 0.757304305197287\n",
            "Epoch: 389, Train loss: 0.49948305561336587, Val loss: 0.4100533196408498, Val g-mean: 0.7671896002553943\n",
            "Epoch: 390, Train loss: 0.4986423200876548, Val loss: 0.4437418356537819, Val g-mean: 0.7587514192785949\n",
            "Epoch: 391, Train loss: 0.4998454795242167, Val loss: 0.4183547285042311, Val g-mean: 0.7561203372117251\n",
            "Epoch: 392, Train loss: 0.49714305616774274, Val loss: 0.41413368578804166, Val g-mean: 0.7680857517935266\n",
            "Epoch: 393, Train loss: 0.5005087616196218, Val loss: 0.4374158186347861, Val g-mean: 0.7661591863992989\n",
            "Epoch: 394, Train loss: 0.4971772151614253, Val loss: 0.4277782859770875, Val g-mean: 0.7616885601232803\n",
            "Epoch: 395, Train loss: 0.5033627733259479, Val loss: 0.4252721682975167, Val g-mean: 0.7598596953923151\n",
            "Epoch: 396, Train loss: 0.4947837041780328, Val loss: 0.43732789001966776, Val g-mean: 0.761388579253144\n",
            "Epoch: 397, Train loss: 0.4983536359170157, Val loss: 0.41953299351428686, Val g-mean: 0.7680187076086824\n",
            "Epoch: 398, Train loss: 0.49800058743540415, Val loss: 0.4500625125671688, Val g-mean: 0.7529122609667108\n",
            "Epoch: 399, Train loss: 0.49769058105746494, Val loss: 0.43730845047455086, Val g-mean: 0.7514352894732583\n",
            "Epoch: 400, Train loss: 0.502755899773721, Val loss: 0.4181338078097293, Val g-mean: 0.7737477703597654\n",
            "Epoch: 401, Train loss: 0.50794613726167, Val loss: 0.419032431746784, Val g-mean: 0.7638939329343019\n",
            "Epoch: 402, Train loss: 0.5078416538257421, Val loss: 0.4225119309205758, Val g-mean: 0.7682653127296368\n",
            "Epoch: 403, Train loss: 0.5042290485533157, Val loss: 0.456753277072781, Val g-mean: 0.7585544810246113\n",
            "Epoch: 404, Train loss: 0.5068987367357917, Val loss: 0.42755128580488655, Val g-mean: 0.7532540635038562\n",
            "Epoch: 405, Train loss: 0.5031278354499583, Val loss: 0.43214785993883487, Val g-mean: 0.7510063137419773\n",
            "Epoch: 406, Train loss: 0.5068265425075926, Val loss: 0.4483472371571942, Val g-mean: 0.758878728532942\n",
            "Epoch: 407, Train loss: 0.5002947047794855, Val loss: 0.41689307124991165, Val g-mean: 0.7651453026486312\n",
            "Epoch: 408, Train loss: 0.4997123994729106, Val loss: 0.4201532743479076, Val g-mean: 0.7685992744437496\n",
            "Epoch: 409, Train loss: 0.5004339521869787, Val loss: 0.42140704275746094, Val g-mean: 0.7655767393733144\n",
            "Epoch: 410, Train loss: 0.5047314629893156, Val loss: 0.4157908123574759, Val g-mean: 0.7651405473066022\n",
            "Epoch: 411, Train loss: 0.5021550840085552, Val loss: 0.42645234124440895, Val g-mean: 0.7609859768616842\n",
            "Epoch: 412, Train loss: 0.49959852085287587, Val loss: 0.41681380217012604, Val g-mean: 0.7642342697081682\n",
            "Epoch: 413, Train loss: 0.5029786876904796, Val loss: 0.4185125518002008, Val g-mean: 0.7609323160570612\n",
            "Epoch: 414, Train loss: 0.5054197665684783, Val loss: 0.4263191762331285, Val g-mean: 0.7527763083067294\n",
            "Epoch: 415, Train loss: 0.4947033384824881, Val loss: 0.4238149949202412, Val g-mean: 0.7695550051368836\n",
            "Epoch: 416, Train loss: 0.4966910650316693, Val loss: 0.41458234504649516, Val g-mean: 0.7702893374584305\n",
            "Epoch: 417, Train loss: 0.4913425332359677, Val loss: 0.4275421208064807, Val g-mean: 0.7766735201612638\n",
            "Epoch: 418, Train loss: 0.5017182243655968, Val loss: 0.4110801270917842, Val g-mean: 0.771898789309024\n",
            "Epoch: 419, Train loss: 0.5014209747648837, Val loss: 0.41428020204368393, Val g-mean: 0.7570956464264965\n",
            "Epoch: 420, Train loss: 0.4984995667880476, Val loss: 0.4269634469559318, Val g-mean: 0.7630838877121591\n",
            "Epoch: 421, Train loss: 0.5014882282330951, Val loss: 0.44875307008624077, Val g-mean: 0.7568890305314121\n",
            "Epoch: 422, Train loss: 0.49420649727163174, Val loss: 0.4330786799913959, Val g-mean: 0.7523004823063376\n",
            "Epoch: 423, Train loss: 0.5041974620651722, Val loss: 0.41940432002669886, Val g-mean: 0.7701283131204502\n",
            "Epoch: 424, Train loss: 0.49978786272080195, Val loss: 0.43254529606354863, Val g-mean: 0.767199305785684\n",
            "Epoch: 425, Train loss: 0.5042710005092703, Val loss: 0.4553808740487224, Val g-mean: 0.756379061912233\n",
            "Epoch: 426, Train loss: 0.5001122637768798, Val loss: 0.428759867619527, Val g-mean: 0.7557831630023426\n",
            "Epoch: 427, Train loss: 0.5018992611822858, Val loss: 0.4378905866883303, Val g-mean: 0.7681121442968201\n",
            "Epoch: 428, Train loss: 0.4926965022010171, Val loss: 0.41281418737612274, Val g-mean: 0.7651138111191875\n",
            "Epoch: 429, Train loss: 0.4976308104864405, Val loss: 0.41247706762269926, Val g-mean: 0.7568004531339249\n",
            "Epoch: 430, Train loss: 0.49953013516693695, Val loss: 0.4425260948115273, Val g-mean: 0.7604790629790399\n",
            "Epoch: 431, Train loss: 0.5010933765866331, Val loss: 0.4297955334186554, Val g-mean: 0.7569086365276516\n",
            "Epoch: 432, Train loss: 0.5021726219515475, Val loss: 0.4313926622271538, Val g-mean: 0.7700592942975351\n",
            "Epoch: 433, Train loss: 0.49676341453315, Val loss: 0.4450668906302829, Val g-mean: 0.7739344032682881\n",
            "Epoch: 434, Train loss: 0.5030181888004983, Val loss: 0.4376896884488432, Val g-mean: 0.774440249743184\n",
            "Epoch: 435, Train loss: 0.4982683098715147, Val loss: 0.44054488661257846, Val g-mean: 0.7710280297683284\n",
            "Epoch: 436, Train loss: 0.4986573105747281, Val loss: 0.41695126244112063, Val g-mean: 0.7566832658085688\n",
            "Epoch: 437, Train loss: 0.5086208517763039, Val loss: 0.4445381403753632, Val g-mean: 0.7617414613680451\n",
            "Epoch: 438, Train loss: 0.4957627456319254, Val loss: 0.42011617026046705, Val g-mean: 0.7741379647042334\n",
            "Epoch: 439, Train loss: 0.5022876249617964, Val loss: 0.4321824110260135, Val g-mean: 0.7458224793559083\n",
            "Epoch: 440, Train loss: 0.4959742269674587, Val loss: 0.43137016676758466, Val g-mean: 0.7570803149890436\n",
            "Epoch: 441, Train loss: 0.4954964302656934, Val loss: 0.4282533889146228, Val g-mean: 0.7617430747346443\n",
            "Epoch: 442, Train loss: 0.5029876399005658, Val loss: 0.4147610329091549, Val g-mean: 0.7643449678309652\n",
            "Epoch: 443, Train loss: 0.496354454096904, Val loss: 0.4245093264862111, Val g-mean: 0.7607454493189157\n",
            "Epoch: 444, Train loss: 0.5057905369509926, Val loss: 0.42622837718380124, Val g-mean: 0.7562357438056208\n",
            "Epoch: 445, Train loss: 0.4986147129052294, Val loss: 0.42185899066297633, Val g-mean: 0.7643925084798122\n",
            "Epoch: 446, Train loss: 0.4992159302912088, Val loss: 0.43700517988518667, Val g-mean: 0.756478513609737\n",
            "Epoch: 447, Train loss: 0.4961948606421077, Val loss: 0.42386167653297124, Val g-mean: 0.7598689070334601\n",
            "Epoch: 448, Train loss: 0.5000781163652986, Val loss: 0.453210356792337, Val g-mean: 0.76507154892091\n",
            "Epoch: 449, Train loss: 0.4962585397066784, Val loss: 0.4268355316629535, Val g-mean: 0.7672670496484132\n",
            "Epoch: 450, Train loss: 0.4950395982712563, Val loss: 0.4037174633459041, Val g-mean: 0.7612225202843884\n",
            "Epoch: 451, Train loss: 0.5078270870898116, Val loss: 0.4193139227205201, Val g-mean: 0.7647413685570836\n",
            "Epoch: 452, Train loss: 0.5013646494883559, Val loss: 0.4160039756251009, Val g-mean: 0.766431626639141\n",
            "Epoch: 453, Train loss: 0.5092565488175819, Val loss: 0.41067730164841604, Val g-mean: 0.7633659777588424\n",
            "Epoch: 454, Train loss: 0.49400745429322457, Val loss: 0.4329658727112569, Val g-mean: 0.7503707793112364\n",
            "Epoch: 455, Train loss: 0.5005540240223275, Val loss: 0.4369433549673934, Val g-mean: 0.7609463367652247\n",
            "Epoch: 456, Train loss: 0.4935903784172971, Val loss: 0.4177379306209715, Val g-mean: 0.7710067692546472\n",
            "Epoch: 457, Train loss: 0.49408681752529376, Val loss: 0.4247699966164012, Val g-mean: 0.7568131761200789\n",
            "Epoch: 458, Train loss: 0.49994396733668983, Val loss: 0.4413347091329725, Val g-mean: 0.7659951889434947\n",
            "Epoch: 459, Train loss: 0.5027272386423923, Val loss: 0.4354554724536444, Val g-mean: 0.7698857723631665\n",
            "Epoch: 460, Train loss: 0.48918575514584306, Val loss: 0.4536059983074665, Val g-mean: 0.749388840761528\n",
            "Epoch: 461, Train loss: 0.4950497284974, Val loss: 0.41963890410567584, Val g-mean: 0.7630891339010778\n",
            "Epoch: 462, Train loss: 0.49850459196966296, Val loss: 0.44489548865117523, Val g-mean: 0.7568455528065141\n",
            "Epoch: 463, Train loss: 0.5014482104722872, Val loss: 0.42302761736669037, Val g-mean: 0.7642474864781565\n",
            "Epoch: 464, Train loss: 0.5053612149701416, Val loss: 0.4322932395888002, Val g-mean: 0.7613028339380783\n",
            "Epoch: 465, Train loss: 0.5005149170937923, Val loss: 0.44248538287846667, Val g-mean: 0.7517070058717019\n",
            "Epoch: 466, Train loss: 0.5048268247204465, Val loss: 0.4213557241386489, Val g-mean: 0.7604539687248308\n",
            "Epoch: 467, Train loss: 0.4950102468555367, Val loss: 0.43728400139432205, Val g-mean: 0.7540863519145361\n",
            "Epoch: 468, Train loss: 0.5027566478969371, Val loss: 0.4447162723854968, Val g-mean: 0.7554814056234556\n",
            "Epoch: 469, Train loss: 0.5003591609270994, Val loss: 0.4179601845772643, Val g-mean: 0.7628197728574978\n",
            "Epoch: 470, Train loss: 0.49822190046381437, Val loss: 0.42813365357486827, Val g-mean: 0.7635519446720774\n",
            "Epoch: 471, Train loss: 0.5012831192895173, Val loss: 0.4407244738387434, Val g-mean: 0.7628992888375651\n",
            "Epoch: 472, Train loss: 0.494039575786142, Val loss: 0.4286864246977003, Val g-mean: 0.7698390066802127\n",
            "Epoch: 473, Train loss: 0.5057466738718317, Val loss: 0.4072361523775678, Val g-mean: 0.7596205781776175\n",
            "Epoch: 474, Train loss: 0.49926300611751534, Val loss: 0.4052003371088128, Val g-mean: 0.7644565250732224\n",
            "Epoch: 475, Train loss: 0.5058110188409575, Val loss: 0.42308007200297554, Val g-mean: 0.7599160269352313\n",
            "Epoch: 476, Train loss: 0.4890988009827307, Val loss: 0.4247625538785207, Val g-mean: 0.7634212665655281\n",
            "Epoch: 477, Train loss: 0.49925145542845645, Val loss: 0.41338670116506127, Val g-mean: 0.764497083676469\n",
            "Epoch: 478, Train loss: 0.5013146024095975, Val loss: 0.42565746703430224, Val g-mean: 0.7531170707296395\n",
            "Epoch: 479, Train loss: 0.502524014944434, Val loss: 0.4195990317354077, Val g-mean: 0.7740001081510426\n",
            "Epoch: 480, Train loss: 0.4978125721904949, Val loss: 0.44145719824652924, Val g-mean: 0.7524878378139648\n",
            "Epoch: 481, Train loss: 0.4999547468222372, Val loss: 0.4273412478597541, Val g-mean: 0.7659809985982154\n",
            "Epoch: 482, Train loss: 0.49461546662239964, Val loss: 0.41843723035172414, Val g-mean: 0.7618056607686327\n",
            "Epoch: 483, Train loss: 0.5041425181383596, Val loss: 0.413064514531901, Val g-mean: 0.7632257903356752\n",
            "Epoch: 484, Train loss: 0.4972966123907051, Val loss: 0.4201341263557735, Val g-mean: 0.7635957011712623\n",
            "Epoch: 485, Train loss: 0.49879201726246375, Val loss: 0.43318209581469236, Val g-mean: 0.7595383079077757\n",
            "Epoch: 486, Train loss: 0.4995143514129489, Val loss: 0.42502970542562635, Val g-mean: 0.7742746569378067\n",
            "Epoch: 487, Train loss: 0.49722381903741697, Val loss: 0.4404639354661891, Val g-mean: 0.7596939461480485\n",
            "Epoch: 488, Train loss: 0.493666799307396, Val loss: 0.4445003559322734, Val g-mean: 0.7572542027688827\n",
            "Epoch: 489, Train loss: 0.49276349344380416, Val loss: 0.4271225519478321, Val g-mean: 0.7683410703830919\n",
            "Epoch: 490, Train loss: 0.49660091347823215, Val loss: 0.441722243436073, Val g-mean: 0.7532549128408307\n",
            "Epoch: 491, Train loss: 0.5091663815335563, Val loss: 0.4529121124038571, Val g-mean: 0.7570823585612713\n",
            "Epoch: 492, Train loss: 0.505538155158769, Val loss: 0.4504495539555424, Val g-mean: 0.7577990381452306\n",
            "Epoch: 493, Train loss: 0.49454364859013206, Val loss: 0.41195532032533694, Val g-mean: 0.7707067789223958\n",
            "Epoch: 494, Train loss: 0.49684502986516565, Val loss: 0.44461577169989286, Val g-mean: 0.7599083864469333\n",
            "Epoch: 495, Train loss: 0.49339458406689035, Val loss: 0.4373849514675768, Val g-mean: 0.750285931896027\n",
            "Epoch: 496, Train loss: 0.49322400518172727, Val loss: 0.4306226596236229, Val g-mean: 0.7566387696327548\n",
            "Epoch: 497, Train loss: 0.4936004471956412, Val loss: 0.4464766098872611, Val g-mean: 0.7588524807637208\n",
            "Epoch: 498, Train loss: 0.49400916667407724, Val loss: 0.4298371355000295, Val g-mean: 0.7539629350007144\n",
            "Epoch: 499, Train loss: 0.5036448875466004, Val loss: 0.4434348284021804, Val g-mean: 0.7650426923021761\n",
            "Epoch: 500, Train loss: 0.5035800534889338, Val loss: 0.42528122859565837, Val g-mean: 0.7753861167940455\n",
            "Epoch: 501, Train loss: 0.4958946506140661, Val loss: 0.4312682171401225, Val g-mean: 0.7534923230460675\n",
            "Epoch: 502, Train loss: 0.4946009558542727, Val loss: 0.4310877480004963, Val g-mean: 0.7567234976498399\n",
            "Epoch: 503, Train loss: 0.5049075218724163, Val loss: 0.4273188378857939, Val g-mean: 0.7650497215100686\n",
            "Epoch: 504, Train loss: 0.5024252911217563, Val loss: 0.41916478170376076, Val g-mean: 0.7702477250440697\n",
            "Epoch: 505, Train loss: 0.5026826884135017, Val loss: 0.4318846513174082, Val g-mean: 0.7534521025108109\n",
            "Epoch: 506, Train loss: 0.4997144161760399, Val loss: 0.42890996583982516, Val g-mean: 0.7564452507963163\n",
            "Epoch: 507, Train loss: 0.49836896155774485, Val loss: 0.43222738253442866, Val g-mean: 0.7737638151603309\n",
            "Epoch: 508, Train loss: 0.5038892120369954, Val loss: 0.4067245880632024, Val g-mean: 0.7627099678646064\n",
            "Epoch: 509, Train loss: 0.49923228143142157, Val loss: 0.42134695323674304, Val g-mean: 0.7610494892450667\n",
            "Epoch: 510, Train loss: 0.49420289196426576, Val loss: 0.42771725670287486, Val g-mean: 0.7638335350920228\n",
            "Epoch: 511, Train loss: 0.49998989413648703, Val loss: 0.4261810154114899, Val g-mean: 0.7600158681380755\n",
            "Epoch: 512, Train loss: 0.49629451170053135, Val loss: 0.41902661480401693, Val g-mean: 0.7616909729681572\n",
            "Epoch: 513, Train loss: 0.4970198794766702, Val loss: 0.4212898468892825, Val g-mean: 0.7671118798928628\n",
            "Epoch: 514, Train loss: 0.496383548369963, Val loss: 0.41316072838871104, Val g-mean: 0.7708603862505395\n",
            "Epoch: 515, Train loss: 0.49740373776123603, Val loss: 0.42797196734892695, Val g-mean: 0.7554121063970625\n",
            "Epoch: 516, Train loss: 0.49813671480296096, Val loss: 0.43447299634939746, Val g-mean: 0.7598141675849043\n",
            "Epoch: 517, Train loss: 0.5090831608549226, Val loss: 0.4614495566408885, Val g-mean: 0.754895843586244\n",
            "Epoch: 518, Train loss: 0.5033647028264651, Val loss: 0.434066202099386, Val g-mean: 0.7660656077419616\n",
            "Epoch: 519, Train loss: 0.499145761675106, Val loss: 0.4428032818985613, Val g-mean: 0.7583017045454258\n",
            "Epoch: 520, Train loss: 0.4978556464520062, Val loss: 0.43890936064877006, Val g-mean: 0.7586498351043957\n",
            "Epoch: 521, Train loss: 0.49584342651829116, Val loss: 0.45217638952951683, Val g-mean: 0.7513444075262553\n",
            "Epoch: 522, Train loss: 0.5036257407679017, Val loss: 0.43329768412207303, Val g-mean: 0.7741574921678708\n",
            "Epoch: 523, Train loss: 0.5011145462111785, Val loss: 0.4500150621721619, Val g-mean: 0.7728728062712349\n",
            "Epoch: 524, Train loss: 0.5036618215199001, Val loss: 0.4252354398761925, Val g-mean: 0.779255993437802\n",
            "Epoch: 525, Train loss: 0.5039759602524619, Val loss: 0.4487336628923291, Val g-mean: 0.7565172106225729\n",
            "Epoch: 526, Train loss: 0.4921234219387377, Val loss: 0.43737829790303584, Val g-mean: 0.7574640040458016\n",
            "Epoch: 527, Train loss: 0.5028173507021059, Val loss: 0.4121689886638993, Val g-mean: 0.7625639326542616\n",
            "Epoch: 528, Train loss: 0.5078266777510345, Val loss: 0.4251667310140635, Val g-mean: 0.766050675153456\n",
            "Epoch: 529, Train loss: 0.4973938980591791, Val loss: 0.43015149863142715, Val g-mean: 0.7556002473222438\n",
            "Epoch: 530, Train loss: 0.4925627423976924, Val loss: 0.43664273835326495, Val g-mean: 0.7579196894250344\n",
            "Epoch: 531, Train loss: 0.5081308115826758, Val loss: 0.4326720872991963, Val g-mean: 0.7623087357892818\n",
            "Epoch: 532, Train loss: 0.49444827732628555, Val loss: 0.4291655209503676, Val g-mean: 0.7627026594934143\n",
            "Epoch: 533, Train loss: 0.49159659453054944, Val loss: 0.41673511678450986, Val g-mean: 0.7661980200729026\n",
            "Epoch: 534, Train loss: 0.4985233157109327, Val loss: 0.44278443310605853, Val g-mean: 0.7570616922064709\n",
            "Epoch: 535, Train loss: 0.5040989147185001, Val loss: 0.4085332457172243, Val g-mean: 0.7631804374333679\n",
            "Epoch: 536, Train loss: 0.4998804924952323, Val loss: 0.4290394514406982, Val g-mean: 0.7452922191379865\n",
            "Epoch: 537, Train loss: 0.5051641589666337, Val loss: 0.4164241658229577, Val g-mean: 0.7515683683167792\n",
            "Epoch: 538, Train loss: 0.4946264392615017, Val loss: 0.4215579109364434, Val g-mean: 0.7563610313237911\n",
            "Epoch: 539, Train loss: 0.4981828263100842, Val loss: 0.42118193405239207, Val g-mean: 0.7588298838213675\n",
            "Epoch: 540, Train loss: 0.4994593002051033, Val loss: 0.445456308557799, Val g-mean: 0.7683516960128327\n",
            "Epoch: 541, Train loss: 0.4980796568687807, Val loss: 0.43283643691163315, Val g-mean: 0.7639341649032979\n",
            "Epoch: 542, Train loss: 0.492952135030366, Val loss: 0.4496258875649226, Val g-mean: 0.7608753946520975\n",
            "Epoch: 543, Train loss: 0.5035752614031741, Val loss: 0.4233440580336671, Val g-mean: 0.7707205822262534\n",
            "Epoch: 544, Train loss: 0.4984240201473511, Val loss: 0.43586553260684013, Val g-mean: 0.7619424062996915\n",
            "Epoch: 545, Train loss: 0.508796057333445, Val loss: 0.40833494449524504, Val g-mean: 0.7600628821974127\n",
            "Epoch: 546, Train loss: 0.5086360226396378, Val loss: 0.42567202684126404, Val g-mean: 0.7691972085704993\n",
            "Epoch: 547, Train loss: 0.4997253455883674, Val loss: 0.4260136047868352, Val g-mean: 0.7715740082433655\n",
            "Epoch: 548, Train loss: 0.5006811674701708, Val loss: 0.4143732901858656, Val g-mean: 0.7735654551603157\n",
            "Epoch: 549, Train loss: 0.4990931633174093, Val loss: 0.4184674587296812, Val g-mean: 0.7669766531073124\n",
            "Epoch: 550, Train loss: 0.49249913395251016, Val loss: 0.4195005715285477, Val g-mean: 0.7705529892215957\n",
            "Epoch: 551, Train loss: 0.49421587013905643, Val loss: 0.4112358246194689, Val g-mean: 0.7648418696147494\n",
            "Epoch: 552, Train loss: 0.5000652049779122, Val loss: 0.4182182056339164, Val g-mean: 0.7693866971366197\n",
            "Epoch: 553, Train loss: 0.5015708677988283, Val loss: 0.42417147265453087, Val g-mean: 0.7605579578025178\n",
            "Epoch: 554, Train loss: 0.48966199283281187, Val loss: 0.4249482108770232, Val g-mean: 0.7674344266009182\n",
            "Epoch: 555, Train loss: 0.5018529366831191, Val loss: 0.44347750611211123, Val g-mean: 0.7587760965432548\n",
            "Epoch: 556, Train loss: 0.5018323677448332, Val loss: 0.4254885402164961, Val g-mean: 0.7682029044954206\n",
            "Epoch: 557, Train loss: 0.504422708607734, Val loss: 0.42601622190130384, Val g-mean: 0.7601557255049336\n",
            "Epoch: 558, Train loss: 0.4987905938964408, Val loss: 0.39802511056002815, Val g-mean: 0.7617537789069752\n",
            "Epoch: 559, Train loss: 0.49430252303037026, Val loss: 0.4230165201190271, Val g-mean: 0.759756871891368\n",
            "Epoch: 560, Train loss: 0.4995524306018502, Val loss: 0.43840684271172475, Val g-mean: 0.7570860343403477\n",
            "Epoch: 561, Train loss: 0.49507586113956775, Val loss: 0.43498512769215986, Val g-mean: 0.7624786346035869\n",
            "Epoch: 562, Train loss: 0.49971585235897575, Val loss: 0.42290703873885305, Val g-mean: 0.7663158850451828\n",
            "Epoch: 563, Train loss: 0.4945611777958911, Val loss: 0.40524711577515854, Val g-mean: 0.7626061627725338\n",
            "Epoch: 564, Train loss: 0.5034729018103073, Val loss: 0.432894256162016, Val g-mean: 0.7609351319942358\n",
            "Epoch: 565, Train loss: 0.4993254387438584, Val loss: 0.4516438371256778, Val g-mean: 0.7498116471782977\n",
            "Epoch: 566, Train loss: 0.4957718374542508, Val loss: 0.42703484665406377, Val g-mean: 0.7668949822927327\n",
            "Epoch: 567, Train loss: 0.49903925265714816, Val loss: 0.445918433368206, Val g-mean: 0.7595796314000388\n",
            "Epoch: 568, Train loss: 0.4971174555927633, Val loss: 0.4379226151657732, Val g-mean: 0.7663813643011099\n",
            "Epoch: 569, Train loss: 0.49033224650095986, Val loss: 0.46651655494382505, Val g-mean: 0.7522680646308108\n",
            "Epoch: 570, Train loss: 0.5046463671660922, Val loss: 0.4360146861719458, Val g-mean: 0.7552492251074098\n",
            "Epoch: 571, Train loss: 0.5013895985414493, Val loss: 0.43454741294446747, Val g-mean: 0.7604477712089724\n",
            "Epoch: 572, Train loss: 0.49367297181155056, Val loss: 0.4162078486069253, Val g-mean: 0.7703082765333046\n",
            "Epoch: 573, Train loss: 0.495752672743553, Val loss: 0.43938623937337024, Val g-mean: 0.7621468281901835\n",
            "Epoch: 574, Train loss: 0.503749260264315, Val loss: 0.42952914124256686, Val g-mean: 0.7564441208893927\n",
            "Epoch: 575, Train loss: 0.4982564686933546, Val loss: 0.4268931242588319, Val g-mean: 0.7689314645093887\n",
            "Epoch: 576, Train loss: 0.49562548398762263, Val loss: 0.43126180493517924, Val g-mean: 0.7585189303394355\n",
            "Epoch: 577, Train loss: 0.48946402151328156, Val loss: 0.4264145564091833, Val g-mean: 0.7582953944350029\n",
            "Epoch: 578, Train loss: 0.49928008743517804, Val loss: 0.4348619574386823, Val g-mean: 0.7545858954206491\n",
            "Epoch: 579, Train loss: 0.508123334099429, Val loss: 0.42692991875504194, Val g-mean: 0.7636821484316385\n",
            "Epoch: 580, Train loss: 0.5019277960545551, Val loss: 0.4161606146708915, Val g-mean: 0.7710401227471739\n",
            "Epoch: 581, Train loss: 0.5030505524666862, Val loss: 0.44074360202801854, Val g-mean: 0.7635637933445599\n",
            "Epoch: 582, Train loss: 0.5028094269376883, Val loss: 0.42158209728567225, Val g-mean: 0.767781771666483\n",
            "Epoch: 583, Train loss: 0.495674765992431, Val loss: 0.4359784775266522, Val g-mean: 0.7656533270290222\n",
            "Epoch: 584, Train loss: 0.49862516336504487, Val loss: 0.4319120825905549, Val g-mean: 0.7644835297647127\n",
            "Epoch: 585, Train loss: 0.5023086683825609, Val loss: 0.4246181824097508, Val g-mean: 0.7672898351532743\n",
            "Epoch: 586, Train loss: 0.49214002180746763, Val loss: 0.419130145327041, Val g-mean: 0.7648224404633499\n",
            "Epoch: 587, Train loss: 0.4984259886931567, Val loss: 0.4229152282994044, Val g-mean: 0.7629685306126783\n",
            "Epoch: 588, Train loss: 0.49118989466415997, Val loss: 0.42454112879931927, Val g-mean: 0.7617235152037595\n",
            "Epoch: 589, Train loss: 0.49511612474886657, Val loss: 0.4220845081696385, Val g-mean: 0.769715234759454\n",
            "Epoch: 590, Train loss: 0.49121996238042603, Val loss: 0.4322036553762461, Val g-mean: 0.7701068856215902\n",
            "Epoch: 591, Train loss: 0.5000007229609996, Val loss: 0.41395563730283785, Val g-mean: 0.7710756841959509\n",
            "Epoch: 592, Train loss: 0.49591428846006774, Val loss: 0.43488944047375727, Val g-mean: 0.770733059125728\n",
            "Epoch: 593, Train loss: 0.5003153205466597, Val loss: 0.4211556295815267, Val g-mean: 0.7636340523296469\n",
            "Epoch: 594, Train loss: 0.49493579284285333, Val loss: 0.42581353493426977, Val g-mean: 0.7642268272763612\n",
            "Epoch: 595, Train loss: 0.49788464872877186, Val loss: 0.44084436622889417, Val g-mean: 0.7690313784619336\n",
            "Epoch: 596, Train loss: 0.4990157434733396, Val loss: 0.43906243380747345, Val g-mean: 0.7706245767074552\n",
            "Epoch: 597, Train loss: 0.4996888542107105, Val loss: 0.4414344390756206, Val g-mean: 0.7700303076428239\n",
            "Epoch: 598, Train loss: 0.49002987434971584, Val loss: 0.42627195386510147, Val g-mean: 0.7661487882443159\n",
            "Epoch: 599, Train loss: 0.4955542786316151, Val loss: 0.4317127615213394, Val g-mean: 0.7736807172345218\n",
            "Epoch: 600, Train loss: 0.4947799878251032, Val loss: 0.43737982997768804, Val g-mean: 0.7659236249866547\n",
            "Epoch: 601, Train loss: 0.5030218272275346, Val loss: 0.43913558968587924, Val g-mean: 0.7746491403856569\n",
            "Epoch: 602, Train loss: 0.4976703820230318, Val loss: 0.417560122318958, Val g-mean: 0.7619718842887817\n",
            "Epoch: 603, Train loss: 0.5019409609405664, Val loss: 0.422658180131724, Val g-mean: 0.7566679199489275\n",
            "Epoch: 604, Train loss: 0.4945642178324452, Val loss: 0.44123152839510066, Val g-mean: 0.7577239367030899\n",
            "Epoch: 605, Train loss: 0.496976707653695, Val loss: 0.4156927891859883, Val g-mean: 0.7662532952066731\n",
            "Epoch: 606, Train loss: 0.5016847222592876, Val loss: 0.4259987442116988, Val g-mean: 0.7674135607929863\n",
            "Epoch: 607, Train loss: 0.4944018446289302, Val loss: 0.41715342649503756, Val g-mean: 0.7709299043392644\n",
            "Epoch: 608, Train loss: 0.49591770068303226, Val loss: 0.4508529042727069, Val g-mean: 0.7536309651995707\n",
            "Epoch: 609, Train loss: 0.5053285483978023, Val loss: 0.4319719952580176, Val g-mean: 0.7577923935474566\n",
            "Epoch: 610, Train loss: 0.49794468159336225, Val loss: 0.42042892406645577, Val g-mean: 0.7649794332088862\n",
            "Epoch: 611, Train loss: 0.4986006060386065, Val loss: 0.42518241017272596, Val g-mean: 0.76782256672195\n",
            "Epoch: 612, Train loss: 0.49397844556952875, Val loss: 0.4343008144121421, Val g-mean: 0.7612546103424492\n",
            "Epoch: 613, Train loss: 0.5051402403816804, Val loss: 0.4340725764632225, Val g-mean: 0.7727667225378123\n",
            "Epoch: 614, Train loss: 0.509597063728344, Val loss: 0.4125431605467671, Val g-mean: 0.7639482742613587\n",
            "Epoch: 615, Train loss: 0.4997130130641338, Val loss: 0.4056590740617953, Val g-mean: 0.7669731347448515\n",
            "Epoch: 616, Train loss: 0.4897542402276676, Val loss: 0.4400928326343235, Val g-mean: 0.758975799203364\n",
            "Epoch: 617, Train loss: 0.4931519261711765, Val loss: 0.413619643763492, Val g-mean: 0.7712202729689022\n",
            "Epoch: 618, Train loss: 0.49134318983270914, Val loss: 0.43523627205898885, Val g-mean: 0.7658908000615281\n",
            "Epoch: 619, Train loss: 0.49679653632947074, Val loss: 0.44922275998090444, Val g-mean: 0.7522656123468787\n",
            "Epoch: 620, Train loss: 0.5022340468639627, Val loss: 0.4302271826094703, Val g-mean: 0.7698439126560297\n",
            "Epoch: 621, Train loss: 0.501648805779518, Val loss: 0.43106019653772054, Val g-mean: 0.7526130705308182\n",
            "Epoch: 622, Train loss: 0.49470436003757934, Val loss: 0.4207242791982074, Val g-mean: 0.7591732252776838\n",
            "Epoch: 623, Train loss: 0.49899642703484903, Val loss: 0.42332064576054873, Val g-mean: 0.7613271034345166\n",
            "Epoch: 624, Train loss: 0.4899401481835339, Val loss: 0.43479174865703835, Val g-mean: 0.7522151964183998\n",
            "Epoch: 625, Train loss: 0.4991869694200741, Val loss: 0.42959115222880717, Val g-mean: 0.7627039455352537\n",
            "Epoch: 626, Train loss: 0.4978861739923232, Val loss: 0.43080773969229896, Val g-mean: 0.7551074117995104\n",
            "Epoch: 627, Train loss: 0.4996752399903894, Val loss: 0.4120458699762821, Val g-mean: 0.7658465372766576\n",
            "Epoch: 628, Train loss: 0.4983350915833062, Val loss: 0.4219974006869291, Val g-mean: 0.7627405010748219\n",
            "Epoch: 629, Train loss: 0.49599511681712444, Val loss: 0.4344929844925278, Val g-mean: 0.7673915227528937\n",
            "Epoch: 630, Train loss: 0.5034153370634288, Val loss: 0.4305929288660225, Val g-mean: 0.7648559408182466\n",
            "Epoch: 631, Train loss: 0.5015231078355343, Val loss: 0.44409526139497757, Val g-mean: 0.7730517565467425\n",
            "Epoch: 632, Train loss: 0.4976308149870259, Val loss: 0.4250819757580757, Val g-mean: 0.7671104445755051\n",
            "Epoch: 633, Train loss: 0.5017268362347913, Val loss: 0.4325378013676719, Val g-mean: 0.7636229909045315\n",
            "Epoch: 634, Train loss: 0.49731059529986466, Val loss: 0.41701170645262065, Val g-mean: 0.7688361237977189\n",
            "Epoch: 635, Train loss: 0.5000919460880668, Val loss: 0.43583490287787036, Val g-mean: 0.7605066196907077\n",
            "Epoch: 636, Train loss: 0.4929964096912896, Val loss: 0.4268181169111478, Val g-mean: 0.7701481284638488\n",
            "Epoch: 637, Train loss: 0.49966821270963907, Val loss: 0.43074752919768033, Val g-mean: 0.7747100246786839\n",
            "Epoch: 638, Train loss: 0.5008483482551631, Val loss: 0.41014041594768824, Val g-mean: 0.7632768867437351\n",
            "Epoch: 639, Train loss: 0.48886583341096584, Val loss: 0.4351725041082031, Val g-mean: 0.7591205993061132\n",
            "Epoch: 640, Train loss: 0.49298111103421105, Val loss: 0.43670010900026873, Val g-mean: 0.7573789743414533\n",
            "Epoch: 641, Train loss: 0.498268301681062, Val loss: 0.4335044179307787, Val g-mean: 0.7562725010354453\n",
            "Epoch: 642, Train loss: 0.5002251234858544, Val loss: 0.44353832284870903, Val g-mean: 0.7580217338421512\n",
            "Epoch: 643, Train loss: 0.5024833599914827, Val loss: 0.44519112455217463, Val g-mean: 0.7594582051326842\n",
            "Epoch: 644, Train loss: 0.5010764141882452, Val loss: 0.44539150556451396, Val g-mean: 0.7620799823766362\n",
            "Epoch: 645, Train loss: 0.49912245372932473, Val loss: 0.4222231381817868, Val g-mean: 0.776376814537337\n",
            "Epoch: 646, Train loss: 0.4932882224520051, Val loss: 0.4233411144661276, Val g-mean: 0.7676635339763604\n",
            "Epoch: 647, Train loss: 0.4928366189855233, Val loss: 0.4304635030658622, Val g-mean: 0.7630935868391528\n",
            "Epoch: 648, Train loss: 0.4966739814680341, Val loss: 0.43290798366069794, Val g-mean: 0.7723675196536272\n",
            "Epoch: 649, Train loss: 0.4994732151983117, Val loss: 0.4477758695812602, Val g-mean: 0.7573682202980639\n",
            "Epoch: 650, Train loss: 0.503410202919804, Val loss: 0.4144283699754037, Val g-mean: 0.7716124417847823\n",
            "Epoch: 651, Train loss: 0.49361825492976424, Val loss: 0.4226804915815592, Val g-mean: 0.753695460308411\n",
            "Epoch: 652, Train loss: 0.5059332448942092, Val loss: 0.4549855415366198, Val g-mean: 0.7574129135026193\n",
            "Epoch: 653, Train loss: 0.49490301755097516, Val loss: 0.42658134667496933, Val g-mean: 0.7695809646222622\n",
            "Epoch: 654, Train loss: 0.4972572786627702, Val loss: 0.43062730741344, Val g-mean: 0.7682099422844597\n",
            "Epoch: 655, Train loss: 0.4917835840609474, Val loss: 0.42714562933695943, Val g-mean: 0.7568948669937561\n",
            "Epoch: 656, Train loss: 0.5001582062794107, Val loss: 0.43856210398830864, Val g-mean: 0.7650439407937498\n",
            "Epoch: 657, Train loss: 0.5002054089949667, Val loss: 0.4407484578458886, Val g-mean: 0.7593254545260084\n",
            "Epoch: 658, Train loss: 0.49665869567123977, Val loss: 0.4234061731319678, Val g-mean: 0.775569943351117\n",
            "Epoch: 659, Train loss: 0.4986833839284035, Val loss: 0.40552775052032974, Val g-mean: 0.7628701567490337\n",
            "Epoch: 660, Train loss: 0.5034530372176951, Val loss: 0.4353519533025591, Val g-mean: 0.758373670338563\n",
            "Epoch: 661, Train loss: 0.4939524325120721, Val loss: 0.42315535953170375, Val g-mean: 0.7732347116700867\n",
            "Epoch: 662, Train loss: 0.5010443049888555, Val loss: 0.44014690051737587, Val g-mean: 0.7635868207742144\n",
            "Epoch: 663, Train loss: 0.49844031589653603, Val loss: 0.43496712669730186, Val g-mean: 0.7666382448771525\n",
            "Epoch: 664, Train loss: 0.49557847465795035, Val loss: 0.42610011602702896, Val g-mean: 0.7503401551769634\n",
            "Epoch: 665, Train loss: 0.49389096092909096, Val loss: 0.44849502216828496, Val g-mean: 0.7599824080778932\n",
            "Epoch: 666, Train loss: 0.49920882885699985, Val loss: 0.45418792806173625, Val g-mean: 0.7532635891925651\n",
            "Epoch: 667, Train loss: 0.5063498072731509, Val loss: 0.4115147310260095, Val g-mean: 0.7600340568673648\n",
            "Epoch: 668, Train loss: 0.4916614428761445, Val loss: 0.40959006332253156, Val g-mean: 0.7651739969309211\n",
            "Epoch: 669, Train loss: 0.5013219451057358, Val loss: 0.4325366608406368, Val g-mean: 0.7684680286810605\n",
            "Epoch: 670, Train loss: 0.495229913307772, Val loss: 0.42321738130167913, Val g-mean: 0.7760473442528966\n",
            "Epoch: 671, Train loss: 0.4988717589334842, Val loss: 0.4168128108507709, Val g-mean: 0.7697176565933564\n",
            "Epoch: 672, Train loss: 0.49703562142963, Val loss: 0.4177781888528874, Val g-mean: 0.7693414961959647\n",
            "Epoch: 673, Train loss: 0.498287200897021, Val loss: 0.4416327723547032, Val g-mean: 0.7617803332364478\n",
            "Epoch: 674, Train loss: 0.49661301076331377, Val loss: 0.4341247701331189, Val g-mean: 0.7756383231233623\n",
            "Epoch: 675, Train loss: 0.49881045402662755, Val loss: 0.41801264470345095, Val g-mean: 0.7708294822482664\n",
            "Epoch: 676, Train loss: 0.49858750517472283, Val loss: 0.4297390023344441, Val g-mean: 0.7762299012840589\n",
            "Epoch: 677, Train loss: 0.4944531576715291, Val loss: 0.40926976599975634, Val g-mean: 0.7734564147984562\n",
            "Epoch: 678, Train loss: 0.49910157829230917, Val loss: 0.43788883688026353, Val g-mean: 0.7589068987675505\n",
            "Epoch: 679, Train loss: 0.4967663413363868, Val loss: 0.42210157960653305, Val g-mean: 0.7503072239203057\n",
            "Epoch: 680, Train loss: 0.49318446095072954, Val loss: 0.40195721721178607, Val g-mean: 0.7540032333134445\n",
            "Epoch: 681, Train loss: 0.49846313380758067, Val loss: 0.4007136180604759, Val g-mean: 0.7575785692900902\n",
            "Epoch: 682, Train loss: 0.49743963019459925, Val loss: 0.43288242405182437, Val g-mean: 0.7741690750201282\n",
            "Epoch: 683, Train loss: 0.4979833946772859, Val loss: 0.4335492027825431, Val g-mean: 0.7563357563002862\n",
            "Epoch: 684, Train loss: 0.5020205550710057, Val loss: 0.4386263701476549, Val g-mean: 0.7745136148145308\n",
            "Epoch: 685, Train loss: 0.49302085416703817, Val loss: 0.43097458750401674, Val g-mean: 0.7689882118029193\n",
            "Epoch: 686, Train loss: 0.49848600729965054, Val loss: 0.4310640629969145, Val g-mean: 0.7738940073876732\n",
            "Epoch: 687, Train loss: 0.4957330609896368, Val loss: 0.4250357803937636, Val g-mean: 0.7657755576097411\n",
            "Epoch: 688, Train loss: 0.5012727413405937, Val loss: 0.41903310602432803, Val g-mean: 0.7665361003128517\n",
            "Epoch: 689, Train loss: 0.4961650105783551, Val loss: 0.4132529706939271, Val g-mean: 0.7568901740327592\n",
            "Epoch: 690, Train loss: 0.4955537076907963, Val loss: 0.4242866632381552, Val g-mean: 0.7669001204052242\n",
            "Epoch: 691, Train loss: 0.4989144207514106, Val loss: 0.4349969658804567, Val g-mean: 0.7719783964372904\n",
            "Epoch: 692, Train loss: 0.49553548662672126, Val loss: 0.4536769641073127, Val g-mean: 0.7623530351151333\n",
            "Epoch: 693, Train loss: 0.4950805202829236, Val loss: 0.4247602577272214, Val g-mean: 0.769698067256907\n",
            "Epoch: 694, Train loss: 0.4978280347055182, Val loss: 0.4105678996757457, Val g-mean: 0.767090763252664\n",
            "Epoch: 695, Train loss: 0.504003330232454, Val loss: 0.42677702499847664, Val g-mean: 0.7641698691953857\n",
            "Epoch: 696, Train loss: 0.5032085654547798, Val loss: 0.42650555015394565, Val g-mean: 0.7665133612471843\n",
            "Epoch: 697, Train loss: 0.49937986589683514, Val loss: 0.42601149784106956, Val g-mean: 0.7705980408523256\n",
            "Epoch: 698, Train loss: 0.4941447516431969, Val loss: 0.4178827755937451, Val g-mean: 0.7609944385647988\n",
            "Epoch: 699, Train loss: 0.4911571619284534, Val loss: 0.4414236184797789, Val g-mean: 0.764097285326844\n",
            "Epoch: 700, Train loss: 0.4993917774144295, Val loss: 0.41996617713256884, Val g-mean: 0.7588727820420118\n",
            "Epoch: 701, Train loss: 0.49749843026196616, Val loss: 0.42371637001633644, Val g-mean: 0.77157279195832\n",
            "Epoch: 702, Train loss: 0.49393518994382485, Val loss: 0.42825577623750033, Val g-mean: 0.7730887312061493\n",
            "Epoch: 703, Train loss: 0.4958985375286538, Val loss: 0.4166888569138552, Val g-mean: 0.7657965695778843\n",
            "Epoch: 704, Train loss: 0.49581474987802115, Val loss: 0.4311013006066021, Val g-mean: 0.7595194092741472\n",
            "Epoch: 705, Train loss: 0.49948975860239575, Val loss: 0.43398929779466827, Val g-mean: 0.7466253736851926\n",
            "Epoch: 706, Train loss: 0.4952862263821618, Val loss: 0.43228095064037725, Val g-mean: 0.7690182135024012\n",
            "Epoch: 707, Train loss: 0.49662497012527124, Val loss: 0.4300897665517895, Val g-mean: 0.7500989267598819\n",
            "Epoch: 708, Train loss: 0.4976998667886151, Val loss: 0.43457802483125735, Val g-mean: 0.7684269659117476\n",
            "Epoch: 709, Train loss: 0.497551148923265, Val loss: 0.42610300685230057, Val g-mean: 0.7631296288088202\n",
            "Epoch: 710, Train loss: 0.49547996661621296, Val loss: 0.4210180889226888, Val g-mean: 0.7627249248577517\n",
            "Epoch: 711, Train loss: 0.4919273757990928, Val loss: 0.42045166774799947, Val g-mean: 0.7631078865938246\n",
            "Epoch: 712, Train loss: 0.4941020895620625, Val loss: 0.42558439526903, Val g-mean: 0.7645746563214358\n",
            "Epoch: 713, Train loss: 0.4929956283736136, Val loss: 0.42626449054009036, Val g-mean: 0.770096255990293\n",
            "Epoch: 714, Train loss: 0.4936886094812968, Val loss: 0.4082026269875075, Val g-mean: 0.7689836953311652\n",
            "Epoch: 715, Train loss: 0.4963561326861497, Val loss: 0.4130263959890918, Val g-mean: 0.7646765412774357\n",
            "Epoch: 716, Train loss: 0.49201805348615923, Val loss: 0.4169438277420245, Val g-mean: 0.768199643331701\n",
            "Epoch: 717, Train loss: 0.5001429517947323, Val loss: 0.4312456828591071, Val g-mean: 0.7525956467244961\n",
            "Epoch: 718, Train loss: 0.4989514340431091, Val loss: 0.4198096292583566, Val g-mean: 0.7548829496075375\n",
            "Epoch: 719, Train loss: 0.4899461010293756, Val loss: 0.4128602501985274, Val g-mean: 0.764283394039954\n",
            "Epoch: 720, Train loss: 0.4968789283622686, Val loss: 0.4227845696242232, Val g-mean: 0.762045769711407\n",
            "Epoch: 721, Train loss: 0.4992416141958335, Val loss: 0.4509405997630797, Val g-mean: 0.7579796400464737\n",
            "Epoch: 722, Train loss: 0.4963800536920399, Val loss: 0.4299205353385524, Val g-mean: 0.774534760659059\n",
            "Epoch: 723, Train loss: 0.496631468407234, Val loss: 0.4376559308484981, Val g-mean: 0.7717848757137629\n",
            "Epoch: 724, Train loss: 0.49878638342472337, Val loss: 0.4158400953992417, Val g-mean: 0.7661317379325412\n",
            "Epoch: 725, Train loss: 0.5031626152248373, Val loss: 0.4388846413869607, Val g-mean: 0.7631264452151167\n",
            "Epoch: 726, Train loss: 0.49367793434348856, Val loss: 0.43126723013426127, Val g-mean: 0.7709120930161242\n",
            "Epoch: 727, Train loss: 0.4965948781510945, Val loss: 0.4461792405498655, Val g-mean: 0.761542766032744\n",
            "Epoch: 728, Train loss: 0.4968599214918277, Val loss: 0.4357179277821591, Val g-mean: 0.7545043702972717\n",
            "Epoch: 729, Train loss: 0.5021931430612918, Val loss: 0.4338015276742609, Val g-mean: 0.7615297243959177\n",
            "Epoch: 730, Train loss: 0.5018313430404321, Val loss: 0.4228537700286037, Val g-mean: 0.777470165562553\n",
            "Epoch: 731, Train loss: 0.5010261486043419, Val loss: 0.40950005756396995, Val g-mean: 0.766491933428428\n",
            "Epoch: 732, Train loss: 0.5072955691763965, Val loss: 0.45796291883054535, Val g-mean: 0.7559303215051228\n",
            "Epoch: 733, Train loss: 0.5047829446665791, Val loss: 0.41959256306290627, Val g-mean: 0.7655994920462816\n",
            "Epoch: 734, Train loss: 0.5020189998899436, Val loss: 0.42089635937621717, Val g-mean: 0.7679142375281928\n",
            "Epoch: 735, Train loss: 0.49664046842370785, Val loss: 0.4321291664321172, Val g-mean: 0.7763685136921457\n",
            "Epoch: 736, Train loss: 0.5038389758265108, Val loss: 0.4278990179300308, Val g-mean: 0.7763529734634836\n",
            "Epoch: 737, Train loss: 0.4964082403036484, Val loss: 0.43340566185744184, Val g-mean: 0.7586547651549755\n",
            "Epoch: 738, Train loss: 0.4966056570751953, Val loss: 0.4326285619876887, Val g-mean: 0.7731850701212541\n",
            "Epoch: 739, Train loss: 0.5011915742967826, Val loss: 0.4328952096402645, Val g-mean: 0.7686885920692136\n",
            "Epoch: 740, Train loss: 0.5018559816626454, Val loss: 0.4429568407174788, Val g-mean: 0.7672625228509913\n",
            "Epoch: 741, Train loss: 0.4972416346092698, Val loss: 0.44588843615431534, Val g-mean: 0.7728567614359493\n",
            "Epoch: 742, Train loss: 0.5055356634416593, Val loss: 0.42984402944382866, Val g-mean: 0.7713650520150714\n",
            "Epoch: 743, Train loss: 0.4993099264741948, Val loss: 0.4233533814549446, Val g-mean: 0.773877961032854\n",
            "Epoch: 744, Train loss: 0.5009607264672482, Val loss: 0.42797216282863365, Val g-mean: 0.7650042261490662\n",
            "Epoch: 745, Train loss: 0.5030544381635094, Val loss: 0.4246440488649042, Val g-mean: 0.767544402103815\n",
            "Epoch: 746, Train loss: 0.4932011906548987, Val loss: 0.43540313290922267, Val g-mean: 0.7709133816857341\n",
            "Epoch: 747, Train loss: 0.499312285384182, Val loss: 0.43500661869582374, Val g-mean: 0.7721356534574341\n",
            "Epoch: 748, Train loss: 0.498842072429381, Val loss: 0.4293404115658057, Val g-mean: 0.7536274517145847\n",
            "Epoch: 749, Train loss: 0.4968570709213386, Val loss: 0.44017671460383817, Val g-mean: 0.7600057208040665\n",
            "Epoch: 750, Train loss: 0.4900789714172868, Val loss: 0.42433165151037666, Val g-mean: 0.7696618728478233\n",
            "Epoch: 751, Train loss: 0.49629147234798365, Val loss: 0.4143547284367837, Val g-mean: 0.7669710041489683\n",
            "Epoch: 752, Train loss: 0.505178088781928, Val loss: 0.4280375834358366, Val g-mean: 0.7563216481710172\n",
            "Epoch: 753, Train loss: 0.4966163963298446, Val loss: 0.42045933145441505, Val g-mean: 0.769470942245543\n",
            "Epoch: 754, Train loss: 0.4942619895470259, Val loss: 0.41301245967808525, Val g-mean: 0.7664106317332441\n",
            "Epoch: 755, Train loss: 0.4959587676889335, Val loss: 0.4181333039151995, Val g-mean: 0.7649711844718482\n",
            "Epoch: 756, Train loss: 0.5033391712097919, Val loss: 0.4175474869185372, Val g-mean: 0.7614088219921343\n",
            "Epoch: 757, Train loss: 0.5079171398700031, Val loss: 0.42884069877235514, Val g-mean: 0.75933638833742\n",
            "Epoch: 758, Train loss: 0.4997187508810527, Val loss: 0.4392576268629024, Val g-mean: 0.7573215320611018\n",
            "Epoch: 759, Train loss: 0.49573991631643016, Val loss: 0.43633276263349935, Val g-mean: 0.7659776803334312\n",
            "Epoch: 760, Train loss: 0.49990114201975916, Val loss: 0.4371028450366698, Val g-mean: 0.7616166393997457\n",
            "Epoch: 761, Train loss: 0.49825291169052405, Val loss: 0.45160916094717224, Val g-mean: 0.7599023239816464\n",
            "Epoch: 762, Train loss: 0.5012794963969383, Val loss: 0.42328179049256603, Val g-mean: 0.7633865021210757\n",
            "Epoch: 763, Train loss: 0.4984506296286813, Val loss: 0.4241908725939299, Val g-mean: 0.7628335365422987\n",
            "Epoch: 764, Train loss: 0.4984890604938588, Val loss: 0.42748497955893217, Val g-mean: 0.7695844829907492\n",
            "Epoch: 765, Train loss: 0.4967390795154674, Val loss: 0.4232240244746208, Val g-mean: 0.7668771777706498\n",
            "Epoch: 766, Train loss: 0.4932272110058637, Val loss: 0.4488916252004473, Val g-mean: 0.7698653248337247\n",
            "Epoch: 767, Train loss: 0.49909602961434024, Val loss: 0.4274770650816591, Val g-mean: 0.7572187244027652\n",
            "Epoch: 768, Train loss: 0.5002770524175264, Val loss: 0.4488706680897035, Val g-mean: 0.7544926685230765\n",
            "Epoch: 769, Train loss: 0.5025692396544347, Val loss: 0.44743830377334043, Val g-mean: 0.7646533170574289\n",
            "Epoch: 770, Train loss: 0.49615366159087454, Val loss: 0.4285318263267216, Val g-mean: 0.7742840260524282\n",
            "Epoch: 771, Train loss: 0.49787027922068927, Val loss: 0.42165911727045713, Val g-mean: 0.7718623698965248\n",
            "Epoch: 772, Train loss: 0.4924931976582907, Val loss: 0.4129994803745496, Val g-mean: 0.7664151736677237\n",
            "Epoch: 773, Train loss: 0.4951029219998244, Val loss: 0.40893485691202314, Val g-mean: 0.765524347191263\n",
            "Epoch: 774, Train loss: 0.4960582117730413, Val loss: 0.4218481826154809, Val g-mean: 0.7738230174894511\n",
            "Epoch: 775, Train loss: 0.4983910828318879, Val loss: 0.42971073394935383, Val g-mean: 0.7704257152657095\n",
            "Epoch: 776, Train loss: 0.49962439089879307, Val loss: 0.43184997827598925, Val g-mean: 0.7731457640693707\n",
            "Epoch: 777, Train loss: 0.5027168704098424, Val loss: 0.41757842898368835, Val g-mean: 0.7674668221815534\n",
            "Epoch: 778, Train loss: 0.49858775032462244, Val loss: 0.45433434549915164, Val g-mean: 0.7548909044339686\n",
            "Epoch: 779, Train loss: 0.49376858595507755, Val loss: 0.42729085938710915, Val g-mean: 0.7768991230188887\n",
            "Epoch: 780, Train loss: 0.4954449647674167, Val loss: 0.439896911774811, Val g-mean: 0.7661831468274596\n",
            "Epoch: 781, Train loss: 0.49868646686016777, Val loss: 0.43849196971247073, Val g-mean: 0.7754498522340625\n",
            "Epoch: 782, Train loss: 0.4939829112209613, Val loss: 0.4304009279922435, Val g-mean: 0.7674143801145539\n",
            "Epoch: 783, Train loss: 0.4966266759161304, Val loss: 0.4276532866060734, Val g-mean: 0.7681397838378169\n",
            "Epoch: 784, Train loss: 0.4942724806904901, Val loss: 0.4620783125099383, Val g-mean: 0.762836623812549\n",
            "Epoch: 785, Train loss: 0.5011178100915334, Val loss: 0.4303439696760554, Val g-mean: 0.7583690069477204\n",
            "Epoch: 786, Train loss: 0.4966147741853448, Val loss: 0.42011431271308347, Val g-mean: 0.7732843688732796\n",
            "Epoch: 787, Train loss: 0.4915735295498038, Val loss: 0.43486647405906725, Val g-mean: 0.7772206275226675\n",
            "Epoch: 788, Train loss: 0.4985886636300141, Val loss: 0.4374826681849204, Val g-mean: 0.7625593196332625\n",
            "Epoch: 789, Train loss: 0.4957080039615256, Val loss: 0.4504900308031785, Val g-mean: 0.7648369207382247\n",
            "Epoch: 790, Train loss: 0.5009935447344519, Val loss: 0.437598777640807, Val g-mean: 0.7611126944328386\n",
            "Epoch: 791, Train loss: 0.49105024794938623, Val loss: 0.4118837955358781, Val g-mean: 0.7668040845935606\n",
            "Epoch: 792, Train loss: 0.4981754109682228, Val loss: 0.42633403837680817, Val g-mean: 0.7701420403737299\n",
            "Epoch: 793, Train loss: 0.49437112318167975, Val loss: 0.4358171108914049, Val g-mean: 0.7726957217489583\n",
            "Epoch: 794, Train loss: 0.4889818439637072, Val loss: 0.43316002680282845, Val g-mean: 0.759173956711552\n",
            "Epoch: 795, Train loss: 0.49537906994358083, Val loss: 0.41000505262299586, Val g-mean: 0.7755705671074848\n",
            "Epoch: 796, Train loss: 0.4956311993823594, Val loss: 0.41976293372480494, Val g-mean: 0.770632222939529\n",
            "Epoch: 797, Train loss: 0.49479054649939175, Val loss: 0.43581021341838333, Val g-mean: 0.7587494271122913\n",
            "Epoch: 798, Train loss: 0.49541720884940865, Val loss: 0.44128341796366793, Val g-mean: 0.7655303170301548\n",
            "Epoch: 799, Train loss: 0.49084185067444047, Val loss: 0.41243772777287585, Val g-mean: 0.7610511026111995\n",
            "Epoch: 800, Train loss: 0.5005840921882028, Val loss: 0.4309034598501105, Val g-mean: 0.7688815288701628\n",
            "Epoch: 801, Train loss: 0.49988781926005355, Val loss: 0.41903744481111826, Val g-mean: 0.7687813543688005\n",
            "Epoch: 802, Train loss: 0.49656748445331245, Val loss: 0.4256195208351863, Val g-mean: 0.7729984811082746\n",
            "Epoch: 803, Train loss: 0.4982477308029405, Val loss: 0.4208535442226811, Val g-mean: 0.7652915307448458\n",
            "Epoch: 804, Train loss: 0.49488410244049336, Val loss: 0.4371346768579985, Val g-mean: 0.7688416413469271\n",
            "Epoch: 805, Train loss: 0.49924875792956713, Val loss: 0.44112623208447505, Val g-mean: 0.7737944881236474\n",
            "Epoch: 806, Train loss: 0.49356038750710624, Val loss: 0.45240703597664833, Val g-mean: 0.7590574439367873\n",
            "Epoch: 807, Train loss: 0.49768192945381595, Val loss: 0.4297454431653023, Val g-mean: 0.766094213601398\n",
            "Epoch: 808, Train loss: 0.49689389017027213, Val loss: 0.4402740101673101, Val g-mean: 0.7669179363134964\n",
            "Epoch: 809, Train loss: 0.5026030213024536, Val loss: 0.43811782527911036, Val g-mean: 0.7681021565833329\n",
            "Epoch: 810, Train loss: 0.49192994982105165, Val loss: 0.4341663100609654, Val g-mean: 0.7703521874182533\n",
            "Epoch: 811, Train loss: 0.5132038836808848, Val loss: 0.42826555630094126, Val g-mean: 0.7726676229157107\n",
            "Epoch: 812, Train loss: 0.49961949948974826, Val loss: 0.43956857213848516, Val g-mean: 0.7765580280120148\n",
            "Epoch: 813, Train loss: 0.49881443735918146, Val loss: 0.4441504176510008, Val g-mean: 0.7625822223297622\n",
            "Epoch: 814, Train loss: 0.4898362670438751, Val loss: 0.4293363476662259, Val g-mean: 0.770075097307642\n",
            "Epoch: 815, Train loss: 0.49199031898352474, Val loss: 0.4116067956936987, Val g-mean: 0.7690505837668861\n",
            "Epoch: 816, Train loss: 0.4950924709462735, Val loss: 0.4407685230437078, Val g-mean: 0.7732802397343456\n",
            "Epoch: 817, Train loss: 0.4983116369891157, Val loss: 0.42367866400041077, Val g-mean: 0.7627241210239593\n",
            "Epoch: 818, Train loss: 0.49790207958390836, Val loss: 0.44428381363027974, Val g-mean: 0.7574165290563059\n",
            "Epoch: 819, Train loss: 0.5001627155605561, Val loss: 0.4280053855557191, Val g-mean: 0.767715397491532\n",
            "Epoch: 820, Train loss: 0.5035775380920517, Val loss: 0.43965096634469536, Val g-mean: 0.7557009269651512\n",
            "Epoch: 821, Train loss: 0.49542794514536637, Val loss: 0.4059048502853042, Val g-mean: 0.7668818594477097\n",
            "Epoch: 822, Train loss: 0.5002580318766832, Val loss: 0.4154040648749, Val g-mean: 0.7775422855604073\n",
            "Epoch: 823, Train loss: 0.4967370943905962, Val loss: 0.4087146259844303, Val g-mean: 0.7605712780494724\n",
            "Epoch: 824, Train loss: 0.4955654615286203, Val loss: 0.43564896971771594, Val g-mean: 0.7612192875109316\n",
            "Epoch: 825, Train loss: 0.5059395609563987, Val loss: 0.42906269058585167, Val g-mean: 0.7688069402829865\n",
            "Epoch: 826, Train loss: 0.49041283177248846, Val loss: 0.4186188211958659, Val g-mean: 0.7657302491714436\n",
            "Epoch: 827, Train loss: 0.4929773854704573, Val loss: 0.4172699818093526, Val g-mean: 0.7684876070179735\n",
            "Epoch: 828, Train loss: 0.49509007221641216, Val loss: 0.4283789490398608, Val g-mean: 0.7725141605030347\n",
            "Epoch: 829, Train loss: 0.4954850627890235, Val loss: 0.4337924260057901, Val g-mean: 0.7786442412724203\n",
            "Epoch: 830, Train loss: 0.49582983462933644, Val loss: 0.4299220654525255, Val g-mean: 0.7641571204503544\n",
            "Epoch: 831, Train loss: 0.495044976841181, Val loss: 0.43575037290391166, Val g-mean: 0.7593951288243772\n",
            "Epoch: 832, Train loss: 0.505229789185178, Val loss: 0.43142425308102056, Val g-mean: 0.7551282417024138\n",
            "Epoch: 833, Train loss: 0.4942981699623469, Val loss: 0.42840818080462906, Val g-mean: 0.7714987951261629\n",
            "Epoch: 834, Train loss: 0.5016569097824207, Val loss: 0.42821454237166207, Val g-mean: 0.7693203340829065\n",
            "Epoch: 835, Train loss: 0.4985296990264594, Val loss: 0.4332966926066499, Val g-mean: 0.767877849967636\n",
            "Epoch: 836, Train loss: 0.49456738632444003, Val loss: 0.4348687225658643, Val g-mean: 0.7664044906038179\n",
            "Epoch: 837, Train loss: 0.504515388869801, Val loss: 0.42940348229910197, Val g-mean: 0.7632867861624385\n",
            "Epoch: 838, Train loss: 0.49323886343427814, Val loss: 0.4249461022646804, Val g-mean: 0.7687095955208988\n",
            "Epoch: 839, Train loss: 0.4935540430662371, Val loss: 0.4298199144633193, Val g-mean: 0.7582936282468596\n",
            "Epoch: 840, Train loss: 0.5048842269938173, Val loss: 0.426847178096834, Val g-mean: 0.76286892300985\n",
            "Epoch: 841, Train loss: 0.49189141036847434, Val loss: 0.4373370599197714, Val g-mean: 0.7707325159967152\n",
            "Epoch: 842, Train loss: 0.49546263867349455, Val loss: 0.4136819276762636, Val g-mean: 0.7629782780753451\n",
            "Epoch: 843, Train loss: 0.49971180089046324, Val loss: 0.4332632293042384, Val g-mean: 0.7720721177578742\n",
            "Epoch: 844, Train loss: 0.5006650079571703, Val loss: 0.4411787126017244, Val g-mean: 0.7688575590524026\n",
            "Epoch: 845, Train loss: 0.4978042464227099, Val loss: 0.4560638099516693, Val g-mean: 0.7571308983828274\n",
            "Epoch: 846, Train loss: 0.49083530882150095, Val loss: 0.43049590387626696, Val g-mean: 0.7765501508858053\n",
            "Epoch: 847, Train loss: 0.4951872417746799, Val loss: 0.4429386711042178, Val g-mean: 0.7750404768787733\n",
            "Epoch: 848, Train loss: 0.4924380094059173, Val loss: 0.431461705581138, Val g-mean: 0.7761233662241185\n",
            "Epoch: 849, Train loss: 0.5043468912187581, Val loss: 0.44114503244820397, Val g-mean: 0.7734051686617506\n",
            "Epoch: 850, Train loss: 0.4965063784199414, Val loss: 0.43896268229735524, Val g-mean: 0.7722010166012414\n",
            "Epoch: 851, Train loss: 0.5008601934598137, Val loss: 0.41161204070637103, Val g-mean: 0.7652035012780954\n",
            "Epoch: 852, Train loss: 0.5003323565211376, Val loss: 0.42233482532595334, Val g-mean: 0.7720147656961454\n",
            "Epoch: 853, Train loss: 0.49463628941012705, Val loss: 0.42043288875567286, Val g-mean: 0.7709929635232934\n",
            "Epoch: 854, Train loss: 0.49357147415824976, Val loss: 0.4491886376942459, Val g-mean: 0.7773287304388301\n",
            "Epoch: 855, Train loss: 0.4917331976849647, Val loss: 0.4140111804008484, Val g-mean: 0.7719180311173648\n",
            "Epoch: 856, Train loss: 0.49364940754980063, Val loss: 0.4188127080468755, Val g-mean: 0.7675715150004455\n",
            "Epoch: 857, Train loss: 0.49350619431974857, Val loss: 0.43873910527480275, Val g-mean: 0.7647149797593141\n",
            "Epoch: 858, Train loss: 0.49745345197138097, Val loss: 0.448705041486966, Val g-mean: 0.7545815630450177\n",
            "Epoch: 859, Train loss: 0.49884794262470306, Val loss: 0.4501963916577791, Val g-mean: 0.761715599647113\n",
            "Epoch: 860, Train loss: 0.49412066636049473, Val loss: 0.44854986295104027, Val g-mean: 0.7542042611553188\n",
            "Epoch: 861, Train loss: 0.49879605150472794, Val loss: 0.43160159768242584, Val g-mean: 0.7605069241190305\n",
            "Epoch: 862, Train loss: 0.4920194981851733, Val loss: 0.449352410867026, Val g-mean: 0.7579772706966658\n",
            "Epoch: 863, Train loss: 0.4960035201707392, Val loss: 0.42037127892437737, Val g-mean: 0.7690570149357904\n",
            "Epoch: 864, Train loss: 0.5011198037465384, Val loss: 0.4272969896463971, Val g-mean: 0.768414220089837\n",
            "Epoch: 865, Train loss: 0.49210225938077556, Val loss: 0.424396639983905, Val g-mean: 0.7758445014386004\n",
            "Epoch: 866, Train loss: 0.49548259126468597, Val loss: 0.41174099319859553, Val g-mean: 0.7765506987513587\n",
            "Epoch: 867, Train loss: 0.4976270143262451, Val loss: 0.43733481239331395, Val g-mean: 0.7761269663844182\n",
            "Epoch: 868, Train loss: 0.4970661925748563, Val loss: 0.43571903046808746, Val g-mean: 0.7562557221854331\n",
            "Epoch: 869, Train loss: 0.4927865547241756, Val loss: 0.43232349208311033, Val g-mean: 0.771381980752714\n",
            "Epoch: 870, Train loss: 0.49750314805147416, Val loss: 0.4268479839359459, Val g-mean: 0.7588628402143989\n",
            "Epoch: 871, Train loss: 0.49729915477027953, Val loss: 0.41578173755030884, Val g-mean: 0.7691787216803527\n",
            "Epoch: 872, Train loss: 0.49441741302797426, Val loss: 0.4234565776821814, Val g-mean: 0.7692806008446923\n",
            "Epoch: 873, Train loss: 0.4992501621693868, Val loss: 0.430786437109897, Val g-mean: 0.7670802316294081\n",
            "Epoch: 874, Train loss: 0.5019215279641134, Val loss: 0.418949073866794, Val g-mean: 0.7671620005917539\n",
            "Epoch: 875, Train loss: 0.4976946490214122, Val loss: 0.4150840900838375, Val g-mean: 0.7775752611375006\n",
            "Epoch: 876, Train loss: 0.4937874621916388, Val loss: 0.42007215752413396, Val g-mean: 0.7674355269839295\n",
            "Epoch: 877, Train loss: 0.49264819906147056, Val loss: 0.4323020997016053, Val g-mean: 0.7739419650164102\n",
            "Epoch: 878, Train loss: 0.4891425108402423, Val loss: 0.4218099313346963, Val g-mean: 0.772625558863041\n",
            "Epoch: 879, Train loss: 0.49916528017763884, Val loss: 0.43146101934345144, Val g-mean: 0.7620599546734874\n",
            "Epoch: 880, Train loss: 0.5030762039128742, Val loss: 0.422543415701703, Val g-mean: 0.7674505488399388\n",
            "Epoch: 881, Train loss: 0.5051700953394275, Val loss: 0.43347875048455436, Val g-mean: 0.7572551587364372\n",
            "Epoch: 882, Train loss: 0.49993437027656745, Val loss: 0.4322086619703393, Val g-mean: 0.7702987928737336\n",
            "Epoch: 883, Train loss: 0.4983018031496028, Val loss: 0.4164439467223067, Val g-mean: 0.7686208109115656\n",
            "Epoch: 884, Train loss: 0.49577656385871977, Val loss: 0.4242485485186702, Val g-mean: 0.7682504532332554\n",
            "Epoch: 885, Train loss: 0.5050611990056469, Val loss: 0.42906270607521657, Val g-mean: 0.768705169411047\n",
            "Epoch: 886, Train loss: 0.49840778320860973, Val loss: 0.4210145996981545, Val g-mean: 0.7741096660532658\n",
            "Epoch: 887, Train loss: 0.4985810863892844, Val loss: 0.4260641940330204, Val g-mean: 0.7635893624156626\n",
            "Epoch: 888, Train loss: 0.49155747040301206, Val loss: 0.42761518472903653, Val g-mean: 0.7653445216086378\n",
            "Epoch: 889, Train loss: 0.497358219964512, Val loss: 0.43660519213268634, Val g-mean: 0.7626365009704862\n",
            "Epoch: 890, Train loss: 0.49166767657700355, Val loss: 0.4364710222733648, Val g-mean: 0.7729756766495256\n",
            "Epoch: 891, Train loss: 0.4933659415655238, Val loss: 0.42814076142875773, Val g-mean: 0.7742967750788505\n",
            "Epoch: 892, Train loss: 0.4963660474963693, Val loss: 0.4179463600249667, Val g-mean: 0.7674779847802464\n",
            "Epoch: 893, Train loss: 0.5030713722798587, Val loss: 0.42713285766934095, Val g-mean: 0.7689507925787366\n",
            "Epoch: 894, Train loss: 0.49264833343053427, Val loss: 0.4442523739447719, Val g-mean: 0.7670003022394201\n",
            "Epoch: 895, Train loss: 0.4914581430617196, Val loss: 0.4368799962103367, Val g-mean: 0.7737764423202423\n",
            "Epoch: 896, Train loss: 0.4925516599434071, Val loss: 0.46059907266968175, Val g-mean: 0.7564693319580637\n",
            "Epoch: 897, Train loss: 0.4961717953651032, Val loss: 0.4342506186742532, Val g-mean: 0.7645637374978169\n",
            "Epoch: 898, Train loss: 0.4959314197050084, Val loss: 0.4137846941226407, Val g-mean: 0.766233529761086\n",
            "Epoch: 899, Train loss: 0.49660732800957025, Val loss: 0.4396349638700485, Val g-mean: 0.7702393011477957\n",
            "Epoch: 900, Train loss: 0.49438898251119323, Val loss: 0.4178576075324887, Val g-mean: 0.7691362132263161\n",
            "Epoch: 901, Train loss: 0.4902086206919433, Val loss: 0.43688322289993886, Val g-mean: 0.7570040226255473\n",
            "Epoch: 902, Train loss: 0.5035345709863632, Val loss: 0.417881983283319, Val g-mean: 0.7663781781116339\n",
            "Epoch: 903, Train loss: 0.48996629646091955, Val loss: 0.4393998931505178, Val g-mean: 0.7582765753887104\n",
            "Epoch: 904, Train loss: 0.49876841414671963, Val loss: 0.4175447102047895, Val g-mean: 0.7720169160972062\n",
            "Epoch: 905, Train loss: 0.48965308763183574, Val loss: 0.43487941022766263, Val g-mean: 0.7589241294059021\n",
            "Epoch: 906, Train loss: 0.4910828127927941, Val loss: 0.42197355922115476, Val g-mean: 0.7680460336044798\n",
            "Epoch: 907, Train loss: 0.4931428815567372, Val loss: 0.4155481640053423, Val g-mean: 0.7637301624671995\n",
            "Epoch: 908, Train loss: 0.49792642207951887, Val loss: 0.4338169531210473, Val g-mean: 0.7701899859782413\n",
            "Epoch: 909, Train loss: 0.49373037663782154, Val loss: 0.42657466095529106, Val g-mean: 0.7676909363953263\n",
            "Epoch: 910, Train loss: 0.49515384384615496, Val loss: 0.43412067956830325, Val g-mean: 0.7625606870643614\n",
            "Epoch: 911, Train loss: 0.49469596852038594, Val loss: 0.4459676781767293, Val g-mean: 0.7646094868271842\n",
            "Epoch: 912, Train loss: 0.49364142138424, Val loss: 0.42956562163798434, Val g-mean: 0.7710295326453016\n",
            "Epoch: 913, Train loss: 0.49861647620500765, Val loss: 0.4274664291817891, Val g-mean: 0.7676766911992151\n",
            "Epoch: 914, Train loss: 0.49847223621050035, Val loss: 0.4027672157083687, Val g-mean: 0.7641359683654295\n",
            "Epoch: 915, Train loss: 0.5024746693700655, Val loss: 0.4400998148086824, Val g-mean: 0.76551778658057\n",
            "Epoch: 916, Train loss: 0.4966400501956284, Val loss: 0.4370704779499455, Val g-mean: 0.7785214730768825\n",
            "Epoch: 917, Train loss: 0.4976625213440828, Val loss: 0.43215183639212656, Val g-mean: 0.7683366948875334\n",
            "Epoch: 918, Train loss: 0.49266923903743576, Val loss: 0.43254128686691584, Val g-mean: 0.7757134900490642\n",
            "Epoch: 919, Train loss: 0.4949588191193761, Val loss: 0.4347991688471091, Val g-mean: 0.7735811032049156\n",
            "Epoch: 920, Train loss: 0.4918858126895533, Val loss: 0.4226022635243441, Val g-mean: 0.7681081694797209\n",
            "Epoch: 921, Train loss: 0.49371945548501167, Val loss: 0.4240549332217166, Val g-mean: 0.7695410468644768\n",
            "Epoch: 922, Train loss: 0.49439411563006586, Val loss: 0.4287251873235953, Val g-mean: 0.7746080634460591\n",
            "Epoch: 923, Train loss: 0.49545506171028014, Val loss: 0.42337547860255365, Val g-mean: 0.7640257217168706\n",
            "Epoch: 924, Train loss: 0.5000425805992307, Val loss: 0.4330541811099178, Val g-mean: 0.7650392737568447\n",
            "Epoch: 925, Train loss: 0.4946601718411802, Val loss: 0.432534280576204, Val g-mean: 0.778138973040039\n",
            "Epoch: 926, Train loss: 0.4886803714061671, Val loss: 0.4225339756200188, Val g-mean: 0.7712804767080012\n",
            "Epoch: 927, Train loss: 0.491931229274589, Val loss: 0.4050333601863761, Val g-mean: 0.7631416318069444\n",
            "Epoch: 928, Train loss: 0.4949252644505709, Val loss: 0.43100777758579506, Val g-mean: 0.7706426201421194\n",
            "Epoch: 929, Train loss: 0.4949771159033154, Val loss: 0.42421205204568413, Val g-mean: 0.7682608601511675\n",
            "Epoch: 930, Train loss: 0.4914490596064878, Val loss: 0.4358102018503766, Val g-mean: 0.7506860194549654\n",
            "Epoch: 931, Train loss: 0.5050764319551853, Val loss: 0.42873104524455574, Val g-mean: 0.77419948226329\n",
            "Epoch: 932, Train loss: 0.49359645089886917, Val loss: 0.41166319521634204, Val g-mean: 0.7668244683990214\n",
            "Epoch: 933, Train loss: 0.49185898472378714, Val loss: 0.4403094269923474, Val g-mean: 0.7575945848781028\n",
            "Epoch: 934, Train loss: 0.4992290326298094, Val loss: 0.44347536857975156, Val g-mean: 0.7684452896589603\n",
            "Epoch: 935, Train loss: 0.49315359139740683, Val loss: 0.42214988093627126, Val g-mean: 0.7702281638454661\n",
            "Epoch: 936, Train loss: 0.5019506526998, Val loss: 0.4296596258094436, Val g-mean: 0.7750818006441111\n",
            "Epoch: 937, Train loss: 0.4961560141439274, Val loss: 0.4440450633042737, Val g-mean: 0.7643394791748118\n",
            "Epoch: 938, Train loss: 0.49026467865861045, Val loss: 0.4455603061145858, Val g-mean: 0.7625606800249259\n",
            "Epoch: 939, Train loss: 0.49569173535395, Val loss: 0.4466960004677898, Val g-mean: 0.7585105552352959\n",
            "Epoch: 940, Train loss: 0.49535706923737033, Val loss: 0.4305871003552487, Val g-mean: 0.7614798883624261\n",
            "Epoch: 941, Train loss: 0.4965213686976344, Val loss: 0.4174973966651841, Val g-mean: 0.7690111164654588\n",
            "Epoch: 942, Train loss: 0.4978253625745144, Val loss: 0.41850742834963295, Val g-mean: 0.7708069001002731\n",
            "Epoch: 943, Train loss: 0.4928621471527233, Val loss: 0.42658786377624464, Val g-mean: 0.7734020971960127\n",
            "Epoch: 944, Train loss: 0.4963545818093655, Val loss: 0.41990740714888825, Val g-mean: 0.7701528673712786\n",
            "Epoch: 945, Train loss: 0.499450443656202, Val loss: 0.43408293316238805, Val g-mean: 0.771434213537196\n",
            "Epoch: 946, Train loss: 0.4931806160815702, Val loss: 0.4393596233505952, Val g-mean: 0.769607122950606\n",
            "Epoch: 947, Train loss: 0.49349601915480834, Val loss: 0.4218864215439872, Val g-mean: 0.7640066474869314\n",
            "Epoch: 948, Train loss: 0.4908982496645895, Val loss: 0.43119709428988007, Val g-mean: 0.7764795242398718\n",
            "Epoch: 949, Train loss: 0.48849080811777357, Val loss: 0.42123649681085035, Val g-mean: 0.7628982360987021\n",
            "Epoch: 950, Train loss: 0.49783615423159605, Val loss: 0.4315872182579417, Val g-mean: 0.7746193297909773\n",
            "Epoch: 951, Train loss: 0.4934871207603542, Val loss: 0.4335969121832597, Val g-mean: 0.7546487514986825\n",
            "Epoch: 952, Train loss: 0.49574102699476574, Val loss: 0.42340348111955745, Val g-mean: 0.7659446055766921\n",
            "Epoch: 953, Train loss: 0.4990506686504781, Val loss: 0.4112235346907063, Val g-mean: 0.7694457975388179\n",
            "Epoch: 954, Train loss: 0.49930721578298976, Val loss: 0.42285680457165364, Val g-mean: 0.7639910293156116\n",
            "Epoch: 955, Train loss: 0.4944616587688262, Val loss: 0.43266476435880913, Val g-mean: 0.765772859458044\n",
            "Epoch: 956, Train loss: 0.49848478334966917, Val loss: 0.4340185772039388, Val g-mean: 0.7789128833146596\n",
            "Epoch: 957, Train loss: 0.4943552540838257, Val loss: 0.44610659836938504, Val g-mean: 0.7590128551855159\n",
            "Epoch: 958, Train loss: 0.4941736944074681, Val loss: 0.44351172682486084, Val g-mean: 0.7574829100855139\n",
            "Epoch: 959, Train loss: 0.4894635713611654, Val loss: 0.40976886706132637, Val g-mean: 0.7677151846293243\n",
            "Epoch: 960, Train loss: 0.5036422964159716, Val loss: 0.4142733559404549, Val g-mean: 0.7729808700818481\n",
            "Epoch: 961, Train loss: 0.5066151573371053, Val loss: 0.4189543924049327, Val g-mean: 0.772231341292536\n",
            "Epoch: 962, Train loss: 0.49366765182821154, Val loss: 0.4383251800348884, Val g-mean: 0.7771400216771372\n",
            "Epoch: 963, Train loss: 0.5023852733043224, Val loss: 0.433248228540546, Val g-mean: 0.7728624848166501\n",
            "Epoch: 964, Train loss: 0.5038506588582078, Val loss: 0.41195443076522725, Val g-mean: 0.7636805935659944\n",
            "Epoch: 965, Train loss: 0.49261371373409735, Val loss: 0.43066307725875, Val g-mean: 0.7706475207652895\n",
            "Epoch: 966, Train loss: 0.4907046692620439, Val loss: 0.43932413152958216, Val g-mean: 0.7697403814022353\n",
            "Epoch: 967, Train loss: 0.4997507290282028, Val loss: 0.4261206286518197, Val g-mean: 0.770006315978631\n",
            "Epoch: 968, Train loss: 0.4955262587683195, Val loss: 0.4192275201019488, Val g-mean: 0.7644189274565256\n",
            "Epoch: 969, Train loss: 0.4956421178846422, Val loss: 0.41683848163014964, Val g-mean: 0.7666595816599336\n",
            "Epoch: 970, Train loss: 0.49723841621757825, Val loss: 0.42599014741809743, Val g-mean: 0.7722450540810917\n",
            "Epoch: 971, Train loss: 0.49940985882341893, Val loss: 0.4322514381063612, Val g-mean: 0.7712783372162534\n",
            "Epoch: 972, Train loss: 0.49151604281289013, Val loss: 0.43365724129896416, Val g-mean: 0.7724454443308126\n",
            "Epoch: 973, Train loss: 0.4965641087885548, Val loss: 0.4458713974607618, Val g-mean: 0.7654327670238985\n",
            "Epoch: 974, Train loss: 0.49157516079669417, Val loss: 0.43079210131576184, Val g-mean: 0.770432116500116\n",
            "Epoch: 975, Train loss: 0.5023940827547262, Val loss: 0.4432194838790517, Val g-mean: 0.7705542423070726\n",
            "Epoch: 976, Train loss: 0.4951840153548246, Val loss: 0.41439856980976303, Val g-mean: 0.7744014591903559\n",
            "Epoch: 977, Train loss: 0.4916105053697059, Val loss: 0.43010567520794113, Val g-mean: 0.7726721901351062\n",
            "Epoch: 978, Train loss: 0.4974098395931983, Val loss: 0.4216850178414269, Val g-mean: 0.7714093942744793\n",
            "Epoch: 979, Train loss: 0.4906820589294252, Val loss: 0.42841341013186857, Val g-mean: 0.7666365022912415\n",
            "Epoch: 980, Train loss: 0.49024510523203757, Val loss: 0.437794117355033, Val g-mean: 0.7635810989603847\n",
            "Epoch: 981, Train loss: 0.49490326254527917, Val loss: 0.3999357707798481, Val g-mean: 0.7605787053186459\n",
            "Epoch: 982, Train loss: 0.4927349254250948, Val loss: 0.4231659338662499, Val g-mean: 0.7677676387719294\n",
            "Epoch: 983, Train loss: 0.48709450783635555, Val loss: 0.41703463483013603, Val g-mean: 0.7700867500448045\n",
            "Epoch: 984, Train loss: 0.499213569682498, Val loss: 0.4373950034772095, Val g-mean: 0.7696701545279473\n",
            "Epoch: 985, Train loss: 0.5012450506635313, Val loss: 0.4161729808700712, Val g-mean: 0.7703475930314522\n",
            "Epoch: 986, Train loss: 0.4964569571518788, Val loss: 0.4167861742408652, Val g-mean: 0.7640630659479397\n",
            "Epoch: 987, Train loss: 0.4990091380396012, Val loss: 0.4264813902738847, Val g-mean: 0.7606863939745538\n",
            "Epoch: 988, Train loss: 0.4988348822764099, Val loss: 0.4273134550373805, Val g-mean: 0.7613168674534954\n",
            "Epoch: 989, Train loss: 0.5013977243214625, Val loss: 0.42025250745447057, Val g-mean: 0.7661273364508837\n",
            "Epoch: 990, Train loss: 0.4944555891545974, Val loss: 0.43964395750510066, Val g-mean: 0.7533274032530279\n",
            "Epoch: 991, Train loss: 0.49587393782900185, Val loss: 0.4310707693037234, Val g-mean: 0.7581364226819798\n",
            "Epoch: 992, Train loss: 0.4953882634070794, Val loss: 0.4283537023553723, Val g-mean: 0.7613141817790802\n",
            "Epoch: 993, Train loss: 0.49953854460315555, Val loss: 0.41748207474225446, Val g-mean: 0.768833691262845\n",
            "Epoch: 994, Train loss: 0.4968675909279722, Val loss: 0.4278051298307745, Val g-mean: 0.7769143539996669\n",
            "Epoch: 995, Train loss: 0.5013092433094951, Val loss: 0.4151249047564833, Val g-mean: 0.7725742528577355\n",
            "Epoch: 996, Train loss: 0.497033262657178, Val loss: 0.41906850353667613, Val g-mean: 0.766307144146002\n",
            "Epoch: 997, Train loss: 0.4948954729763837, Val loss: 0.40659544734578384, Val g-mean: 0.7581026166494472\n",
            "Epoch: 998, Train loss: 0.5009372708790348, Val loss: 0.41584587959866776, Val g-mean: 0.7711628353394363\n",
            "Epoch: 999, Train loss: 0.49770885124957576, Val loss: 0.43713394807357536, Val g-mean: 0.7602886705198569\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlrNw0bEqAeh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(classifier):\n",
        "  if torch.cuda.is_available():\n",
        "    classifier = classifier.cuda()\n",
        "  train_gmean, _ = calc_gmean(classifier, train_dataloader)\n",
        "  test_gmean, test_recalls = calc_gmean(classifier, test_dataloader)\n",
        "  print('Epoch: {}'.format(classifier.epoch))\n",
        "  print('Train g-mean: {}'.format(train_gmean.item()))\n",
        "  print('Val g-mean: {}'.format(classifier.val_gmean))\n",
        "  print('Test g-mean: {}, recalls: {}'.format(test_gmean.item(), test_recalls))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFoBTASdvqRv",
        "colab_type": "code",
        "outputId": "5a0d8e1e-7bfb-4532-cbb6-47b1298cce7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print('Best classifier')\n",
        "evaluate(load_classifier())"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best classifier\n",
            "Epoch: 242\n",
            "Train g-mean: 0.777369367674029\n",
            "Val g-mean: 0.7820709490874402\n",
            "Test g-mean: 0.7369236129898312, recalls: [0.76328873 0.71146919]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "918KM5F07hFl",
        "colab_type": "code",
        "outputId": "775d2012-6e24-4ffd-eb88-6f159c12082e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print('Last classifier')\n",
        "evaluate(classifier)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Last classifier\n",
            "Epoch: 999\n",
            "Train g-mean: 0.7858457412375744\n",
            "Val g-mean: 0.7602886705198569\n",
            "Test g-mean: 0.7168680331342272, recalls: [0.76034988 0.67587276]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_t4Gh43GYWl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}