{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JIT-SDP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dinaldoap/jit-sdp-nn/blob/master/notebook/mlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXwvNEAQE6NO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import random\n",
        "from scipy.stats import mstats"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAupgTDfmJ_I",
        "colab_type": "code",
        "outputId": "258f3ded-6a1e-4c27-8316-3de2791fa59c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/dinaldoap/jit-sdp-data/master/jenkins.csv')\n",
        "df.head()"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fix</th>\n",
              "      <th>ns</th>\n",
              "      <th>nd</th>\n",
              "      <th>nf</th>\n",
              "      <th>entropy</th>\n",
              "      <th>la</th>\n",
              "      <th>ld</th>\n",
              "      <th>lt</th>\n",
              "      <th>ndev</th>\n",
              "      <th>age</th>\n",
              "      <th>nuc</th>\n",
              "      <th>exp</th>\n",
              "      <th>rexp</th>\n",
              "      <th>sexp</th>\n",
              "      <th>author_date_unix_timestamp</th>\n",
              "      <th>classification</th>\n",
              "      <th>contains_bug</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>False</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.641604</td>\n",
              "      <td>9.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>426.428571</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.000093</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5171.0</td>\n",
              "      <td>30.227271</td>\n",
              "      <td>1472.714286</td>\n",
              "      <td>1555326371</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>False</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.750000</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>426.428571</td>\n",
              "      <td>100.0</td>\n",
              "      <td>6.314775</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5170.0</td>\n",
              "      <td>29.227271</td>\n",
              "      <td>1471.714286</td>\n",
              "      <td>1555326363</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>False</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.906580</td>\n",
              "      <td>15.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>96.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.034722</td>\n",
              "      <td>2.0</td>\n",
              "      <td>629.0</td>\n",
              "      <td>14.828373</td>\n",
              "      <td>414.000000</td>\n",
              "      <td>1554971763</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>False</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.058824</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1554969774</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>False</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.662506</td>\n",
              "      <td>14.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>67.000000</td>\n",
              "      <td>6.0</td>\n",
              "      <td>21.280683</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.058824</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1554967752</td>\n",
              "      <td>Feature Addition</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     fix   ns   nd  ...  author_date_unix_timestamp    classification  contains_bug\n",
              "0  False  7.0  7.0  ...                  1555326371              None         False\n",
              "1  False  7.0  7.0  ...                  1555326363              None         False\n",
              "2  False  1.0  1.0  ...                  1554971763              None         False\n",
              "3  False  1.0  1.0  ...                  1554969774              None         False\n",
              "4  False  1.0  2.0  ...                  1554967752  Feature Addition         False\n",
              "\n",
              "[5 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alizaxdPD3El",
        "colab_type": "code",
        "outputId": "5dbc887d-0d11-4551-f2a2-b75d3e08d3b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "#df = df.sample(frac=1)\n",
        "label_col = 'contains_bug'\n",
        "features_cols = ['fix', 'ns', 'nd', 'nf', 'entropy', 'la', 'ld', 'lt', 'ndev', 'age', 'nuc', 'exp', 'rexp', 'sexp', 'classification']\n",
        "x = df[features_cols]\n",
        "x['fix'] = x['fix'].astype('int')\n",
        "df_classification = pd.get_dummies(x, columns=['classification'])\n",
        "x = pd.concat([x, df_classification], axis='columns')\n",
        "x = x.drop(['classification'], axis='columns')\n",
        "x = x.values\n",
        "y = df[label_col]\n",
        "y = y.astype('category')\n",
        "y = y.cat.codes\n",
        "y = y.values"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBD1-tp2ExzQ",
        "colab_type": "code",
        "outputId": "f1f8549c-b11d-468b-ecca-f065cf5182ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "print(x[:3])\n",
        "print(y[:3])"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.00000000e+00 7.00000000e+00 7.00000000e+00 7.00000000e+00\n",
            "  2.64160417e+00 9.00000000e+00 9.00000000e+00 4.26428571e+02\n",
            "  1.00000000e+02 9.25925926e-05 1.00000000e+00 5.17100000e+03\n",
            "  3.02272705e+01 1.47271429e+03 0.00000000e+00 7.00000000e+00\n",
            "  7.00000000e+00 7.00000000e+00 2.64160417e+00 9.00000000e+00\n",
            "  9.00000000e+00 4.26428571e+02 1.00000000e+02 9.25925926e-05\n",
            "  1.00000000e+00 5.17100000e+03 3.02272705e+01 1.47271429e+03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 7.00000000e+00 7.00000000e+00 7.00000000e+00\n",
            "  2.75000000e+00 8.00000000e+00 8.00000000e+00 4.26428571e+02\n",
            "  1.00000000e+02 6.31477513e+00 2.00000000e+00 5.17000000e+03\n",
            "  2.92272705e+01 1.47171429e+03 0.00000000e+00 7.00000000e+00\n",
            "  7.00000000e+00 7.00000000e+00 2.75000000e+00 8.00000000e+00\n",
            "  8.00000000e+00 4.26428571e+02 1.00000000e+02 6.31477513e+00\n",
            "  2.00000000e+00 5.17000000e+03 2.92272705e+01 1.47171429e+03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 1.00000000e+00 2.00000000e+00\n",
            "  9.06579548e-01 1.50000000e+01 4.40000000e+01 9.60000000e+01\n",
            "  4.00000000e+00 3.47222222e-02 2.00000000e+00 6.29000000e+02\n",
            "  1.48283729e+01 4.14000000e+02 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 2.00000000e+00 9.06579548e-01 1.50000000e+01\n",
            "  4.40000000e+01 9.60000000e+01 4.00000000e+00 3.47222222e-02\n",
            "  2.00000000e+00 6.29000000e+02 1.48283729e+01 4.14000000e+02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "[0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwCn1uNTMjfG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_index = int( len(x) * 0.8 )\n",
        "test_index = int( len(x) * 0.9 )\n",
        "x_train, y_train = x[:val_index], y[:val_index]\n",
        "x_val, y_val = x[val_index:test_index], y[val_index:test_index]\n",
        "x_test, y_test = x[test_index:], y[test_index:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSqnoapE5Poq",
        "colab_type": "code",
        "outputId": "8d1b9360-d80a-43aa-ec2d-be88629eb0fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(x_train[:3])\n",
        "print(x_val[:3])\n",
        "print(x_test[:3])"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.00000000e+00 7.00000000e+00 7.00000000e+00 7.00000000e+00\n",
            "  2.64160417e+00 9.00000000e+00 9.00000000e+00 4.26428571e+02\n",
            "  1.00000000e+02 9.25925926e-05 1.00000000e+00 5.17100000e+03\n",
            "  3.02272705e+01 1.47271429e+03 0.00000000e+00 7.00000000e+00\n",
            "  7.00000000e+00 7.00000000e+00 2.64160417e+00 9.00000000e+00\n",
            "  9.00000000e+00 4.26428571e+02 1.00000000e+02 9.25925926e-05\n",
            "  1.00000000e+00 5.17100000e+03 3.02272705e+01 1.47271429e+03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 7.00000000e+00 7.00000000e+00 7.00000000e+00\n",
            "  2.75000000e+00 8.00000000e+00 8.00000000e+00 4.26428571e+02\n",
            "  1.00000000e+02 6.31477513e+00 2.00000000e+00 5.17000000e+03\n",
            "  2.92272705e+01 1.47171429e+03 0.00000000e+00 7.00000000e+00\n",
            "  7.00000000e+00 7.00000000e+00 2.75000000e+00 8.00000000e+00\n",
            "  8.00000000e+00 4.26428571e+02 1.00000000e+02 6.31477513e+00\n",
            "  2.00000000e+00 5.17000000e+03 2.92272705e+01 1.47171429e+03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 1.00000000e+00 2.00000000e+00\n",
            "  9.06579548e-01 1.50000000e+01 4.40000000e+01 9.60000000e+01\n",
            "  4.00000000e+00 3.47222222e-02 2.00000000e+00 6.29000000e+02\n",
            "  1.48283729e+01 4.14000000e+02 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 2.00000000e+00 9.06579548e-01 1.50000000e+01\n",
            "  4.40000000e+01 9.60000000e+01 4.00000000e+00 3.47222222e-02\n",
            "  2.00000000e+00 6.29000000e+02 1.48283729e+01 4.14000000e+02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "[[0.00000000e+00 8.00000000e+00 8.00000000e+00 8.00000000e+00\n",
            "  2.73215889e+00 1.10000000e+01 1.10000000e+01 1.58625000e+02\n",
            "  5.00000000e+00 1.96759259e-04 1.00000000e+00 4.17300000e+03\n",
            "  2.45570735e+02 8.19125000e+02 0.00000000e+00 8.00000000e+00\n",
            "  8.00000000e+00 8.00000000e+00 2.73215889e+00 1.10000000e+01\n",
            "  1.10000000e+01 1.58625000e+02 5.00000000e+00 1.96759259e-04\n",
            "  1.00000000e+00 4.17300000e+03 2.45570735e+02 8.19125000e+02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 8.00000000e+00 8.00000000e+00 8.00000000e+00\n",
            "  2.73215889e+00 1.10000000e+01 1.10000000e+01 1.58625000e+02\n",
            "  5.00000000e+00 4.29604022e+00 5.00000000e+00 4.17200000e+03\n",
            "  2.44570735e+02 8.18125000e+02 0.00000000e+00 8.00000000e+00\n",
            "  8.00000000e+00 8.00000000e+00 2.73215889e+00 1.10000000e+01\n",
            "  1.10000000e+01 1.58625000e+02 5.00000000e+00 4.29604022e+00\n",
            "  5.00000000e+00 4.17200000e+03 2.44570735e+02 8.18125000e+02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 1.00000000e+00 1.07000000e+02\n",
            "  1.00000000e+00 5.29513889e-01 1.00000000e+00 4.17100000e+03\n",
            "  2.43572058e+02 8.40000000e+01 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.07000000e+02 1.00000000e+00 5.29513889e-01\n",
            "  1.00000000e+00 4.17100000e+03 2.43572058e+02 8.40000000e+01\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "[[0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 1.00000000e+00 2.31000000e+02\n",
            "  2.00000000e+00 8.17129630e-03 1.00000000e+00 2.29000000e+03\n",
            "  2.35138387e+02 1.96900000e+03 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 2.31000000e+02 2.00000000e+00 8.17129630e-03\n",
            "  1.00000000e+00 2.29000000e+03 2.35138387e+02 1.96900000e+03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 3.00000000e+00 4.00000000e+00\n",
            "  1.92417435e+00 1.00000000e+01 4.00000000e+00 7.95000000e+01\n",
            "  2.00000000e+00 7.59004630e-01 4.00000000e+00 2.28900000e+03\n",
            "  2.34145689e+02 1.96800000e+03 0.00000000e+00 1.00000000e+00\n",
            "  3.00000000e+00 4.00000000e+00 1.92417435e+00 1.00000000e+01\n",
            "  4.00000000e+00 7.95000000e+01 2.00000000e+00 7.59004630e-01\n",
            "  4.00000000e+00 2.28900000e+03 2.34145689e+02 1.96800000e+03\n",
            "  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 5.00000000e+00 4.00000000e+00 2.10000000e+01\n",
            "  1.00000000e+00 5.92943287e+00 1.00000000e+00 2.28800000e+03\n",
            "  2.33161190e+02 1.96700000e+03 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 5.00000000e+00\n",
            "  4.00000000e+00 2.10000000e+01 1.00000000e+00 5.92943287e+00\n",
            "  1.00000000e+00 2.28800000e+03 2.33161190e+02 1.96700000e+03\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46STsbti3AmT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean_train = x_train.mean(axis=0)\n",
        "std_train = x_train.std(axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5vvbrrK3PES",
        "colab_type": "code",
        "outputId": "260a8426-5c89-464b-9263-34d6325b03dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "print(mean_train)\n",
        "print(std_train)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2.90871477e-01 1.61432635e+00 2.60858547e+00 4.18908715e+00\n",
            " 7.43390526e-01 5.00800621e+01 2.96331523e+01 8.11968929e+02\n",
            " 2.32963538e+01 9.76994760e+01 2.02296354e+00 1.73095790e+03\n",
            " 6.53773667e+01 8.28714959e+02 2.90871477e-01 1.61432635e+00\n",
            " 2.60858547e+00 4.18908715e+00 7.43390526e-01 5.00800621e+01\n",
            " 2.96331523e+01 8.11968929e+02 2.32963538e+01 9.76994760e+01\n",
            " 2.02296354e+00 1.73095790e+03 6.53773667e+01 8.28714959e+02\n",
            " 2.90871477e-01 1.47142488e-01 3.46521852e-02 4.71373157e-01\n",
            " 1.51538660e-02 4.08068270e-02]\n",
            "[4.54164354e-01 1.62736223e+00 6.93052365e+00 2.56616350e+01\n",
            " 1.13236779e+00 5.86250473e+02 6.19852440e+02 1.15236538e+03\n",
            " 2.84287684e+01 2.37802272e+02 7.67653474e+00 2.11471132e+03\n",
            " 6.96820390e+01 1.34456679e+03 4.54164354e-01 1.62736223e+00\n",
            " 6.93052365e+00 2.56616350e+01 1.13236779e+00 5.86250473e+02\n",
            " 6.19852440e+02 1.15236538e+03 2.84287684e+01 2.37802272e+02\n",
            " 7.67653474e+00 2.11471132e+03 6.96820390e+01 1.34456679e+03\n",
            " 4.54164354e-01 3.54247902e-01 1.82897270e-01 4.99179831e-01\n",
            " 1.22164751e-01 1.97842437e-01]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2IoFA5h425Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = (x_train - mean_train) / std_train\n",
        "x_val = (x_val - mean_train) / std_train\n",
        "x_test = (x_test - mean_train) / std_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1AoKNoU5V1w",
        "colab_type": "code",
        "outputId": "7fb018c2-42dd-4dc9-840f-51ad47eca730",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(x_train[:3])\n",
        "print(x_val[:3])\n",
        "print(x_test[:3])"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-6.40454219e-01  3.30944983e+00  6.33633872e-01  1.09537559e-01\n",
            "   1.67632253e+00 -7.00725440e-02 -3.32872003e-02 -3.34564335e-01\n",
            "   2.69809951e+00 -4.10842935e-01 -1.33258504e-01  1.62671948e+00\n",
            "  -5.04435529e-01  4.78964178e-01 -6.40454219e-01  3.30944983e+00\n",
            "   6.33633872e-01  1.09537559e-01  1.67632253e+00 -7.00725440e-02\n",
            "  -3.32872003e-02 -3.34564335e-01  2.69809951e+00 -4.10842935e-01\n",
            "  -1.33258504e-01  1.62671948e+00 -5.04435529e-01  4.78964178e-01\n",
            "  -6.40454219e-01 -4.15365869e-01 -1.89462561e-01  1.05899079e+00\n",
            "  -1.24044505e-01 -2.06259221e-01]\n",
            " [-6.40454219e-01  3.30944983e+00  6.33633872e-01  1.09537559e-01\n",
            "   1.77204746e+00 -7.17782996e-02 -3.49004875e-02 -3.34564335e-01\n",
            "   2.69809951e+00 -3.84288594e-01 -2.99139370e-03  1.62624660e+00\n",
            "  -5.18786429e-01  4.78220444e-01 -6.40454219e-01  3.30944983e+00\n",
            "   6.33633872e-01  1.09537559e-01  1.77204746e+00 -7.17782996e-02\n",
            "  -3.49004875e-02 -3.34564335e-01  2.69809951e+00 -3.84288594e-01\n",
            "  -2.99139370e-03  1.62624660e+00 -5.18786429e-01  4.78220444e-01\n",
            "  -6.40454219e-01 -4.15365869e-01 -1.89462561e-01  1.05899079e+00\n",
            "  -1.24044505e-01 -2.06259221e-01]\n",
            " [-6.40454219e-01 -3.77498225e-01 -2.32101577e-01 -8.53058331e-02\n",
            "   1.44113090e-01 -5.98380107e-02  2.31778513e-02 -6.21303747e-01\n",
            "  -6.78761509e-01 -4.10697311e-01 -2.99139370e-03 -5.21091408e-01\n",
            "  -7.25423575e-01 -3.08437605e-01 -6.40454219e-01 -3.77498225e-01\n",
            "  -2.32101577e-01 -8.53058331e-02  1.44113090e-01 -5.98380107e-02\n",
            "   2.31778513e-02 -6.21303747e-01 -6.78761509e-01 -4.10697311e-01\n",
            "  -2.99139370e-03 -5.21091408e-01 -7.25423575e-01 -3.08437605e-01\n",
            "  -6.40454219e-01 -4.15365869e-01 -1.89462561e-01  1.05899079e+00\n",
            "  -1.24044505e-01 -2.06259221e-01]]\n",
            "[[-0.64045422  3.92394118  0.77792311  0.14850624  1.75629188 -0.06666103\n",
            "  -0.03006063 -0.566959   -0.64358587 -0.4108425  -0.1332585   1.15478745\n",
            "   2.58593708 -0.00713238 -0.64045422  3.92394118  0.77792311  0.14850624\n",
            "   1.75629188 -0.06666103 -0.03006063 -0.566959   -0.64358587 -0.4108425\n",
            "  -0.1332585   1.15478745  2.58593708 -0.00713238 -0.64045422 -0.41536587\n",
            "  -0.18946256  1.05899079 -0.1240445  -0.20625922]\n",
            " [-0.64045422  3.92394118  0.77792311  0.14850624  1.75629188 -0.06666103\n",
            "  -0.03006063 -0.566959   -0.64358587 -0.39277773  0.38780994  1.15431457\n",
            "   2.57158618 -0.00787611 -0.64045422  3.92394118  0.77792311  0.14850624\n",
            "   1.75629188 -0.06666103 -0.03006063 -0.566959   -0.64358587 -0.39277773\n",
            "   0.38780994  1.15431457  2.57158618 -0.00787611 -0.64045422 -0.41536587\n",
            "  -0.18946256  1.05899079 -0.1240445  -0.20625922]\n",
            " [-0.64045422 -0.37749822 -0.23210158 -0.12427451 -0.65649211 -0.08371859\n",
            "  -0.0461935  -0.61175816 -0.78428842 -0.40861663 -0.1332585   1.1538417\n",
            "   2.55725426 -0.55386982 -0.64045422 -0.37749822 -0.23210158 -0.12427451\n",
            "  -0.65649211 -0.08371859 -0.0461935  -0.61175816 -0.78428842 -0.40861663\n",
            "  -0.1332585   1.1538417   2.55725426 -0.55386982 -0.64045422 -0.41536587\n",
            "  -0.18946256  1.05899079 -0.1240445  -0.20625922]]\n",
            "[[-0.64045422 -0.37749822 -0.23210158 -0.12427451 -0.65649211 -0.08371859\n",
            "  -0.0461935  -0.5041534  -0.74911278 -0.41080896 -0.1332585   0.26435859\n",
            "   2.43622349  0.84806872 -0.64045422 -0.37749822 -0.23210158 -0.12427451\n",
            "  -0.65649211 -0.08371859 -0.0461935  -0.5041534  -0.74911278 -0.41080896\n",
            "  -0.1332585   0.26435859  2.43622349  0.84806872 -0.64045422 -0.41536587\n",
            "  -0.18946256  1.05899079 -0.1240445  -0.20625922]\n",
            " [-0.64045422 -0.37749822  0.05647691 -0.00736848  1.04275646 -0.06836679\n",
            "  -0.04135364 -0.63562212 -0.74911278 -0.40765158  0.25754283  0.26388571\n",
            "   2.42197738  0.84732499 -0.64045422 -0.37749822  0.05647691 -0.00736848\n",
            "   1.04275646 -0.06836679 -0.04135364 -0.63562212 -0.74911278 -0.40765158\n",
            "   0.25754283  0.26388571  2.42197738  0.84732499 -0.64045422  2.40751606\n",
            "  -0.18946256 -0.94429528 -0.1240445  -0.20625922]\n",
            " [-0.64045422 -0.37749822 -0.23210158 -0.12427451 -0.65649211 -0.07689557\n",
            "  -0.04135364 -0.68638727 -0.78428842 -0.38590903 -0.1332585   0.26341283\n",
            "   2.40784893  0.84658126 -0.64045422 -0.37749822 -0.23210158 -0.12427451\n",
            "  -0.65649211 -0.07689557 -0.04135364 -0.68638727 -0.78428842 -0.38590903\n",
            "  -0.1332585   0.26341283  2.40784893  0.84658126 -0.64045422 -0.41536587\n",
            "   5.27808762 -0.94429528 -0.1240445  -0.20625922]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNcuSWBQp2Ua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_sampler(y):\n",
        "  _, counts = np.unique(y, return_counts=True)\n",
        "  n_samples = len(y)\n",
        "  class_weights = n_samples / counts\n",
        "  weights = class_weights[y]\n",
        "  fade = reversed(range(n_samples))\n",
        "  fading_factor = 1 #0.999999\n",
        "  fade = [fading_factor**x for x in (fade)]\n",
        "  weights = weights * fade\n",
        "  return data.WeightedRandomSampler(weights=weights, num_samples=n_samples, replacement=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fh6O6Nz6uC51",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sampler = create_sampler(y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iP6L6GN22_bR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, y_train = torch.from_numpy(x_train), torch.from_numpy(y_train)\n",
        "x_val, y_val = torch.from_numpy(x_val), torch.from_numpy(y_val)\n",
        "x_test, y_test = torch.from_numpy(x_test), torch.from_numpy(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XnnrzRjL3IP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = data.TensorDataset(x_train, y_train)\n",
        "val_dataset = data.TensorDataset(x_val, y_val)\n",
        "test_dataset = data.TensorDataset(x_test, y_test)\n",
        "\n",
        "train_dataloader = data.DataLoader(train_dataset, batch_size=512, sampler=sampler)\n",
        "val_dataloader = data.DataLoader(val_dataset, batch_size=32)\n",
        "test_dataloader = data.DataLoader(test_dataset, batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHoY9-JcORo-",
        "colab_type": "code",
        "outputId": "80d59ef7-370a-4a65-e397-b40bf1be430c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "source": [
        "inputs, targets = next(iter(train_dataloader))\n",
        "print(inputs[:3])\n",
        "print(targets[:3])\n",
        "\n",
        "inputs, targets = next(iter(val_dataloader))\n",
        "print(inputs[:3])\n",
        "print(targets[:3])\n",
        "\n",
        "inputs, targets = next(iter(test_dataloader))\n",
        "print(inputs[:3])\n",
        "print(targets[:3])"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 1.5614,  0.2370,  0.2008, -0.0074,  0.7450, -0.0240, -0.0042,  1.7282,\n",
            "          2.8036, -0.2230,  0.2575, -0.7003, -0.5025, -0.5234,  1.5614,  0.2370,\n",
            "          0.2008, -0.0074,  0.7450, -0.0240, -0.0042,  1.7282,  2.8036, -0.2230,\n",
            "          0.2575, -0.7003, -0.5025, -0.5234,  1.5614, -0.4154, -0.1895, -0.9443,\n",
            "         -0.1240, -0.2063],\n",
            "        [ 1.5614, -0.3775, -0.2321, -0.1243, -0.6565, -0.0837, -0.0462, -0.4226,\n",
            "         -0.5029,  0.1034, -0.1333, -0.8185, -0.9382, -0.6163,  1.5614, -0.3775,\n",
            "         -0.2321, -0.1243, -0.6565, -0.0837, -0.0462, -0.4226, -0.5029,  0.1034,\n",
            "         -0.1333, -0.8185, -0.9382, -0.6163,  1.5614, -0.4154, -0.1895, -0.9443,\n",
            "         -0.1240, -0.2063],\n",
            "        [-0.6405, -0.3775, -0.2321, -0.1243, -0.6565, -0.0769, -0.0446, -0.3913,\n",
            "         -0.1511,  0.3088, -0.1333,  0.2634, -0.3782,  0.5625, -0.6405, -0.3775,\n",
            "         -0.2321, -0.1243, -0.6565, -0.0769, -0.0446, -0.3913, -0.1511,  0.3088,\n",
            "         -0.1333,  0.2634, -0.3782,  0.5625, -0.6405, -0.4154, -0.1895,  1.0590,\n",
            "         -0.1240, -0.2063]], dtype=torch.float64)\n",
            "tensor([1, 0, 0], dtype=torch.int8)\n",
            "tensor([[-0.6405,  3.9239,  0.7779,  0.1485,  1.7563, -0.0667, -0.0301, -0.5670,\n",
            "         -0.6436, -0.4108, -0.1333,  1.1548,  2.5859, -0.0071, -0.6405,  3.9239,\n",
            "          0.7779,  0.1485,  1.7563, -0.0667, -0.0301, -0.5670, -0.6436, -0.4108,\n",
            "         -0.1333,  1.1548,  2.5859, -0.0071, -0.6405, -0.4154, -0.1895,  1.0590,\n",
            "         -0.1240, -0.2063],\n",
            "        [-0.6405,  3.9239,  0.7779,  0.1485,  1.7563, -0.0667, -0.0301, -0.5670,\n",
            "         -0.6436, -0.3928,  0.3878,  1.1543,  2.5716, -0.0079, -0.6405,  3.9239,\n",
            "          0.7779,  0.1485,  1.7563, -0.0667, -0.0301, -0.5670, -0.6436, -0.3928,\n",
            "          0.3878,  1.1543,  2.5716, -0.0079, -0.6405, -0.4154, -0.1895,  1.0590,\n",
            "         -0.1240, -0.2063],\n",
            "        [-0.6405, -0.3775, -0.2321, -0.1243, -0.6565, -0.0837, -0.0462, -0.6118,\n",
            "         -0.7843, -0.4086, -0.1333,  1.1538,  2.5573, -0.5539, -0.6405, -0.3775,\n",
            "         -0.2321, -0.1243, -0.6565, -0.0837, -0.0462, -0.6118, -0.7843, -0.4086,\n",
            "         -0.1333,  1.1538,  2.5573, -0.5539, -0.6405, -0.4154, -0.1895,  1.0590,\n",
            "         -0.1240, -0.2063]], dtype=torch.float64)\n",
            "tensor([0, 0, 0], dtype=torch.int8)\n",
            "tensor([[-0.6405, -0.3775, -0.2321, -0.1243, -0.6565, -0.0837, -0.0462, -0.5042,\n",
            "         -0.7491, -0.4108, -0.1333,  0.2644,  2.4362,  0.8481, -0.6405, -0.3775,\n",
            "         -0.2321, -0.1243, -0.6565, -0.0837, -0.0462, -0.5042, -0.7491, -0.4108,\n",
            "         -0.1333,  0.2644,  2.4362,  0.8481, -0.6405, -0.4154, -0.1895,  1.0590,\n",
            "         -0.1240, -0.2063],\n",
            "        [-0.6405, -0.3775,  0.0565, -0.0074,  1.0428, -0.0684, -0.0414, -0.6356,\n",
            "         -0.7491, -0.4077,  0.2575,  0.2639,  2.4220,  0.8473, -0.6405, -0.3775,\n",
            "          0.0565, -0.0074,  1.0428, -0.0684, -0.0414, -0.6356, -0.7491, -0.4077,\n",
            "          0.2575,  0.2639,  2.4220,  0.8473, -0.6405,  2.4075, -0.1895, -0.9443,\n",
            "         -0.1240, -0.2063],\n",
            "        [-0.6405, -0.3775, -0.2321, -0.1243, -0.6565, -0.0769, -0.0414, -0.6864,\n",
            "         -0.7843, -0.3859, -0.1333,  0.2634,  2.4078,  0.8466, -0.6405, -0.3775,\n",
            "         -0.2321, -0.1243, -0.6565, -0.0769, -0.0414, -0.6864, -0.7843, -0.3859,\n",
            "         -0.1333,  0.2634,  2.4078,  0.8466, -0.6405, -0.4154,  5.2781, -0.9443,\n",
            "         -0.1240, -0.2063]], dtype=torch.float64)\n",
            "tensor([1, 1, 0], dtype=torch.int8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-GctbL8E1oA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Classifier(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, drop_prob, epoch=None, val_gmean=None):\n",
        "    super(Classifier, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.drop_prob = drop_prob\n",
        "    self.epoch = epoch\n",
        "    self.val_gmean = val_gmean\n",
        "    self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "    self.fcout = nn.Linear(hidden_size, 1)\n",
        "    self.dropout = nn.Dropout(drop_prob)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = torch.relu(self.fc1(x))\n",
        "    x = self.dropout(x)\n",
        "    x = torch.sigmoid(self.fcout(x))\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inpSePkGLTIs",
        "colab_type": "code",
        "outputId": "2ce4fa75-1901-401c-d7e8-2aa7f4f7a3d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "classifier = Classifier(input_size=x.shape[1], hidden_size=x.shape[1], drop_prob=0.5)\n",
        "classifier"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Classifier(\n",
              "  (fc1): Linear(in_features=34, out_features=34, bias=True)\n",
              "  (fcout): Linear(in_features=34, out_features=1, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsT5eIrNoXAh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = 'classifier.cpt'\n",
        "def save_classifier(classifier):\n",
        "  checkpoint = {\n",
        "      'input_size': classifier.input_size,\n",
        "      'hidden_size': classifier.hidden_size,\n",
        "      'drop_prob': classifier.drop_prob,\n",
        "      'val_gmean': classifier.val_gmean,\n",
        "      'epoch': classifier.epoch,\n",
        "      'state_dict': classifier.state_dict()\n",
        "  }\n",
        "  with open(filename, 'wb') as f:\n",
        "    torch.save(checkpoint, f)\n",
        "\n",
        "def load_classifier():\n",
        "  with open(filename, 'rb') as f:\n",
        "    checkpoint = torch.load(f)\n",
        "  classifier = Classifier(checkpoint['input_size'], checkpoint['hidden_size'], checkpoint['drop_prob'], checkpoint['epoch'], checkpoint['val_gmean'])\n",
        "  classifier.load_state_dict(checkpoint['state_dict'])\n",
        "  return classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LX_eofKUq0ok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_classifier(classifier)\n",
        "classifier = load_classifier()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PR8s5ZA5rX-V",
        "colab_type": "code",
        "outputId": "fd3788d8-39c4-4bca-8f54-48ee45d8d301",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  classifier = classifier.cuda()\n",
        "  inputs = inputs.cuda()\n",
        "\n",
        "classifier(inputs[:3].float())"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3267],\n",
              "        [0.2422],\n",
              "        [0.5549]], device='cuda:0', grad_fn=<SigmoidBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arYOr5lMLYy7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(params=classifier.parameters(), lr=0.003)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXw0R-Tkucgw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_loss(classifier, dataloader):\n",
        "  with torch.no_grad():\n",
        "    classifier.eval()\n",
        "    loss = 0\n",
        "    for inputs, targets in dataloader:\n",
        "      if torch.cuda.is_available():\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "      outputs = classifier(inputs.float())\n",
        "      batch_loss = criterion(outputs.squeeze(), targets.float())\n",
        "      loss += batch_loss.item()\n",
        "\n",
        "    return loss / len(dataloader)\n",
        "\n",
        "def calc_recalls(targets, predictions):\n",
        "  n_classes = 2\n",
        "  confusion_matrix, _, _ = np.histogram2d(targets.detach().cpu().numpy(), predictions.squeeze().detach().cpu().numpy(), bins=n_classes)\n",
        "  recalls = np.diag(confusion_matrix) / ((np.sum(confusion_matrix, axis=1)) + 1e-12)\n",
        "  return recalls\n",
        "\n",
        "def calc_gmean(classifier, dataloader):\n",
        "  with torch.no_grad():\n",
        "    classifier.eval()\n",
        "    recalls = np.zeros((2))    \n",
        "    for inputs, targets in dataloader:\n",
        "      if torch.cuda.is_available():\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "      \n",
        "      outputs = classifier(inputs.float())\n",
        "      predictions = torch.round(outputs).int()\n",
        "      recalls += calc_recalls(targets, predictions)\n",
        "    recalls = recalls / len(dataloader)\n",
        "    return mstats.gmean(recalls)    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFy03p96Lza1",
        "colab_type": "code",
        "outputId": "1bba81a4-9c65-4b72-db71-877394194e55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_loss = 0\n",
        "epochs = 200\n",
        "for epoch in range(epochs):\n",
        "  classifier.train()\n",
        "  for inputs, targets in train_dataloader:\n",
        "    if torch.cuda.is_available():\n",
        "      inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "    outputs = classifier(inputs.float())\n",
        "    loss = criterion(outputs.squeeze(), targets.float())\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  train_loss = train_loss / len(train_dataloader)\n",
        "  val_loss = calc_loss(classifier, val_dataloader)\n",
        "  val_gmean = calc_gmean(classifier, val_dataloader)\n",
        "  print('Epoch: {}, Train loss: {}, Val loss: {}, Val g-mean: {}'.format(epoch, train_loss, val_loss, val_gmean))\n",
        "\n",
        "  if classifier.val_gmean is None or val_gmean > classifier.val_gmean:\n",
        "    classifier.epoch = epoch\n",
        "    classifier.val_gmean = val_gmean\n",
        "    save_classifier(classifier)\n",
        "\n",
        "classifier.epoch = epoch\n",
        "classifier.val_gmean = val_gmean"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Train loss: 0.6491969870893579, Val loss: 0.6429272317572644, Val g-mean: 0.6423976661943214\n",
            "Epoch: 1, Train loss: 0.6279411175987397, Val loss: 0.6474884724930713, Val g-mean: 0.6568415541242986\n",
            "Epoch: 2, Train loss: 0.6113454303698092, Val loss: 0.6199725376147973, Val g-mean: 0.6720486274688406\n",
            "Epoch: 3, Train loss: 0.5995705948627736, Val loss: 0.6018250486568401, Val g-mean: 0.676607057281924\n",
            "Epoch: 4, Train loss: 0.5935236394048699, Val loss: 0.5918367665849233, Val g-mean: 0.6880979126243536\n",
            "Epoch: 5, Train loss: 0.5851511473709057, Val loss: 0.6079338922312385, Val g-mean: 0.6827955374760384\n",
            "Epoch: 6, Train loss: 0.5822624614240143, Val loss: 0.6302640669439968, Val g-mean: 0.6598565484170602\n",
            "Epoch: 7, Train loss: 0.5754042520083916, Val loss: 0.5961661887796301, Val g-mean: 0.6849910413072388\n",
            "Epoch: 8, Train loss: 0.5720597920029779, Val loss: 0.5939275763536754, Val g-mean: 0.6928907234447111\n",
            "Epoch: 9, Train loss: 0.5623286126402632, Val loss: 0.5863124739966894, Val g-mean: 0.7048044954089041\n",
            "Epoch: 10, Train loss: 0.5672605799958048, Val loss: 0.5937113812879512, Val g-mean: 0.6974581059731759\n",
            "Epoch: 11, Train loss: 0.5626484881559091, Val loss: 0.5717499299268973, Val g-mean: 0.7125122905855172\n",
            "Epoch: 12, Train loss: 0.5604734141855885, Val loss: 0.5603471723826308, Val g-mean: 0.7226527273941549\n",
            "Epoch: 13, Train loss: 0.5582201599008928, Val loss: 0.5603912790354929, Val g-mean: 0.7110502411747206\n",
            "Epoch: 14, Train loss: 0.5573881146126857, Val loss: 0.5503876334742496, Val g-mean: 0.7244752087529599\n",
            "Epoch: 15, Train loss: 0.562056617979454, Val loss: 0.5218044593930244, Val g-mean: 0.7229742647717321\n",
            "Epoch: 16, Train loss: 0.558488309044698, Val loss: 0.5484990508932817, Val g-mean: 0.7218143493710774\n",
            "Epoch: 17, Train loss: 0.5485159315471864, Val loss: 0.5309957222718942, Val g-mean: 0.7283006379596806\n",
            "Epoch: 18, Train loss: 0.5487093898230608, Val loss: 0.5372881093307545, Val g-mean: 0.7251661464720538\n",
            "Epoch: 19, Train loss: 0.5449585576658024, Val loss: 0.5564944155906376, Val g-mean: 0.727896085003579\n",
            "Epoch: 20, Train loss: 0.5414800384060452, Val loss: 0.5333676785230637, Val g-mean: 0.7255135499391607\n",
            "Epoch: 21, Train loss: 0.5486456737142277, Val loss: 0.5423056483268738, Val g-mean: 0.71541682952312\n",
            "Epoch: 22, Train loss: 0.5388580842635649, Val loss: 0.5327387303113937, Val g-mean: 0.7312184277040509\n",
            "Epoch: 23, Train loss: 0.5438455452621549, Val loss: 0.5208347718182363, Val g-mean: 0.7281304597784822\n",
            "Epoch: 24, Train loss: 0.5406049485010551, Val loss: 0.5479819437390879, Val g-mean: 0.7320876339355079\n",
            "Epoch: 25, Train loss: 0.5377502637343518, Val loss: 0.5151408365682552, Val g-mean: 0.7321060801783625\n",
            "Epoch: 26, Train loss: 0.5438918696088668, Val loss: 0.515696940453429, Val g-mean: 0.7381986527680784\n",
            "Epoch: 27, Train loss: 0.5457562433686253, Val loss: 0.5009573203952689, Val g-mean: 0.7353660534013771\n",
            "Epoch: 28, Train loss: 0.5337874228677373, Val loss: 0.5056057627263822, Val g-mean: 0.7351199468651329\n",
            "Epoch: 29, Train loss: 0.5330519368641241, Val loss: 0.4959098167325321, Val g-mean: 0.7357897380084337\n",
            "Epoch: 30, Train loss: 0.5302371523631055, Val loss: 0.5200090055402956, Val g-mean: 0.7320462411620573\n",
            "Epoch: 31, Train loss: 0.5289363841264197, Val loss: 0.5199844272513139, Val g-mean: 0.7397645745855064\n",
            "Epoch: 32, Train loss: 0.5380359518947536, Val loss: 0.5333899941883589, Val g-mean: 0.7364133247010868\n",
            "Epoch: 33, Train loss: 0.537147559430574, Val loss: 0.5042475650185033, Val g-mean: 0.7382710810413804\n",
            "Epoch: 34, Train loss: 0.5269681311413482, Val loss: 0.5317462212161014, Val g-mean: 0.7443387241285796\n",
            "Epoch: 35, Train loss: 0.534846893519594, Val loss: 0.5051419397718028, Val g-mean: 0.7428927379165617\n",
            "Epoch: 36, Train loss: 0.5298778451736149, Val loss: 0.5398026163640776, Val g-mean: 0.7409523060221923\n",
            "Epoch: 37, Train loss: 0.5290328061299768, Val loss: 0.48746068893294586, Val g-mean: 0.7425102038470635\n",
            "Epoch: 38, Train loss: 0.5303849031415876, Val loss: 0.5192859388495746, Val g-mean: 0.7414226118772863\n",
            "Epoch: 39, Train loss: 0.5337646581360314, Val loss: 0.5082764249098929, Val g-mean: 0.74053007952681\n",
            "Epoch: 40, Train loss: 0.5312317530774981, Val loss: 0.4729073161357327, Val g-mean: 0.7267642125045153\n",
            "Epoch: 41, Train loss: 0.5291294327965533, Val loss: 0.5013602740670505, Val g-mean: 0.740997787409173\n",
            "Epoch: 42, Train loss: 0.5294206037113309, Val loss: 0.4726316791615988, Val g-mean: 0.7284992265558553\n",
            "Epoch: 43, Train loss: 0.5282303641082666, Val loss: 0.46629322673145096, Val g-mean: 0.7314903829784487\n",
            "Epoch: 44, Train loss: 0.5296727629341625, Val loss: 0.48142022266983986, Val g-mean: 0.7323218637550722\n",
            "Epoch: 45, Train loss: 0.5358638423886162, Val loss: 0.5022456908696576, Val g-mean: 0.7436516488414093\n",
            "Epoch: 46, Train loss: 0.5269704334521667, Val loss: 0.5079707891533249, Val g-mean: 0.7469713719615699\n",
            "Epoch: 47, Train loss: 0.5242902852793127, Val loss: 0.4813872638501619, Val g-mean: 0.744703216157729\n",
            "Epoch: 48, Train loss: 0.5245108861089265, Val loss: 0.4669723222522359, Val g-mean: 0.7412939213734734\n",
            "Epoch: 49, Train loss: 0.5327168380439296, Val loss: 0.5290125220229751, Val g-mean: 0.7425732408328805\n",
            "Epoch: 50, Train loss: 0.5289667012796592, Val loss: 0.47801973553080307, Val g-mean: 0.7456995102695447\n",
            "Epoch: 51, Train loss: 0.519826897828479, Val loss: 0.47288351074645396, Val g-mean: 0.7401323638590804\n",
            "Epoch: 52, Train loss: 0.5270787918998667, Val loss: 0.49283939286282186, Val g-mean: 0.7452138565176727\n",
            "Epoch: 53, Train loss: 0.5234538030322023, Val loss: 0.5178528798646048, Val g-mean: 0.7457221605313854\n",
            "Epoch: 54, Train loss: 0.525573682658755, Val loss: 0.4788829484268239, Val g-mean: 0.7509002393945415\n",
            "Epoch: 55, Train loss: 0.5219245030658518, Val loss: 0.46175037187181023, Val g-mean: 0.749006506870006\n",
            "Epoch: 56, Train loss: 0.5256794983205489, Val loss: 0.49068229645490646, Val g-mean: 0.7465051354760157\n",
            "Epoch: 57, Train loss: 0.5260237596381581, Val loss: 0.49441286647006083, Val g-mean: 0.7416226812063287\n",
            "Epoch: 58, Train loss: 0.5228782158722712, Val loss: 0.4540074216692071, Val g-mean: 0.744915059634808\n",
            "Epoch: 59, Train loss: 0.5325467528001003, Val loss: 0.503426728279967, Val g-mean: 0.743541025822711\n",
            "Epoch: 60, Train loss: 0.5323623989786379, Val loss: 0.49889325723052025, Val g-mean: 0.7519450171746629\n",
            "Epoch: 61, Train loss: 0.5226399427306382, Val loss: 0.46461018370954615, Val g-mean: 0.7506341522249821\n",
            "Epoch: 62, Train loss: 0.5225774986327001, Val loss: 0.46597749230108765, Val g-mean: 0.7437822735272137\n",
            "Epoch: 63, Train loss: 0.5237253497639777, Val loss: 0.47380764076584264, Val g-mean: 0.7484111172412984\n",
            "Epoch: 64, Train loss: 0.5220345059784868, Val loss: 0.47640508707416684, Val g-mean: 0.7504944701595663\n",
            "Epoch: 65, Train loss: 0.5240928039209692, Val loss: 0.5121032072133139, Val g-mean: 0.7450973369587297\n",
            "Epoch: 66, Train loss: 0.5190325876214438, Val loss: 0.47422291652152415, Val g-mean: 0.7479826148766795\n",
            "Epoch: 67, Train loss: 0.5198854228910081, Val loss: 0.4973792033760171, Val g-mean: 0.7458325998011233\n",
            "Epoch: 68, Train loss: 0.5195994777665229, Val loss: 0.4722191567875837, Val g-mean: 0.7450668112334338\n",
            "Epoch: 69, Train loss: 0.5256390778202386, Val loss: 0.47451857555853694, Val g-mean: 0.7504262114441024\n",
            "Epoch: 70, Train loss: 0.5231285837867243, Val loss: 0.49741176398176895, Val g-mean: 0.7460446078558766\n",
            "Epoch: 71, Train loss: 0.5205397154876495, Val loss: 0.4897536788331835, Val g-mean: 0.7532228763066471\n",
            "Epoch: 72, Train loss: 0.5203343837564162, Val loss: 0.47989745202817413, Val g-mean: 0.7538991211690599\n",
            "Epoch: 73, Train loss: 0.5143505559916689, Val loss: 0.4721891372219512, Val g-mean: 0.7440393763386676\n",
            "Epoch: 74, Train loss: 0.52033836022426, Val loss: 0.47453190464722483, Val g-mean: 0.7496554444152077\n",
            "Epoch: 75, Train loss: 0.5156561662689136, Val loss: 0.4623281892977263, Val g-mean: 0.7493081465902737\n",
            "Epoch: 76, Train loss: 0.5191217408105345, Val loss: 0.4677216708660126, Val g-mean: 0.7516127177897872\n",
            "Epoch: 77, Train loss: 0.5227744219740352, Val loss: 0.4616728785790895, Val g-mean: 0.7423138184155422\n",
            "Epoch: 78, Train loss: 0.5232234799496963, Val loss: 0.5028758266646611, Val g-mean: 0.7416903905221564\n",
            "Epoch: 79, Train loss: 0.5188829794359642, Val loss: 0.4791761224991397, Val g-mean: 0.7546470207736791\n",
            "Epoch: 80, Train loss: 0.5290212761802434, Val loss: 0.4595892513661008, Val g-mean: 0.7426576902948174\n",
            "Epoch: 81, Train loss: 0.5211911377453046, Val loss: 0.45681822280350487, Val g-mean: 0.7473684770791275\n",
            "Epoch: 82, Train loss: 0.5208417079875951, Val loss: 0.49411267042160034, Val g-mean: 0.7474589849018389\n",
            "Epoch: 83, Train loss: 0.5253492144169255, Val loss: 0.4589827601846896, Val g-mean: 0.7559489716815391\n",
            "Epoch: 84, Train loss: 0.5185037470894474, Val loss: 0.4591746093019059, Val g-mean: 0.7592197223836354\n",
            "Epoch: 85, Train loss: 0.5154829697573327, Val loss: 0.4860883657085268, Val g-mean: 0.747893836756747\n",
            "Epoch: 86, Train loss: 0.5189987697537639, Val loss: 0.4659324049165374, Val g-mean: 0.7552699676878339\n",
            "Epoch: 87, Train loss: 0.5169663616919097, Val loss: 0.4454294196084926, Val g-mean: 0.7476050311595651\n",
            "Epoch: 88, Train loss: 0.5258632746414751, Val loss: 0.4674396701157093, Val g-mean: 0.7476901237420757\n",
            "Epoch: 89, Train loss: 0.5217911275517428, Val loss: 0.4824938444714797, Val g-mean: 0.7411469073873388\n",
            "Epoch: 90, Train loss: 0.5183104581798679, Val loss: 0.4647164105584747, Val g-mean: 0.7503161546319775\n",
            "Epoch: 91, Train loss: 0.5198579368025733, Val loss: 0.48058777831886945, Val g-mean: 0.7426264294175061\n",
            "Epoch: 92, Train loss: 0.5136950947658814, Val loss: 0.46957918746691, Val g-mean: 0.745290989223436\n",
            "Epoch: 93, Train loss: 0.5207441599165573, Val loss: 0.4866286018177083, Val g-mean: 0.7414083155303001\n",
            "Epoch: 94, Train loss: 0.5192237484637979, Val loss: 0.4704506946237464, Val g-mean: 0.7483830303814413\n",
            "Epoch: 95, Train loss: 0.5223971071687291, Val loss: 0.5040938234642932, Val g-mean: 0.7445510043584738\n",
            "Epoch: 96, Train loss: 0.5184638203833368, Val loss: 0.45910654334645523, Val g-mean: 0.7432189137555326\n",
            "Epoch: 97, Train loss: 0.5128180917983878, Val loss: 0.43585673366722305, Val g-mean: 0.7543905029705162\n",
            "Epoch: 98, Train loss: 0.5148007766115185, Val loss: 0.49915070674921336, Val g-mean: 0.7480272363975512\n",
            "Epoch: 99, Train loss: 0.516785328052827, Val loss: 0.4661360088931887, Val g-mean: 0.7576162785155246\n",
            "Epoch: 100, Train loss: 0.5249650573642208, Val loss: 0.4575762791853202, Val g-mean: 0.7441374248053316\n",
            "Epoch: 101, Train loss: 0.5225474617353425, Val loss: 0.45037518284822764, Val g-mean: 0.7469511299484457\n",
            "Epoch: 102, Train loss: 0.5171618965672593, Val loss: 0.48008654560697706, Val g-mean: 0.754218115950674\n",
            "Epoch: 103, Train loss: 0.5155627050614334, Val loss: 0.49581865123228025, Val g-mean: 0.749403959636321\n",
            "Epoch: 104, Train loss: 0.517736441782211, Val loss: 0.4557995851102628, Val g-mean: 0.7544071623271625\n",
            "Epoch: 105, Train loss: 0.5119056985799444, Val loss: 0.4916334097322665, Val g-mean: 0.7481088212517474\n",
            "Epoch: 106, Train loss: 0.5181064323009733, Val loss: 0.48913369759132985, Val g-mean: 0.7500506150756348\n",
            "Epoch: 107, Train loss: 0.5153941128217905, Val loss: 0.46325189973178665, Val g-mean: 0.7507751396652091\n",
            "Epoch: 108, Train loss: 0.5173053323531458, Val loss: 0.49921015552, Val g-mean: 0.7496837898009946\n",
            "Epoch: 109, Train loss: 0.5180575718986996, Val loss: 0.47547300159931183, Val g-mean: 0.7423163742670656\n",
            "Epoch: 110, Train loss: 0.5216235499384844, Val loss: 0.46474793710206685, Val g-mean: 0.7468655928731064\n",
            "Epoch: 111, Train loss: 0.5111722812338954, Val loss: 0.46194317113412053, Val g-mean: 0.7494182914975853\n",
            "Epoch: 112, Train loss: 0.5171809051781482, Val loss: 0.4813382009926595, Val g-mean: 0.7462486882661415\n",
            "Epoch: 113, Train loss: 0.5161147200076976, Val loss: 0.4470984459315476, Val g-mean: 0.7577484800000679\n",
            "Epoch: 114, Train loss: 0.5214213566377739, Val loss: 0.4620080811804847, Val g-mean: 0.7529153021778124\n",
            "Epoch: 115, Train loss: 0.5114994115379687, Val loss: 0.46802720623581034, Val g-mean: 0.7582208240063995\n",
            "Epoch: 116, Train loss: 0.519560861918872, Val loss: 0.4709630447782968, Val g-mean: 0.7541903093879468\n",
            "Epoch: 117, Train loss: 0.522210692583823, Val loss: 0.44953784503434835, Val g-mean: 0.7439621380632053\n",
            "Epoch: 118, Train loss: 0.518374242417104, Val loss: 0.44980749742765175, Val g-mean: 0.752401078326144\n",
            "Epoch: 119, Train loss: 0.5132987854259078, Val loss: 0.45256000404295166, Val g-mean: 0.7562927535380315\n",
            "Epoch: 120, Train loss: 0.5190208375385374, Val loss: 0.49174622407085017, Val g-mean: 0.7464203750516002\n",
            "Epoch: 121, Train loss: 0.5067483534936607, Val loss: 0.4472954990832429, Val g-mean: 0.7634990398554872\n",
            "Epoch: 122, Train loss: 0.5172028974235175, Val loss: 0.4574076682329178, Val g-mean: 0.7593788139231265\n",
            "Epoch: 123, Train loss: 0.5190830171514501, Val loss: 0.4588775830833535, Val g-mean: 0.7538912690162093\n",
            "Epoch: 124, Train loss: 0.5168668477169489, Val loss: 0.4433846079597348, Val g-mean: 0.7485080425783381\n",
            "Epoch: 125, Train loss: 0.5125957770526195, Val loss: 0.4778011137325513, Val g-mean: 0.7436305899769649\n",
            "Epoch: 126, Train loss: 0.5156245915078798, Val loss: 0.47786738880370794, Val g-mean: 0.7529098594234719\n",
            "Epoch: 127, Train loss: 0.5127692005299259, Val loss: 0.4649288207292557, Val g-mean: 0.7513648910700331\n",
            "Epoch: 128, Train loss: 0.5193155598255174, Val loss: 0.4704587087035179, Val g-mean: 0.7503299604994728\n",
            "Epoch: 129, Train loss: 0.5147000548390238, Val loss: 0.47611932632954496, Val g-mean: 0.7513811110968969\n",
            "Epoch: 130, Train loss: 0.5180234179610425, Val loss: 0.44000901242620066, Val g-mean: 0.752856054151432\n",
            "Epoch: 131, Train loss: 0.5211627770037882, Val loss: 0.45163092330882426, Val g-mean: 0.7506529954861997\n",
            "Epoch: 132, Train loss: 0.5141755012790661, Val loss: 0.4768167563567036, Val g-mean: 0.749425979413877\n",
            "Epoch: 133, Train loss: 0.5155714349477968, Val loss: 0.4485053616134744, Val g-mean: 0.7596998373338294\n",
            "Epoch: 134, Train loss: 0.5145647617194984, Val loss: 0.44754514058953837, Val g-mean: 0.7526558247924661\n",
            "Epoch: 135, Train loss: 0.5220050398509072, Val loss: 0.46462392963861165, Val g-mean: 0.7543466418400018\n",
            "Epoch: 136, Train loss: 0.5163949160303996, Val loss: 0.4836614524063311, Val g-mean: 0.7494888923569485\n",
            "Epoch: 137, Train loss: 0.5187160961683548, Val loss: 0.45834965219623164, Val g-mean: 0.7502219683957523\n",
            "Epoch: 138, Train loss: 0.517853929789589, Val loss: 0.4648230313078353, Val g-mean: 0.7539847456596153\n",
            "Epoch: 139, Train loss: 0.5124126762724688, Val loss: 0.44872797888360527, Val g-mean: 0.7572291065192918\n",
            "Epoch: 140, Train loss: 0.5213551978546714, Val loss: 0.46201036497950554, Val g-mean: 0.74593004950888\n",
            "Epoch: 141, Train loss: 0.5178152703409612, Val loss: 0.47430279654891866, Val g-mean: 0.750849343896246\n",
            "Epoch: 142, Train loss: 0.5227461733921714, Val loss: 0.4672950219951178, Val g-mean: 0.7591139127207329\n",
            "Epoch: 143, Train loss: 0.5137029352379063, Val loss: 0.44646374685199636, Val g-mean: 0.7616357021468351\n",
            "Epoch: 144, Train loss: 0.5047061382223436, Val loss: 0.4574176790682893, Val g-mean: 0.7661959173472798\n",
            "Epoch: 145, Train loss: 0.514871050491901, Val loss: 0.46101231206404536, Val g-mean: 0.7455865385799413\n",
            "Epoch: 146, Train loss: 0.5138320841699047, Val loss: 0.44689464510271426, Val g-mean: 0.7486298704219844\n",
            "Epoch: 147, Train loss: 0.5086524335394543, Val loss: 0.47060567983671237, Val g-mean: 0.7444407518853771\n",
            "Epoch: 148, Train loss: 0.5134040795602564, Val loss: 0.4785927983098908, Val g-mean: 0.7629800494650913\n",
            "Epoch: 149, Train loss: 0.5138119382890645, Val loss: 0.45195762460169037, Val g-mean: 0.7600928590654091\n",
            "Epoch: 150, Train loss: 0.514701815611523, Val loss: 0.4675570889131019, Val g-mean: 0.7558295831595211\n",
            "Epoch: 151, Train loss: 0.5182039212805766, Val loss: 0.4600978215274058, Val g-mean: 0.760851328545008\n",
            "Epoch: 152, Train loss: 0.5159123745707638, Val loss: 0.49942478027782944, Val g-mean: 0.7512084075731843\n",
            "Epoch: 153, Train loss: 0.518661893269336, Val loss: 0.4621064310011111, Val g-mean: 0.7614901189665151\n",
            "Epoch: 154, Train loss: 0.5157040477024075, Val loss: 0.48025939495939957, Val g-mean: 0.7477331981545906\n",
            "Epoch: 155, Train loss: 0.5172971152951491, Val loss: 0.45742929471950783, Val g-mean: 0.7579267398694541\n",
            "Epoch: 156, Train loss: 0.5146126644226809, Val loss: 0.45871433537257345, Val g-mean: 0.759545228850479\n",
            "Epoch: 157, Train loss: 0.5114726343082046, Val loss: 0.459746499987025, Val g-mean: 0.7596602944733233\n",
            "Epoch: 158, Train loss: 0.5178170060858566, Val loss: 0.48396047792936625, Val g-mean: 0.7542524151646957\n",
            "Epoch: 159, Train loss: 0.5143031599640465, Val loss: 0.42891252374178485, Val g-mean: 0.7569619001635371\n",
            "Epoch: 160, Train loss: 0.5162183575005898, Val loss: 0.4587609928689505, Val g-mean: 0.753280496690886\n",
            "Epoch: 161, Train loss: 0.5114474451650955, Val loss: 0.44388903775497485, Val g-mean: 0.7536279430527942\n",
            "Epoch: 162, Train loss: 0.5201615161961364, Val loss: 0.4391645540140177, Val g-mean: 0.7606070100800605\n",
            "Epoch: 163, Train loss: 0.5129480591135748, Val loss: 0.4319532492050999, Val g-mean: 0.7470757047708303\n",
            "Epoch: 164, Train loss: 0.5138790331146109, Val loss: 0.44116644639717906, Val g-mean: 0.7621912453370759\n",
            "Epoch: 165, Train loss: 0.5136631594463947, Val loss: 0.48980196447748886, Val g-mean: 0.7491276173481143\n",
            "Epoch: 166, Train loss: 0.5123149121706024, Val loss: 0.4321635736054496, Val g-mean: 0.7561174602768334\n",
            "Epoch: 167, Train loss: 0.5125481650100056, Val loss: 0.4638374753688511, Val g-mean: 0.7603784779493207\n",
            "Epoch: 168, Train loss: 0.5148779158133842, Val loss: 0.45666263526991796, Val g-mean: 0.7629097665682377\n",
            "Epoch: 169, Train loss: 0.5076466869480217, Val loss: 0.45122350556285756, Val g-mean: 0.7542006859010777\n",
            "Epoch: 170, Train loss: 0.5206580312920422, Val loss: 0.48561414723333557, Val g-mean: 0.7484556374872731\n",
            "Epoch: 171, Train loss: 0.5143549242350951, Val loss: 0.4625580871575757, Val g-mean: 0.7528434637905499\n",
            "Epoch: 172, Train loss: 0.5148825279420235, Val loss: 0.43493429492962987, Val g-mean: 0.7607380134112611\n",
            "Epoch: 173, Train loss: 0.5185138165729758, Val loss: 0.47796733598960073, Val g-mean: 0.7482388666584657\n",
            "Epoch: 174, Train loss: 0.5105227856253781, Val loss: 0.4599760974708356, Val g-mean: 0.7578089338962055\n",
            "Epoch: 175, Train loss: 0.5087254675553953, Val loss: 0.4535658686960998, Val g-mean: 0.7558169548563325\n",
            "Epoch: 176, Train loss: 0.5158926029799452, Val loss: 0.4513842236054571, Val g-mean: 0.7622227538402363\n",
            "Epoch: 177, Train loss: 0.5127049608183484, Val loss: 0.46553720867163256, Val g-mean: 0.7469330893747489\n",
            "Epoch: 178, Train loss: 0.5122699170864271, Val loss: 0.45866486665449646, Val g-mean: 0.7610192839790926\n",
            "Epoch: 179, Train loss: 0.5147256028483747, Val loss: 0.47643558818258736, Val g-mean: 0.7485881731559404\n",
            "Epoch: 180, Train loss: 0.5161799186463691, Val loss: 0.46061117358897863, Val g-mean: 0.7540583385719415\n",
            "Epoch: 181, Train loss: 0.5102247956395033, Val loss: 0.43631440479504435, Val g-mean: 0.7613846982836908\n",
            "Epoch: 182, Train loss: 0.5143629439645688, Val loss: 0.4498163971461748, Val g-mean: 0.7496453942575737\n",
            "Epoch: 183, Train loss: 0.5135072135820036, Val loss: 0.4273127097832529, Val g-mean: 0.759581191808021\n",
            "Epoch: 184, Train loss: 0.5147138919325618, Val loss: 0.4410931711134158, Val g-mean: 0.7618199941979572\n",
            "Epoch: 185, Train loss: 0.5185908253932521, Val loss: 0.4677018764379777, Val g-mean: 0.7595749952732844\n",
            "Epoch: 186, Train loss: 0.5250800876134962, Val loss: 0.4761611495755221, Val g-mean: 0.7514039228110202\n",
            "Epoch: 187, Train loss: 0.5128774235863286, Val loss: 0.47089507077869613, Val g-mean: 0.7540681302180569\n",
            "Epoch: 188, Train loss: 0.5165880954846447, Val loss: 0.44733904851110357, Val g-mean: 0.75843481604604\n",
            "Epoch: 189, Train loss: 0.5167425260119917, Val loss: 0.4606672719513115, Val g-mean: 0.7509854151091025\n",
            "Epoch: 190, Train loss: 0.5071398301740835, Val loss: 0.4441779553890228, Val g-mean: 0.7650509830261844\n",
            "Epoch: 191, Train loss: 0.5189879465621301, Val loss: 0.4577987186218563, Val g-mean: 0.752110121139955\n",
            "Epoch: 192, Train loss: 0.521656251864793, Val loss: 0.447685232484027, Val g-mean: 0.7611993013478342\n",
            "Epoch: 193, Train loss: 0.5244281513366041, Val loss: 0.4397625299660783, Val g-mean: 0.7569928986065656\n",
            "Epoch: 194, Train loss: 0.5178422643502368, Val loss: 0.4512003203363795, Val g-mean: 0.7593545915030206\n",
            "Epoch: 195, Train loss: 0.5177545861553723, Val loss: 0.4578069121037659, Val g-mean: 0.7603740793217331\n",
            "Epoch: 196, Train loss: 0.5157992534369533, Val loss: 0.4693828842749721, Val g-mean: 0.7554842302559043\n",
            "Epoch: 197, Train loss: 0.5219959875275437, Val loss: 0.4728055096378452, Val g-mean: 0.7503361646495738\n",
            "Epoch: 198, Train loss: 0.5178584290709667, Val loss: 0.44700652909906285, Val g-mean: 0.7494356447881134\n",
            "Epoch: 199, Train loss: 0.5248132429969065, Val loss: 0.44129383113039167, Val g-mean: 0.7676645761265135\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlrNw0bEqAeh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(classifier):\n",
        "  if torch.cuda.is_available():\n",
        "    classifier = classifier.cuda()\n",
        "  train_gmean = calc_gmean(classifier, train_dataloader)\n",
        "  test_gmean = calc_gmean(classifier, test_dataloader)\n",
        "  print('Epoch: {}'.format(classifier.epoch))\n",
        "  print('Train g-mean: {}'.format(train_gmean.item()))\n",
        "  print('Val g-mean: {}'.format(classifier.val_gmean))\n",
        "  print('Test g-mean: {}'.format(test_gmean.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFoBTASdvqRv",
        "colab_type": "code",
        "outputId": "9c798e57-bf5c-48a6-c099-11987d482078",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print('Best classifier')\n",
        "evaluate(load_classifier())"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best classifier\n",
            "Epoch: 199\n",
            "Train g-mean: 0.770348364046038\n",
            "Val g-mean: 0.7676645761265135\n",
            "Test g-mean: 0.7371777644322304\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "918KM5F07hFl",
        "colab_type": "code",
        "outputId": "1e4cd932-0c09-48db-ac42-d5024ef1485d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print('Last classifier')\n",
        "evaluate(classifier)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Last classifier\n",
            "Epoch: 199\n",
            "Train g-mean: 0.7705440443778532\n",
            "Val g-mean: 0.7676645761265135\n",
            "Test g-mean: 0.7371777644322304\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_t4Gh43GYWl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}