{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JIT-SDP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dinaldoap/jit-sdp-nn/blob/master/notebook/mlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXwvNEAQE6NO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import random\n",
        "from scipy.stats import mstats"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAupgTDfmJ_I",
        "colab_type": "code",
        "outputId": "5a1cee97-2698-4e03-c7cd-1a7b8913ae3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/dinaldoap/jit-sdp-data/master/jenkins.csv')\n",
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fix</th>\n",
              "      <th>ns</th>\n",
              "      <th>nd</th>\n",
              "      <th>nf</th>\n",
              "      <th>entropy</th>\n",
              "      <th>la</th>\n",
              "      <th>ld</th>\n",
              "      <th>lt</th>\n",
              "      <th>ndev</th>\n",
              "      <th>age</th>\n",
              "      <th>nuc</th>\n",
              "      <th>exp</th>\n",
              "      <th>rexp</th>\n",
              "      <th>sexp</th>\n",
              "      <th>author_date_unix_timestamp</th>\n",
              "      <th>classification</th>\n",
              "      <th>contains_bug</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>False</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.641604</td>\n",
              "      <td>9.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>426.428571</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.000093</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5171.0</td>\n",
              "      <td>30.227271</td>\n",
              "      <td>1472.714286</td>\n",
              "      <td>1555326371</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>False</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.750000</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>426.428571</td>\n",
              "      <td>100.0</td>\n",
              "      <td>6.314775</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5170.0</td>\n",
              "      <td>29.227271</td>\n",
              "      <td>1471.714286</td>\n",
              "      <td>1555326363</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>False</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.906580</td>\n",
              "      <td>15.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>96.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.034722</td>\n",
              "      <td>2.0</td>\n",
              "      <td>629.0</td>\n",
              "      <td>14.828373</td>\n",
              "      <td>414.000000</td>\n",
              "      <td>1554971763</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>False</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.058824</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1554969774</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>False</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.662506</td>\n",
              "      <td>14.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>67.000000</td>\n",
              "      <td>6.0</td>\n",
              "      <td>21.280683</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.058824</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1554967752</td>\n",
              "      <td>Feature Addition</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     fix   ns   nd  ...  author_date_unix_timestamp    classification  contains_bug\n",
              "0  False  7.0  7.0  ...                  1555326371              None         False\n",
              "1  False  7.0  7.0  ...                  1555326363              None         False\n",
              "2  False  1.0  1.0  ...                  1554971763              None         False\n",
              "3  False  1.0  1.0  ...                  1554969774              None         False\n",
              "4  False  1.0  2.0  ...                  1554967752  Feature Addition         False\n",
              "\n",
              "[5 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alizaxdPD3El",
        "colab_type": "code",
        "outputId": "77aa48d6-d954-472d-aa09-9bceb5dd342e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "df = df.sample(frac=1)\n",
        "label_col = 'contains_bug'\n",
        "features_cols = ['fix', 'ns', 'nd', 'nf', 'entropy', 'la', 'ld', 'lt', 'ndev', 'age', 'nuc', 'exp', 'rexp', 'sexp', 'classification']\n",
        "x = df[features_cols]\n",
        "x['fix'] = x['fix'].astype('int')\n",
        "df_classification = pd.get_dummies(x, columns=['classification'])\n",
        "x = pd.concat([x, df_classification], axis='columns')\n",
        "x = x.drop(['classification'], axis='columns')\n",
        "x = x.values\n",
        "y = df[label_col]\n",
        "y = y.astype('category')\n",
        "y = y.cat.codes\n",
        "y = y.values"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBD1-tp2ExzQ",
        "colab_type": "code",
        "outputId": "5ceffde9-b181-4654-d882-2e5516836e97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "print(x[:3])\n",
        "print(y[:3])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 1.00000000e+00 9.15000000e+02\n",
            "  3.40000000e+01 5.10699074e+00 1.00000000e+00 4.03600000e+03\n",
            "  8.70081028e+01 2.43400000e+03 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 9.15000000e+02 3.40000000e+01 5.10699074e+00\n",
            "  1.00000000e+00 4.03600000e+03 8.70081028e+01 2.43400000e+03\n",
            "  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 1.10000000e+01 7.00000000e+00 4.51000000e+02\n",
            "  6.00000000e+00 4.34344907e+00 1.00000000e+00 6.40500000e+03\n",
            "  1.53869726e+02 5.22200000e+03 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 1.10000000e+01\n",
            "  7.00000000e+00 4.51000000e+02 6.00000000e+00 4.34344907e+00\n",
            "  1.00000000e+00 6.40500000e+03 1.53869726e+02 5.22200000e+03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.00000000e+00 2.00000000e+00 2.00000000e+00 2.00000000e+00\n",
            "  9.70950594e-01 4.00000000e+00 1.00000000e+00 2.45500000e+03\n",
            "  7.40000000e+01 8.82973958e+01 2.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 2.00000000e+00\n",
            "  2.00000000e+00 2.00000000e+00 9.70950594e-01 4.00000000e+00\n",
            "  1.00000000e+00 2.45500000e+03 7.40000000e+01 8.82973958e+01\n",
            "  2.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "[0 1 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwCn1uNTMjfG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_index = int( len(x) * 0.8 )\n",
        "test_index = int( len(x) * 0.9 )\n",
        "x_train, y_train = x[:val_index], y[:val_index]\n",
        "x_val, y_val = x[val_index:test_index], y[val_index:test_index]\n",
        "x_test, y_test = x[test_index:], y[test_index:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSqnoapE5Poq",
        "colab_type": "code",
        "outputId": "2e363c43-96df-4ffc-d27e-19ad773177e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(x_train[:3])\n",
        "print(x_val[:3])\n",
        "print(x_test[:3])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 1.00000000e+00 9.15000000e+02\n",
            "  3.40000000e+01 5.10699074e+00 1.00000000e+00 4.03600000e+03\n",
            "  8.70081028e+01 2.43400000e+03 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 9.15000000e+02 3.40000000e+01 5.10699074e+00\n",
            "  1.00000000e+00 4.03600000e+03 8.70081028e+01 2.43400000e+03\n",
            "  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 1.10000000e+01 7.00000000e+00 4.51000000e+02\n",
            "  6.00000000e+00 4.34344907e+00 1.00000000e+00 6.40500000e+03\n",
            "  1.53869726e+02 5.22200000e+03 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 1.10000000e+01\n",
            "  7.00000000e+00 4.51000000e+02 6.00000000e+00 4.34344907e+00\n",
            "  1.00000000e+00 6.40500000e+03 1.53869726e+02 5.22200000e+03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.00000000e+00 2.00000000e+00 2.00000000e+00 2.00000000e+00\n",
            "  9.70950594e-01 4.00000000e+00 1.00000000e+00 2.45500000e+03\n",
            "  7.40000000e+01 8.82973958e+01 2.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 2.00000000e+00\n",
            "  2.00000000e+00 2.00000000e+00 9.70950594e-01 4.00000000e+00\n",
            "  1.00000000e+00 2.45500000e+03 7.40000000e+01 8.82973958e+01\n",
            "  2.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "[[0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 1.00000000e+00 8.84000000e+02\n",
            "  6.30000000e+01 5.70995370e-01 1.00000000e+00 5.85000000e+02\n",
            "  1.35348790e+01 3.80000000e+02 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 8.84000000e+02 6.30000000e+01 5.70995370e-01\n",
            "  1.00000000e+00 5.85000000e+02 1.35348790e+01 3.80000000e+02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 1.00000000e+00 2.00000000e+00\n",
            "  3.53359335e-01 4.20000000e+01 4.80000000e+01 1.20400000e+03\n",
            "  1.10000000e+01 5.10439815e-01 2.00000000e+00 7.67000000e+02\n",
            "  1.22175388e+02 1.17000000e+02 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 2.00000000e+00 3.53359335e-01 4.20000000e+01\n",
            "  4.80000000e+01 1.20400000e+03 1.10000000e+01 5.10439815e-01\n",
            "  2.00000000e+00 7.67000000e+02 1.22175388e+02 1.17000000e+02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.00000000e+00 2.00000000e+00 3.00000000e+00 4.00000000e+00\n",
            "  1.37522063e+00 3.30000000e+01 2.20000000e+01 6.55500000e+02\n",
            "  1.10000000e+01 2.31922381e+02 4.00000000e+00 2.00000000e+01\n",
            "  6.30119048e+00 1.15000000e+01 1.00000000e+00 2.00000000e+00\n",
            "  3.00000000e+00 4.00000000e+00 1.37522063e+00 3.30000000e+01\n",
            "  2.20000000e+01 6.55500000e+02 1.10000000e+01 2.31922381e+02\n",
            "  4.00000000e+00 2.00000000e+01 6.30119048e+00 1.15000000e+01\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "[[0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 3.00000000e+00 0.00000000e+00 2.05000000e+03\n",
            "  8.60000000e+01 1.17178241e+00 1.00000000e+00 1.37000000e+02\n",
            "  1.65326285e+01 3.70000000e+01 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 3.00000000e+00\n",
            "  0.00000000e+00 2.05000000e+03 8.60000000e+01 1.17178241e+00\n",
            "  1.00000000e+00 1.37000000e+02 1.65326285e+01 3.70000000e+01\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 9.00000000e+00 0.00000000e+00 9.90000000e+01\n",
            "  2.00000000e+00 2.29180208e+01 1.00000000e+00 3.29000000e+03\n",
            "  2.18008650e+02 2.87700000e+03 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 9.00000000e+00\n",
            "  0.00000000e+00 9.90000000e+01 2.00000000e+00 2.29180208e+01\n",
            "  1.00000000e+00 3.29000000e+03 2.18008650e+02 2.87700000e+03\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 1.00000000e+00 5.43000000e+02\n",
            "  4.00000000e+00 7.99537037e-01 1.00000000e+00 3.83900000e+03\n",
            "  1.94606712e+02 3.34800000e+03 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 5.43000000e+02 4.00000000e+00 7.99537037e-01\n",
            "  1.00000000e+00 3.83900000e+03 1.94606712e+02 3.34800000e+03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46STsbti3AmT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean_train = x_train.mean(axis=0)\n",
        "std_train = x_train.std(axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5vvbrrK3PES",
        "colab_type": "code",
        "outputId": "8b05b35c-75f1-4498-880a-908d1ef812a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "print(mean_train)\n",
        "print(std_train)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2.77476080e-01 1.60739591e+00 2.51771399e+00 3.91052495e+00\n",
            " 7.36091560e-01 5.29580553e+01 2.43177657e+01 7.08759211e+02\n",
            " 1.90589604e+01 8.25230425e+01 1.90918024e+00 1.74726636e+03\n",
            " 8.53276442e+01 9.02828547e+02 2.77476080e-01 1.60739591e+00\n",
            " 2.51771399e+00 3.91052495e+00 7.36091560e-01 5.29580553e+01\n",
            " 2.43177657e+01 7.08759211e+02 1.90589604e+01 8.25230425e+01\n",
            " 1.90918024e+00 1.74726636e+03 8.53276442e+01 9.02828547e+02\n",
            " 2.77476080e-01 1.58210499e-01 3.50142229e-02 4.79958624e-01\n",
            " 1.54641841e-02 3.38763900e-02]\n",
            "[4.47753397e-01 1.64389526e+00 6.21038647e+00 2.39289237e+01\n",
            " 1.11867691e+00 7.30512812e+02 5.16431750e+02 1.06995963e+03\n",
            " 2.68694733e+01 2.14019615e+02 7.43961996e+00 1.98299249e+03\n",
            " 8.19211779e+01 1.31232638e+03 4.47753397e-01 1.64389526e+00\n",
            " 6.21038647e+00 2.39289237e+01 1.11867691e+00 7.30512812e+02\n",
            " 5.16431750e+02 1.06995963e+03 2.68694733e+01 2.14019615e+02\n",
            " 7.43961996e+00 1.98299249e+03 8.19211779e+01 1.31232638e+03\n",
            " 4.47753397e-01 3.64938265e-01 1.83815742e-01 4.99598182e-01\n",
            " 1.23389802e-01 1.80910973e-01]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2IoFA5h425Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = (x_train - mean_train) / std_train\n",
        "x_val = (x_val - mean_train) / std_train\n",
        "x_test = (x_test - mean_train) / std_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1AoKNoU5V1w",
        "colab_type": "code",
        "outputId": "05408250-bf76-40d0-b09d-1d7036f0af6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        }
      },
      "source": [
        "print(x_train[:3])\n",
        "print(x_val[:3])\n",
        "print(x_test[:3])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.61970737 -0.36948577 -0.24438318 -0.12163209 -0.65800193 -0.07112545\n",
            "  -0.04515169  0.19275567  0.55606001 -0.3617241  -0.12220789  1.1541817\n",
            "   0.02051312  1.16676116 -0.61970737 -0.36948577 -0.24438318 -0.12163209\n",
            "  -0.65800193 -0.07112545 -0.04515169  0.19275567  0.55606001 -0.3617241\n",
            "  -0.12220789  1.1541817   0.02051312  1.16676116 -0.61970737  2.30666275\n",
            "  -0.19048544 -0.96068929 -0.1253279  -0.18725448]\n",
            " [-0.61970737 -0.36948577 -0.24438318 -0.12163209 -0.65800193 -0.05743644\n",
            "  -0.0335335  -0.24090555 -0.48601475 -0.36529172 -0.12220789  2.34884079\n",
            "   0.83668331  3.29123266 -0.61970737 -0.36948577 -0.24438318 -0.12163209\n",
            "  -0.65800193 -0.05743644 -0.0335335  -0.24090555 -0.48601475 -0.36529172\n",
            "  -0.12220789  2.34884079  0.83668331  3.29123266 -0.61970737 -0.43352675\n",
            "  -0.19048544  1.04091927 -0.1253279  -0.18725448]\n",
            " [ 1.61366485  0.23882549 -0.0833626  -0.07984166  0.20994358 -0.06701875\n",
            "  -0.04515169  1.63206232  2.04473824  0.02698049  0.01220758 -0.88112606\n",
            "  -1.04158224 -0.68796037  1.61366485  0.23882549 -0.0833626  -0.07984166\n",
            "   0.20994358 -0.06701875 -0.04515169  1.63206232  2.04473824  0.02698049\n",
            "   0.01220758 -0.88112606 -1.04158224 -0.68796037  1.61366485 -0.43352675\n",
            "  -0.19048544 -0.96068929 -0.1253279  -0.18725448]]\n",
            "[[-0.61970737 -0.36948577 -0.24438318 -0.12163209 -0.65800193 -0.07112545\n",
            "  -0.04515169  0.16378262  1.63535173 -0.38291839 -0.12220789 -0.58611738\n",
            "  -0.87636393 -0.39839826 -0.61970737 -0.36948577 -0.24438318 -0.12163209\n",
            "  -0.65800193 -0.07112545 -0.04515169  0.16378262  1.63535173 -0.38291839\n",
            "  -0.12220789 -0.58611738 -0.87636393 -0.39839826 -0.61970737 -0.43352675\n",
            "  -0.19048544  1.04091927 -0.1253279  -0.18725448]\n",
            " [-0.61970737 -0.36948577 -0.24438318 -0.07984166 -0.34212937 -0.0150005\n",
            "   0.04585743  0.46285932 -0.29992997 -0.38320134  0.01220758 -0.4943369\n",
            "   0.44979509 -0.59880572 -0.61970737 -0.36948577 -0.24438318 -0.07984166\n",
            "  -0.34212937 -0.0150005   0.04585743  0.46285932 -0.29992997 -0.38320134\n",
            "   0.01220758 -0.4943369   0.44979509 -0.59880572 -0.61970737 -0.43352675\n",
            "  -0.19048544  1.04091927 -0.1253279  -0.18725448]\n",
            " [ 1.61366485  0.23882549  0.07765797  0.0037392   0.57132588 -0.02732061\n",
            "  -0.00448804 -0.04977684 -0.29992997  0.69806377  0.28103852 -0.87104029\n",
            "  -0.96466452 -0.67919731  1.61366485  0.23882549  0.07765797  0.0037392\n",
            "   0.57132588 -0.02732061 -0.00448804 -0.04977684 -0.29992997  0.69806377\n",
            "   0.28103852 -0.87104029 -0.96466452 -0.67919731  1.61366485 -0.43352675\n",
            "  -0.19048544 -0.96068929 -0.1253279  -0.18725448]]\n",
            "[[-0.61970737 -0.36948577 -0.24438318 -0.12163209 -0.65800193 -0.06838765\n",
            "  -0.04708805  1.25354336  2.49134171 -0.38011123 -0.12220789 -0.81203856\n",
            "  -0.83977083 -0.65976617 -0.61970737 -0.36948577 -0.24438318 -0.12163209\n",
            "  -0.65800193 -0.06838765 -0.04708805  1.25354336  2.49134171 -0.38011123\n",
            "  -0.12220789 -0.81203856 -0.83977083 -0.65976617 -0.61970737 -0.43352675\n",
            "  -0.19048544  1.04091927 -0.1253279  -0.18725448]\n",
            " [-0.61970737 -0.36948577 -0.24438318 -0.12163209 -0.65800193 -0.06017424\n",
            "  -0.04708805 -0.56988992 -0.63488258 -0.27850261 -0.12220789  0.7779826\n",
            "   1.6196179   1.50432962 -0.61970737 -0.36948577 -0.24438318 -0.12163209\n",
            "  -0.65800193 -0.06017424 -0.04708805 -0.56988992 -0.63488258 -0.27850261\n",
            "  -0.12220789  0.7779826   1.6196179   1.50432962 -0.61970737 -0.43352675\n",
            "   5.24974502 -0.96068929 -0.1253279  -0.18725448]\n",
            " [-0.61970737 -0.36948577 -0.24438318 -0.12163209 -0.65800193 -0.07112545\n",
            "  -0.04515169 -0.154921   -0.56044866 -0.38185054 -0.12220789  1.0548369\n",
            "   1.33395382  1.86323424 -0.61970737 -0.36948577 -0.24438318 -0.12163209\n",
            "  -0.65800193 -0.07112545 -0.04515169 -0.154921   -0.56044866 -0.38185054\n",
            "  -0.12220789  1.0548369   1.33395382  1.86323424 -0.61970737 -0.43352675\n",
            "  -0.19048544  1.04091927 -0.1253279  -0.18725448]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNcuSWBQp2Ua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_sampler(y):\n",
        "  _, counts = np.unique(y, return_counts=True)\n",
        "  n_samples = len(y)\n",
        "  class_weights = n_samples / counts\n",
        "  weights = class_weights[y]\n",
        "  return data.WeightedRandomSampler(weights=weights, num_samples=n_samples, replacement=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fh6O6Nz6uC51",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sampler = create_sampler(y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iP6L6GN22_bR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, y_train = torch.from_numpy(x_train), torch.from_numpy(y_train)\n",
        "x_val, y_val = torch.from_numpy(x_val), torch.from_numpy(y_val)\n",
        "x_test, y_test = torch.from_numpy(x_test), torch.from_numpy(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XnnrzRjL3IP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = data.TensorDataset(x_train, y_train)\n",
        "val_dataset = data.TensorDataset(x_val, y_val)\n",
        "test_dataset = data.TensorDataset(x_test, y_test)\n",
        "\n",
        "train_dataloader = data.DataLoader(train_dataset, batch_size=512, sampler=sampler)\n",
        "val_dataloader = data.DataLoader(val_dataset, batch_size=32)\n",
        "test_dataloader = data.DataLoader(test_dataset, batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHoY9-JcORo-",
        "colab_type": "code",
        "outputId": "2dc65857-8b21-47f8-cfd8-09f84fbc71bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "source": [
        "inputs, targets = next(iter(train_dataloader))\n",
        "print(inputs[:3])\n",
        "print(targets[:3])\n",
        "\n",
        "inputs, targets = next(iter(val_dataloader))\n",
        "print(inputs[:3])\n",
        "print(targets[:3])\n",
        "\n",
        "inputs, targets = next(iter(test_dataloader))\n",
        "print(inputs[:3])\n",
        "print(targets[:3])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 1.6137, -0.3695, -0.2444, -0.1216, -0.6580, -0.0711, -0.0452,  0.4984,\n",
            "          0.1095, -0.3819, -0.1222,  0.0281,  0.5759,  0.1236,  1.6137, -0.3695,\n",
            "         -0.2444, -0.1216, -0.6580, -0.0711, -0.0452,  0.4984,  0.1095, -0.3819,\n",
            "         -0.1222,  0.0281,  0.5759,  0.1236,  1.6137, -0.4335, -0.1905, -0.9607,\n",
            "         -0.1253, -0.1873],\n",
            "        [ 1.6137, -0.3695, -0.2444, -0.1216, -0.6580, -0.0670, -0.0432,  1.3283,\n",
            "         -0.5232, -0.3828, -0.1222, -0.5957, -0.1207, -0.5881,  1.6137, -0.3695,\n",
            "         -0.2444, -0.1216, -0.6580, -0.0670, -0.0432,  1.3283, -0.5232, -0.3828,\n",
            "         -0.1222, -0.5957, -0.1207, -0.5881,  1.6137, -0.4335, -0.1905, -0.9607,\n",
            "         -0.1253, -0.1873],\n",
            "        [-0.6197, -0.3695, -0.2444, -0.0798, -0.3421, -0.0383, -0.0374,  0.0072,\n",
            "         -0.5232, -0.3556,  0.0122,  1.2071,  2.0447,  2.0187, -0.6197, -0.3695,\n",
            "         -0.2444, -0.0798, -0.3421, -0.0383, -0.0374,  0.0072, -0.5232, -0.3556,\n",
            "          0.0122,  1.2071,  2.0447,  2.0187, -0.6197, -0.4335, -0.1905,  1.0409,\n",
            "         -0.1253, -0.1873]], dtype=torch.float64)\n",
            "tensor([0, 0, 1], dtype=torch.int8)\n",
            "tensor([[-0.6197, -0.3695, -0.2444, -0.1216, -0.6580, -0.0711, -0.0452,  0.1638,\n",
            "          1.6354, -0.3829, -0.1222, -0.5861, -0.8764, -0.3984, -0.6197, -0.3695,\n",
            "         -0.2444, -0.1216, -0.6580, -0.0711, -0.0452,  0.1638,  1.6354, -0.3829,\n",
            "         -0.1222, -0.5861, -0.8764, -0.3984, -0.6197, -0.4335, -0.1905,  1.0409,\n",
            "         -0.1253, -0.1873],\n",
            "        [-0.6197, -0.3695, -0.2444, -0.0798, -0.3421, -0.0150,  0.0459,  0.4629,\n",
            "         -0.2999, -0.3832,  0.0122, -0.4943,  0.4498, -0.5988, -0.6197, -0.3695,\n",
            "         -0.2444, -0.0798, -0.3421, -0.0150,  0.0459,  0.4629, -0.2999, -0.3832,\n",
            "          0.0122, -0.4943,  0.4498, -0.5988, -0.6197, -0.4335, -0.1905,  1.0409,\n",
            "         -0.1253, -0.1873],\n",
            "        [ 1.6137,  0.2388,  0.0777,  0.0037,  0.5713, -0.0273, -0.0045, -0.0498,\n",
            "         -0.2999,  0.6981,  0.2810, -0.8710, -0.9647, -0.6792,  1.6137,  0.2388,\n",
            "          0.0777,  0.0037,  0.5713, -0.0273, -0.0045, -0.0498, -0.2999,  0.6981,\n",
            "          0.2810, -0.8710, -0.9647, -0.6792,  1.6137, -0.4335, -0.1905, -0.9607,\n",
            "         -0.1253, -0.1873]], dtype=torch.float64)\n",
            "tensor([0, 0, 1], dtype=torch.int8)\n",
            "tensor([[-0.6197, -0.3695, -0.2444, -0.1216, -0.6580, -0.0684, -0.0471,  1.2535,\n",
            "          2.4913, -0.3801, -0.1222, -0.8120, -0.8398, -0.6598, -0.6197, -0.3695,\n",
            "         -0.2444, -0.1216, -0.6580, -0.0684, -0.0471,  1.2535,  2.4913, -0.3801,\n",
            "         -0.1222, -0.8120, -0.8398, -0.6598, -0.6197, -0.4335, -0.1905,  1.0409,\n",
            "         -0.1253, -0.1873],\n",
            "        [-0.6197, -0.3695, -0.2444, -0.1216, -0.6580, -0.0602, -0.0471, -0.5699,\n",
            "         -0.6349, -0.2785, -0.1222,  0.7780,  1.6196,  1.5043, -0.6197, -0.3695,\n",
            "         -0.2444, -0.1216, -0.6580, -0.0602, -0.0471, -0.5699, -0.6349, -0.2785,\n",
            "         -0.1222,  0.7780,  1.6196,  1.5043, -0.6197, -0.4335,  5.2497, -0.9607,\n",
            "         -0.1253, -0.1873],\n",
            "        [-0.6197, -0.3695, -0.2444, -0.1216, -0.6580, -0.0711, -0.0452, -0.1549,\n",
            "         -0.5604, -0.3819, -0.1222,  1.0548,  1.3340,  1.8632, -0.6197, -0.3695,\n",
            "         -0.2444, -0.1216, -0.6580, -0.0711, -0.0452, -0.1549, -0.5604, -0.3819,\n",
            "         -0.1222,  1.0548,  1.3340,  1.8632, -0.6197, -0.4335, -0.1905,  1.0409,\n",
            "         -0.1253, -0.1873]], dtype=torch.float64)\n",
            "tensor([0, 0, 0], dtype=torch.int8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-GctbL8E1oA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Classifier(nn.Module):\n",
        "  def __init__(self, epochs, input_size, hidden_size, drop_prob, val_loss=None):\n",
        "    super(Classifier, self).__init__()\n",
        "    self.epochs = epochs\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.drop_prob = drop_prob\n",
        "    self.val_loss = val_loss\n",
        "    self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "    self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "    self.fcout = nn.Linear(hidden_size, 1)\n",
        "    self.dropout = nn.Dropout(drop_prob)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = torch.relu(self.fc1(x))\n",
        "    x = self.dropout(x)\n",
        "    x = torch.relu(self.fc2(x))\n",
        "    x = self.dropout(x)\n",
        "    x = torch.sigmoid(self.fcout(x))\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inpSePkGLTIs",
        "colab_type": "code",
        "outputId": "b54b48f8-86b1-4e9a-b377-b25858f1822f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "classifier = Classifier(epochs=1000, input_size=x.shape[1], hidden_size=256, drop_prob=0.5)\n",
        "classifier"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Classifier(\n",
              "  (fc1): Linear(in_features=34, out_features=256, bias=True)\n",
              "  (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
              "  (fcout): Linear(in_features=256, out_features=1, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsT5eIrNoXAh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = 'classifier.cpt'\n",
        "def save_classifier(classifier):\n",
        "  checkpoint = {\n",
        "      'epochs': classifier.epochs,\n",
        "      'input_size': classifier.input_size,\n",
        "      'hidden_size': classifier.hidden_size,\n",
        "      'drop_prob': classifier.drop_prob,\n",
        "      'val_loss': classifier.val_loss,\n",
        "      'state_dict': classifier.state_dict()\n",
        "  }\n",
        "  with open(filename, 'wb') as f:\n",
        "    torch.save(checkpoint, f)\n",
        "\n",
        "def load_classifier():\n",
        "  with open(filename, 'rb') as f:\n",
        "    checkpoint = torch.load(f)\n",
        "  classifier = Classifier(checkpoint['epochs'], checkpoint['input_size'], checkpoint['hidden_size'], checkpoint['drop_prob'], checkpoint['val_loss'])\n",
        "  classifier.load_state_dict(checkpoint['state_dict'])\n",
        "  return classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LX_eofKUq0ok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_classifier(classifier)\n",
        "classifier = load_classifier()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PR8s5ZA5rX-V",
        "colab_type": "code",
        "outputId": "5e25d555-528f-4ee8-fbec-b85257f60621",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  classifier = classifier.cuda()\n",
        "  inputs = inputs.cuda()\n",
        "\n",
        "classifier(inputs[:3].float())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4988],\n",
              "        [0.4987],\n",
              "        [0.5096]], device='cuda:0', grad_fn=<SigmoidBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arYOr5lMLYy7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(params=classifier.parameters(), lr=0.003)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFy03p96Lza1",
        "colab_type": "code",
        "outputId": "f1626118-c00d-4cd4-83ef-714993ffe977",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_loss = 0\n",
        "epochs = classifier.epochs\n",
        "for epoch in range(epochs):\n",
        "  classifier.train()\n",
        "  for inputs, targets in train_dataloader:\n",
        "    if torch.cuda.is_available():\n",
        "      inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "    outputs = classifier(inputs.float())\n",
        "    loss = criterion(outputs.squeeze(), targets.float())\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    classifier.eval()\n",
        "    val_loss = 0\n",
        "    for inputs, targets in val_dataloader:\n",
        "      if torch.cuda.is_available():\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "      outputs = classifier(inputs.float())\n",
        "      loss = criterion(outputs.squeeze(), targets.float())\n",
        "      val_loss += loss.item()\n",
        "\n",
        "    train_loss = train_loss / len(train_dataloader)\n",
        "    val_loss = val_loss / len(val_dataloader)\n",
        "    print('Epoch: {}, Train loss: {}, Val loss: {}'.format(epoch, train_loss, val_loss))\n",
        "\n",
        "    if classifier.val_loss is None or val_loss < classifier.val_loss:\n",
        "      classifier.val_loss = val_loss\n",
        "      classifier.epochs = epoch\n",
        "      save_classifier(classifier)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Train loss: 0.6089057138091639, Val loss: 0.6004581753360597\n",
            "Epoch: 1, Train loss: 0.5770850898156206, Val loss: 0.5182919823809674\n",
            "Epoch: 2, Train loss: 0.5619035633021531, Val loss: 0.533738300204277\n",
            "Epoch: 3, Train loss: 0.5496999628491933, Val loss: 0.514270973048712\n",
            "Epoch: 4, Train loss: 0.535005087149634, Val loss: 0.5310197033380207\n",
            "Epoch: 5, Train loss: 0.5339375379967192, Val loss: 0.5372801466207755\n",
            "Epoch: 6, Train loss: 0.5296086981194258, Val loss: 0.5193872596872481\n",
            "Epoch: 7, Train loss: 0.524873953938215, Val loss: 0.5392486723630052\n",
            "Epoch: 8, Train loss: 0.5166220785724418, Val loss: 0.528553941924321\n",
            "Epoch: 9, Train loss: 0.5241742819588103, Val loss: 0.5215287200714412\n",
            "Epoch: 10, Train loss: 0.5141183140247556, Val loss: 0.5139651839670382\n",
            "Epoch: 11, Train loss: 0.5132377124208356, Val loss: 0.521879185579325\n",
            "Epoch: 12, Train loss: 0.5092885509300079, Val loss: 0.5290270306562123\n",
            "Epoch: 13, Train loss: 0.5086532596307048, Val loss: 0.512881329969356\n",
            "Epoch: 14, Train loss: 0.5063311545565385, Val loss: 0.5126638726184243\n",
            "Epoch: 15, Train loss: 0.5066550376345051, Val loss: 0.48543064688381393\n",
            "Epoch: 16, Train loss: 0.5036338897188749, Val loss: 0.49991662290535477\n",
            "Epoch: 17, Train loss: 0.5066640315130099, Val loss: 0.5234721969617041\n",
            "Epoch: 18, Train loss: 0.5000515225625738, Val loss: 0.48187284759785\n",
            "Epoch: 19, Train loss: 0.4942397975688877, Val loss: 0.5232594966104156\n",
            "Epoch: 20, Train loss: 0.49365625466109814, Val loss: 0.47736677880349915\n",
            "Epoch: 21, Train loss: 0.4933919939778357, Val loss: 0.5127484465115949\n",
            "Epoch: 22, Train loss: 0.49018319195138293, Val loss: 0.4978498075353472\n",
            "Epoch: 23, Train loss: 0.4895646404136761, Val loss: 0.5207139892797721\n",
            "Epoch: 24, Train loss: 0.4893777601313945, Val loss: 0.5263881479438982\n",
            "Epoch: 25, Train loss: 0.4949366276002567, Val loss: 0.5059271168551946\n",
            "Epoch: 26, Train loss: 0.48848384651316007, Val loss: 0.4846980222745946\n",
            "Epoch: 27, Train loss: 0.4786129750789187, Val loss: 0.495226955727527\n",
            "Epoch: 28, Train loss: 0.4860129133075501, Val loss: 0.5142393810184378\n",
            "Epoch: 29, Train loss: 0.4746449277986822, Val loss: 0.49309866640128586\n",
            "Epoch: 30, Train loss: 0.48923797509698946, Val loss: 0.5233367410929579\n",
            "Epoch: 31, Train loss: 0.4850430264027571, Val loss: 0.5117912739515305\n",
            "Epoch: 32, Train loss: 0.48396404342157706, Val loss: 0.4676873432962518\n",
            "Epoch: 33, Train loss: 0.4877450972681252, Val loss: 0.5026984457906923\n",
            "Epoch: 34, Train loss: 0.4863487738524743, Val loss: 0.4869083154358362\n",
            "Epoch: 35, Train loss: 0.47114119863864323, Val loss: 0.5006598685132829\n",
            "Epoch: 36, Train loss: 0.48032554694862645, Val loss: 0.49653106614163045\n",
            "Epoch: 37, Train loss: 0.47774973660938874, Val loss: 0.4763861741674574\n",
            "Epoch: 38, Train loss: 0.47165093143525694, Val loss: 0.47172900877500834\n",
            "Epoch: 39, Train loss: 0.4757015054999064, Val loss: 0.5116343125700951\n",
            "Epoch: 40, Train loss: 0.4748267498601379, Val loss: 0.5094394264252562\n",
            "Epoch: 41, Train loss: 0.4841575378571396, Val loss: 0.5080128974820438\n",
            "Epoch: 42, Train loss: 0.4705264177615037, Val loss: 0.4739106085739638\n",
            "Epoch: 43, Train loss: 0.4777548357757942, Val loss: 0.501458998181318\n",
            "Epoch: 44, Train loss: 0.47477424078059044, Val loss: 0.4847528844287521\n",
            "Epoch: 45, Train loss: 0.46271943259993975, Val loss: 0.46707784149207565\n",
            "Epoch: 46, Train loss: 0.469325323431314, Val loss: 0.4882229217573216\n",
            "Epoch: 47, Train loss: 0.4753177822035818, Val loss: 0.4852714271921861\n",
            "Epoch: 48, Train loss: 0.4711795741016464, Val loss: 0.48464191979483556\n",
            "Epoch: 49, Train loss: 0.473914049157173, Val loss: 0.47665057488177953\n",
            "Epoch: 50, Train loss: 0.47062639792992017, Val loss: 0.5000029006286671\n",
            "Epoch: 51, Train loss: 0.47496399084733065, Val loss: 0.4848936278569071\n",
            "Epoch: 52, Train loss: 0.47024442250268655, Val loss: 0.4894101560900086\n",
            "Epoch: 53, Train loss: 0.4635512354328803, Val loss: 0.47869173829492767\n",
            "Epoch: 54, Train loss: 0.47114411106850645, Val loss: 0.47437111053027603\n",
            "Epoch: 55, Train loss: 0.469798590820909, Val loss: 0.4941836160264517\n",
            "Epoch: 56, Train loss: 0.47183904747382277, Val loss: 0.4678120828772846\n",
            "Epoch: 57, Train loss: 0.4699665043176829, Val loss: 0.4966132981996787\n",
            "Epoch: 58, Train loss: 0.4683328201201443, Val loss: 0.5114839935773298\n",
            "Epoch: 59, Train loss: 0.47079845381888097, Val loss: 0.4903640798047969\n",
            "Epoch: 60, Train loss: 0.46787592431775105, Val loss: 0.46792064332648325\n",
            "Epoch: 61, Train loss: 0.4598573586556136, Val loss: 0.4695431974373366\n",
            "Epoch: 62, Train loss: 0.4681474350555236, Val loss: 0.4898630039472329\n",
            "Epoch: 63, Train loss: 0.4695327535079851, Val loss: 0.49378954972091477\n",
            "Epoch: 64, Train loss: 0.46848763199962784, Val loss: 0.4630813081013529\n",
            "Epoch: 65, Train loss: 0.46819882752924574, Val loss: 0.49577361855067703\n",
            "Epoch: 66, Train loss: 0.4707992427606409, Val loss: 0.4661670801670928\n",
            "Epoch: 67, Train loss: 0.4563477738726009, Val loss: 0.4829527825901383\n",
            "Epoch: 68, Train loss: 0.45839228861171916, Val loss: 0.46290543048005356\n",
            "Epoch: 69, Train loss: 0.4666598869605647, Val loss: 0.4837594040130314\n",
            "Epoch: 70, Train loss: 0.4635661443943616, Val loss: 0.49102239506809336\n",
            "Epoch: 71, Train loss: 0.46732081760668664, Val loss: 0.48776955275159134\n",
            "Epoch: 72, Train loss: 0.458421255880726, Val loss: 0.46626652737981394\n",
            "Epoch: 73, Train loss: 0.4588573440578458, Val loss: 0.48973930352612544\n",
            "Epoch: 74, Train loss: 0.45999441142842146, Val loss: 0.4999789917155316\n",
            "Epoch: 75, Train loss: 0.4644556752934699, Val loss: 0.4877275930423486\n",
            "Epoch: 76, Train loss: 0.4584130757526731, Val loss: 0.4911463907674739\n",
            "Epoch: 77, Train loss: 0.45618606113936866, Val loss: 0.49571782585821655\n",
            "Epoch: 78, Train loss: 0.4603583500321357, Val loss: 0.4909723193237656\n",
            "Epoch: 79, Train loss: 0.4614004704134916, Val loss: 0.4845972786608495\n",
            "Epoch: 80, Train loss: 0.46121831215467696, Val loss: 0.4826085042012365\n",
            "Epoch: 81, Train loss: 0.46307152077739877, Val loss: 0.48496448170197637\n",
            "Epoch: 82, Train loss: 0.46064652804878337, Val loss: 0.5046240824617838\n",
            "Epoch: 83, Train loss: 0.4563356493660094, Val loss: 0.4892633949455462\n",
            "Epoch: 84, Train loss: 0.44982967213253255, Val loss: 0.4951967067624393\n",
            "Epoch: 85, Train loss: 0.4607747025749819, Val loss: 0.501154420015059\n",
            "Epoch: 86, Train loss: 0.45876062890798824, Val loss: 0.5180287247425631\n",
            "Epoch: 87, Train loss: 0.4517703984340322, Val loss: 0.5114591474595823\n",
            "Epoch: 88, Train loss: 0.45227345833171956, Val loss: 0.48995538408819\n",
            "Epoch: 89, Train loss: 0.4549309838782772, Val loss: 0.4760051757881516\n",
            "Epoch: 90, Train loss: 0.4587104695119644, Val loss: 0.4760498580963988\n",
            "Epoch: 91, Train loss: 0.4585434192551808, Val loss: 0.48643037206248235\n",
            "Epoch: 92, Train loss: 0.4565584408456129, Val loss: 0.46728903564967605\n",
            "Epoch: 93, Train loss: 0.457458646797334, Val loss: 0.4814140392761481\n",
            "Epoch: 94, Train loss: 0.45216005809744186, Val loss: 0.4785488220421891\n",
            "Epoch: 95, Train loss: 0.45356836673660234, Val loss: 0.49468883793605\n",
            "Epoch: 96, Train loss: 0.4595630709427081, Val loss: 0.4953622594475746\n",
            "Epoch: 97, Train loss: 0.45276241695697234, Val loss: 0.4849359153916961\n",
            "Epoch: 98, Train loss: 0.45767427097177693, Val loss: 0.4939433987203397\n",
            "Epoch: 99, Train loss: 0.44747880823006136, Val loss: 0.4893631248881942\n",
            "Epoch: 100, Train loss: 0.4512022876756729, Val loss: 0.5030979561178308\n",
            "Epoch: 101, Train loss: 0.4548924757305398, Val loss: 0.47831145595563085\n",
            "Epoch: 102, Train loss: 0.4513545888381663, Val loss: 0.4771226311200543\n",
            "Epoch: 103, Train loss: 0.451819540528136, Val loss: 0.4937870502471924\n",
            "Epoch: 104, Train loss: 0.4538508559911836, Val loss: 0.47355074357045324\n",
            "Epoch: 105, Train loss: 0.45460649829592986, Val loss: 0.4901528170234279\n",
            "Epoch: 106, Train loss: 0.4538726915135422, Val loss: 0.4991955957130382\n",
            "Epoch: 107, Train loss: 0.449703142332441, Val loss: 0.48788572377280187\n",
            "Epoch: 108, Train loss: 0.4533110296330798, Val loss: 0.5049725276859183\n",
            "Epoch: 109, Train loss: 0.4565898275051251, Val loss: 0.4804911715419669\n",
            "Epoch: 110, Train loss: 0.4550444180392219, Val loss: 0.47615232279426173\n",
            "Epoch: 111, Train loss: 0.4491550585016425, Val loss: 0.4749065233688605\n",
            "Epoch: 112, Train loss: 0.45305656043161635, Val loss: 0.47953045289767415\n",
            "Epoch: 113, Train loss: 0.44888018898991894, Val loss: 0.49464923613949824\n",
            "Epoch: 114, Train loss: 0.44619700373600135, Val loss: 0.46765953498451335\n",
            "Epoch: 115, Train loss: 0.44765533586902323, Val loss: 0.46296672444594533\n",
            "Epoch: 116, Train loss: 0.4453235625647042, Val loss: 0.4807569980621338\n",
            "Epoch: 117, Train loss: 0.4505475274811254, Val loss: 0.4988767614490108\n",
            "Epoch: 118, Train loss: 0.45100596733284126, Val loss: 0.47584725249754756\n",
            "Epoch: 119, Train loss: 0.45048541227646877, Val loss: 0.4862366938277295\n",
            "Epoch: 120, Train loss: 0.4530832148956803, Val loss: 0.49973970769267334\n",
            "Epoch: 121, Train loss: 0.45562463443887746, Val loss: 0.49731168150901794\n",
            "Epoch: 122, Train loss: 0.4499915970107475, Val loss: 0.5138952649737659\n",
            "Epoch: 123, Train loss: 0.45102417974289194, Val loss: 0.4918263103616865\n",
            "Epoch: 124, Train loss: 0.4487874603346293, Val loss: 0.4781063861752811\n",
            "Epoch: 125, Train loss: 0.4526371244696026, Val loss: 0.48714501253868403\n",
            "Epoch: 126, Train loss: 0.4434570612199141, Val loss: 0.48967739586767395\n",
            "Epoch: 127, Train loss: 0.45024583904807775, Val loss: 0.4738379983525527\n",
            "Epoch: 128, Train loss: 0.44896753947171864, Val loss: 0.49956946820020676\n",
            "Epoch: 129, Train loss: 0.44011309176166674, Val loss: 0.47392776059476954\n",
            "Epoch: 130, Train loss: 0.4458672371426904, Val loss: 0.46874160868556874\n",
            "Epoch: 131, Train loss: 0.44555755264643954, Val loss: 0.4832220724538753\n",
            "Epoch: 132, Train loss: 0.4464897698496215, Val loss: 0.4777087392775636\n",
            "Epoch: 133, Train loss: 0.44555775791494057, Val loss: 0.4922383309979188\n",
            "Epoch: 134, Train loss: 0.437647067002583, Val loss: 0.49161065330630854\n",
            "Epoch: 135, Train loss: 0.43704005350387226, Val loss: 0.47619914302700445\n",
            "Epoch: 136, Train loss: 0.4402021810856085, Val loss: 0.47600808818089335\n",
            "Epoch: 137, Train loss: 0.44391932311143434, Val loss: 0.4916147632818473\n",
            "Epoch: 138, Train loss: 0.44226594876366904, Val loss: 0.4922367844142412\n",
            "Epoch: 139, Train loss: 0.4407342262203019, Val loss: 0.483938834384868\n",
            "Epoch: 140, Train loss: 0.4453121561939635, Val loss: 0.48790540075615835\n",
            "Epoch: 141, Train loss: 0.44221951417529565, Val loss: 0.4776145608017319\n",
            "Epoch: 142, Train loss: 0.4424049788536743, Val loss: 0.4990789125624456\n",
            "Epoch: 143, Train loss: 0.4414765707594839, Val loss: 0.48502752459362936\n",
            "Epoch: 144, Train loss: 0.44990042053801627, Val loss: 0.4616238917959364\n",
            "Epoch: 145, Train loss: 0.4431641619987505, Val loss: 0.490325894794966\n",
            "Epoch: 146, Train loss: 0.44302089334994876, Val loss: 0.4931791283582386\n",
            "Epoch: 147, Train loss: 0.44341385242224535, Val loss: 0.494996991204588\n",
            "Epoch: 148, Train loss: 0.4424751173186932, Val loss: 0.4828890530686629\n",
            "Epoch: 149, Train loss: 0.4402126695260893, Val loss: 0.503140860482266\n",
            "Epoch: 150, Train loss: 0.4437712886499352, Val loss: 0.4955403655767441\n",
            "Epoch: 151, Train loss: 0.44805048604431635, Val loss: 0.4944776904426123\n",
            "Epoch: 152, Train loss: 0.44447222396460095, Val loss: 0.46726012857336746\n",
            "Epoch: 153, Train loss: 0.44651670049280634, Val loss: 0.48417645222262334\n",
            "Epoch: 154, Train loss: 0.43628745350676956, Val loss: 0.48620471711221497\n",
            "Epoch: 155, Train loss: 0.4494991151056369, Val loss: 0.4975325476966406\n",
            "Epoch: 156, Train loss: 0.4398283703483032, Val loss: 0.49971396240748855\n",
            "Epoch: 157, Train loss: 0.44025636436478316, Val loss: 0.47990464889689494\n",
            "Epoch: 158, Train loss: 0.4390213805128441, Val loss: 0.5104653870588854\n",
            "Epoch: 159, Train loss: 0.4429795224878069, Val loss: 0.4947122241321363\n",
            "Epoch: 160, Train loss: 0.43780702373284225, Val loss: 0.502061128616333\n",
            "Epoch: 161, Train loss: 0.43546704321077473, Val loss: 0.4850788132140511\n",
            "Epoch: 162, Train loss: 0.44277781349236794, Val loss: 0.4893550959072615\n",
            "Epoch: 163, Train loss: 0.43710205432140387, Val loss: 0.4850510884272425\n",
            "Epoch: 164, Train loss: 0.4382741287811711, Val loss: 0.49158501193711635\n",
            "Epoch: 165, Train loss: 0.43611563934428677, Val loss: 0.48942254248418304\n",
            "Epoch: 166, Train loss: 0.436087766418735, Val loss: 0.4941877644313009\n",
            "Epoch: 167, Train loss: 0.441587798928218, Val loss: 0.47962234914302826\n",
            "Epoch: 168, Train loss: 0.4387186638392638, Val loss: 0.48952353079068034\n",
            "Epoch: 169, Train loss: 0.440380939976891, Val loss: 0.4928870561875795\n",
            "Epoch: 170, Train loss: 0.44060717546983036, Val loss: 0.4929687525880964\n",
            "Epoch: 171, Train loss: 0.4375175359280599, Val loss: 0.4960976956706298\n",
            "Epoch: 172, Train loss: 0.4431266176136745, Val loss: 0.47718016450342376\n",
            "Epoch: 173, Train loss: 0.4394155222790085, Val loss: 0.4943793349360165\n",
            "Epoch: 174, Train loss: 0.4394927615100266, Val loss: 0.4853462616079732\n",
            "Epoch: 175, Train loss: 0.43327784144610193, Val loss: 0.48663995767894547\n",
            "Epoch: 176, Train loss: 0.4336381365342438, Val loss: 0.48367326745861455\n",
            "Epoch: 177, Train loss: 0.44163321379371767, Val loss: 0.471261954229129\n",
            "Epoch: 178, Train loss: 0.43613956918934677, Val loss: 0.4958455613569209\n",
            "Epoch: 179, Train loss: 0.43632398743133266, Val loss: 0.47894095864735153\n",
            "Epoch: 180, Train loss: 0.43250806281930015, Val loss: 0.4954770709338941\n",
            "Epoch: 181, Train loss: 0.43171361903087246, Val loss: 0.48126830984103053\n",
            "Epoch: 182, Train loss: 0.43832646018050636, Val loss: 0.47581637297805984\n",
            "Epoch: 183, Train loss: 0.4278809360782782, Val loss: 0.4892052682606797\n",
            "Epoch: 184, Train loss: 0.43337834412622994, Val loss: 0.519703078034677\n",
            "Epoch: 185, Train loss: 0.4389320503553718, Val loss: 0.4911844938209182\n",
            "Epoch: 186, Train loss: 0.4361893672976603, Val loss: 0.48348759350023773\n",
            "Epoch: 187, Train loss: 0.4295542408479769, Val loss: 0.49138245849232925\n",
            "Epoch: 188, Train loss: 0.4360337646108278, Val loss: 0.4914364559870017\n",
            "Epoch: 189, Train loss: 0.4329155819230472, Val loss: 0.4822001484663863\n",
            "Epoch: 190, Train loss: 0.4331586858593926, Val loss: 0.47184197134093236\n",
            "Epoch: 191, Train loss: 0.43407800140339947, Val loss: 0.4909292683005333\n",
            "Epoch: 192, Train loss: 0.4316370783517128, Val loss: 0.47595706150719996\n",
            "Epoch: 193, Train loss: 0.4268582980838506, Val loss: 0.4886212266589466\n",
            "Epoch: 194, Train loss: 0.4380604261918211, Val loss: 0.5018289414675612\n",
            "Epoch: 195, Train loss: 0.43707652059240126, Val loss: 0.49619063382086\n",
            "Epoch: 196, Train loss: 0.43448849523460736, Val loss: 0.49709377320189224\n",
            "Epoch: 197, Train loss: 0.42611305860165855, Val loss: 0.49235371657108007\n",
            "Epoch: 198, Train loss: 0.4309121579317367, Val loss: 0.4849042715994935\n",
            "Epoch: 199, Train loss: 0.4325972785301065, Val loss: 0.49542817943974543\n",
            "Epoch: 200, Train loss: 0.43234249554639076, Val loss: 0.48787807634002284\n",
            "Epoch: 201, Train loss: 0.43141789096556343, Val loss: 0.478719172509093\n",
            "Epoch: 202, Train loss: 0.4300142003407505, Val loss: 0.4936202278262691\n",
            "Epoch: 203, Train loss: 0.43344322650398714, Val loss: 0.4886061219792617\n",
            "Epoch: 204, Train loss: 0.43038279701972376, Val loss: 0.487231131446989\n",
            "Epoch: 205, Train loss: 0.4293562058528537, Val loss: 0.49428095668554306\n",
            "Epoch: 206, Train loss: 0.4387726984628467, Val loss: 0.4886172147173631\n",
            "Epoch: 207, Train loss: 0.434015613381822, Val loss: 0.4723469328723456\n",
            "Epoch: 208, Train loss: 0.4270984125656193, Val loss: 0.49697700770277725\n",
            "Epoch: 209, Train loss: 0.43343631738247135, Val loss: 0.488470264563435\n",
            "Epoch: 210, Train loss: 0.4276148296013793, Val loss: 0.48213421867081996\n",
            "Epoch: 211, Train loss: 0.4280574547005728, Val loss: 0.5084406929580789\n",
            "Epoch: 212, Train loss: 0.4257019199476771, Val loss: 0.4890498224841921\n",
            "Epoch: 213, Train loss: 0.42756499516740737, Val loss: 0.4867083838111476\n",
            "Epoch: 214, Train loss: 0.4280434529686642, Val loss: 0.4870891669078877\n",
            "Epoch: 215, Train loss: 0.4236539758629566, Val loss: 0.49256236302225215\n",
            "Epoch: 216, Train loss: 0.4391315959979226, Val loss: 0.4852260892328463\n",
            "Epoch: 217, Train loss: 0.42988650051932875, Val loss: 0.4864048314721961\n",
            "Epoch: 218, Train loss: 0.4243939717736635, Val loss: 0.4919350668787956\n",
            "Epoch: 219, Train loss: 0.43786941050470607, Val loss: 0.48390506443224457\n",
            "Epoch: 220, Train loss: 0.42865569368762424, Val loss: 0.4759283359897764\n",
            "Epoch: 221, Train loss: 0.43140091027496225, Val loss: 0.48552366424548\n",
            "Epoch: 222, Train loss: 0.4324698977631639, Val loss: 0.4924188420960778\n",
            "Epoch: 223, Train loss: 0.42962264369705866, Val loss: 0.49647136150222076\n",
            "Epoch: 224, Train loss: 0.42973613486760004, Val loss: 0.49415210163907003\n",
            "Epoch: 225, Train loss: 0.42979822967817727, Val loss: 0.48641101192486913\n",
            "Epoch: 226, Train loss: 0.42595207414458985, Val loss: 0.5036918779737071\n",
            "Epoch: 227, Train loss: 0.4253871126450548, Val loss: 0.4998467360672198\n",
            "Epoch: 228, Train loss: 0.4184678070991642, Val loss: 0.49601254808275325\n",
            "Epoch: 229, Train loss: 0.42730815518647275, Val loss: 0.49328018097501053\n",
            "Epoch: 230, Train loss: 0.4334150320889526, Val loss: 0.4775577473797296\n",
            "Epoch: 231, Train loss: 0.4307804093721717, Val loss: 0.4911109569825624\n",
            "Epoch: 232, Train loss: 0.4255148509248391, Val loss: 0.478051825181434\n",
            "Epoch: 233, Train loss: 0.4271576813415067, Val loss: 0.4950985147764808\n",
            "Epoch: 234, Train loss: 0.42916095050687164, Val loss: 0.4872058833900251\n",
            "Epoch: 235, Train loss: 0.42812710132319537, Val loss: 0.48387670360113444\n",
            "Epoch: 236, Train loss: 0.42777955352374303, Val loss: 0.48477498834070404\n",
            "Epoch: 237, Train loss: 0.43135211729666334, Val loss: 0.4978480339050293\n",
            "Epoch: 238, Train loss: 0.42507073697909464, Val loss: 0.4923398439821444\n",
            "Epoch: 239, Train loss: 0.42640327712356646, Val loss: 0.5012093727525911\n",
            "Epoch: 240, Train loss: 0.4334491293421073, Val loss: 0.4806130375517042\n",
            "Epoch: 241, Train loss: 0.4284164061169632, Val loss: 0.5013236552476883\n",
            "Epoch: 242, Train loss: 0.4297340505740643, Val loss: 0.5008273069795809\n",
            "Epoch: 243, Train loss: 0.4303298443383856, Val loss: 0.4875103176424378\n",
            "Epoch: 244, Train loss: 0.430113262643487, Val loss: 0.46891645891101735\n",
            "Epoch: 245, Train loss: 0.4276608042212451, Val loss: 0.4946417867353088\n",
            "Epoch: 246, Train loss: 0.4186477530742218, Val loss: 0.4913230389356613\n",
            "Epoch: 247, Train loss: 0.4263492285911047, Val loss: 0.5010874981158658\n",
            "Epoch: 248, Train loss: 0.42296419504364285, Val loss: 0.48868294962142644\n",
            "Epoch: 249, Train loss: 0.4240727552622766, Val loss: 0.4815669659721224\n",
            "Epoch: 250, Train loss: 0.42574360843512626, Val loss: 0.4787266630875437\n",
            "Epoch: 251, Train loss: 0.4279031172854893, Val loss: 0.4994938095149241\n",
            "Epoch: 252, Train loss: 0.4238668610010837, Val loss: 0.47660832773698003\n",
            "Epoch: 253, Train loss: 0.41751290996177753, Val loss: 0.4887859072340162\n",
            "Epoch: 254, Train loss: 0.4152418285188193, Val loss: 0.48402054960790436\n",
            "Epoch: 255, Train loss: 0.423579929454225, Val loss: 0.48535375257855967\n",
            "Epoch: 256, Train loss: 0.4179920490944358, Val loss: 0.477594225814468\n",
            "Epoch: 257, Train loss: 0.42998796763222463, Val loss: 0.4765447687945868\n",
            "Epoch: 258, Train loss: 0.4266633077834262, Val loss: 0.46740466317063883\n",
            "Epoch: 259, Train loss: 0.43022438330706336, Val loss: 0.4676896443492488\n",
            "Epoch: 260, Train loss: 0.42159737945733794, Val loss: 0.4855893344471329\n",
            "Epoch: 261, Train loss: 0.4202503849019969, Val loss: 0.4881259763711377\n",
            "Epoch: 262, Train loss: 0.41661106360834516, Val loss: 0.4851387535270892\n",
            "Epoch: 263, Train loss: 0.4326961900092221, Val loss: 0.4907383812885535\n",
            "Epoch: 264, Train loss: 0.41981751658753624, Val loss: 0.4877828606649449\n",
            "Epoch: 265, Train loss: 0.42210081793191034, Val loss: 0.4757663874249709\n",
            "Epoch: 266, Train loss: 0.4126683859389398, Val loss: 0.47697231408796814\n",
            "Epoch: 267, Train loss: 0.41869611355390823, Val loss: 0.4665574329464059\n",
            "Epoch: 268, Train loss: 0.42151936674659884, Val loss: 0.4914385837159659\n",
            "Epoch: 269, Train loss: 0.4222435480106676, Val loss: 0.49903909587546397\n",
            "Epoch: 270, Train loss: 0.4222768822933206, Val loss: 0.47950686867299835\n",
            "Epoch: 271, Train loss: 0.41646106065267263, Val loss: 0.4927883410924359\n",
            "Epoch: 272, Train loss: 0.42515131544150647, Val loss: 0.48218373836655365\n",
            "Epoch: 273, Train loss: 0.42290405529399655, Val loss: 0.4893950576844968\n",
            "Epoch: 274, Train loss: 0.4165708127507149, Val loss: 0.4934821132766573\n",
            "Epoch: 275, Train loss: 0.4249803227889134, Val loss: 0.49282789936191157\n",
            "Epoch: 276, Train loss: 0.42164702191349596, Val loss: 0.47828122149956853\n",
            "Epoch: 277, Train loss: 0.4214237295677779, Val loss: 0.4850005389828431\n",
            "Epoch: 278, Train loss: 0.4260038661933011, Val loss: 0.5060555087892633\n",
            "Epoch: 279, Train loss: 0.42003600358956755, Val loss: 0.49548527951303284\n",
            "Epoch: 280, Train loss: 0.4250524322610135, Val loss: 0.48641897659552724\n",
            "Epoch: 281, Train loss: 0.42152816757931266, Val loss: 0.4868657855611098\n",
            "Epoch: 282, Train loss: 0.421626002565124, Val loss: 0.4807453167281653\n",
            "Epoch: 283, Train loss: 0.42400928942192795, Val loss: 0.49447785474752126\n",
            "Epoch: 284, Train loss: 0.42335857048573966, Val loss: 0.4918690817920785\n",
            "Epoch: 285, Train loss: 0.41795787165430104, Val loss: 0.49972767069151525\n",
            "Epoch: 286, Train loss: 0.4252404899842559, Val loss: 0.47574316082816376\n",
            "Epoch: 287, Train loss: 0.41921503693243656, Val loss: 0.47770698760685165\n",
            "Epoch: 288, Train loss: 0.4176199220121902, Val loss: 0.4952592504651923\n",
            "Epoch: 289, Train loss: 0.41539153399828493, Val loss: 0.4827328818409066\n",
            "Epoch: 290, Train loss: 0.4205115170705641, Val loss: 0.4858956513436217\n",
            "Epoch: 291, Train loss: 0.41762289334658026, Val loss: 0.4723524537525679\n",
            "Epoch: 292, Train loss: 0.41584843630574336, Val loss: 0.4788821582731448\n",
            "Epoch: 293, Train loss: 0.4140832385564536, Val loss: 0.48738810067114074\n",
            "Epoch: 294, Train loss: 0.414586529034127, Val loss: 0.48028623704847534\n",
            "Epoch: 295, Train loss: 0.42284413897410156, Val loss: 0.48543174329556915\n",
            "Epoch: 296, Train loss: 0.4196590278554943, Val loss: 0.48497961422330454\n",
            "Epoch: 297, Train loss: 0.41911173440387633, Val loss: 0.49065599668967097\n",
            "Epoch: 298, Train loss: 0.41943089522221916, Val loss: 0.48540254682302475\n",
            "Epoch: 299, Train loss: 0.4232239522843244, Val loss: 0.48501147408234446\n",
            "Epoch: 300, Train loss: 0.4142180079909497, Val loss: 0.5041085140485513\n",
            "Epoch: 301, Train loss: 0.4207324024739295, Val loss: 0.4985501915216446\n",
            "Epoch: 302, Train loss: 0.41659561488895674, Val loss: 0.5075476251934704\n",
            "Epoch: 303, Train loss: 0.42317088515451834, Val loss: 0.48284658358285304\n",
            "Epoch: 304, Train loss: 0.4135204676658677, Val loss: 0.4853714147680684\n",
            "Epoch: 305, Train loss: 0.41137867570935316, Val loss: 0.4868554488608712\n",
            "Epoch: 306, Train loss: 0.41284697840114926, Val loss: 0.49788393197875275\n",
            "Epoch: 307, Train loss: 0.4167178915230591, Val loss: 0.48543440668206467\n",
            "Epoch: 308, Train loss: 0.4166257335592994, Val loss: 0.48106247578796585\n",
            "Epoch: 309, Train loss: 0.4110774992690005, Val loss: 0.48285261894527237\n",
            "Epoch: 310, Train loss: 0.41903974115531045, Val loss: 0.490268242202307\n",
            "Epoch: 311, Train loss: 0.419441395292073, Val loss: 0.486254531694086\n",
            "Epoch: 312, Train loss: 0.416214359258596, Val loss: 0.48178103056393173\n",
            "Epoch: 313, Train loss: 0.4126783338151214, Val loss: 0.48671365137162964\n",
            "Epoch: 314, Train loss: 0.41542400583614864, Val loss: 0.4726601239097746\n",
            "Epoch: 315, Train loss: 0.4164201496110125, Val loss: 0.49885501438065577\n",
            "Epoch: 316, Train loss: 0.4179157054687429, Val loss: 0.5005031181009192\n",
            "Epoch: 317, Train loss: 0.42134678608562054, Val loss: 0.4922778194672183\n",
            "Epoch: 318, Train loss: 0.41794661791868, Val loss: 0.48123424343372645\n",
            "Epoch: 319, Train loss: 0.40975117634047203, Val loss: 0.4694031963222905\n",
            "Epoch: 320, Train loss: 0.41544089661102257, Val loss: 0.4814501667493268\n",
            "Epoch: 321, Train loss: 0.40469572757055383, Val loss: 0.49052757063978597\n",
            "Epoch: 322, Train loss: 0.4138399819570597, Val loss: 0.4840453863143921\n",
            "Epoch: 323, Train loss: 0.4161641994067395, Val loss: 0.4875174623570944\n",
            "Epoch: 324, Train loss: 0.4162347523239317, Val loss: 0.4878384949345338\n",
            "Epoch: 325, Train loss: 0.4164024416497587, Val loss: 0.4963885141830695\n",
            "Epoch: 326, Train loss: 0.41708089992636976, Val loss: 0.4900353633259472\n",
            "Epoch: 327, Train loss: 0.4213322492767934, Val loss: 0.4821297161673245\n",
            "Epoch: 328, Train loss: 0.41376824308991844, Val loss: 0.49221414994252355\n",
            "Epoch: 329, Train loss: 0.41507451893194736, Val loss: 0.47401526805601624\n",
            "Epoch: 330, Train loss: 0.4135760733331777, Val loss: 0.4836181496319018\n",
            "Epoch: 331, Train loss: 0.41150916268126364, Val loss: 0.47945893751947505\n",
            "Epoch: 332, Train loss: 0.41793333089395995, Val loss: 0.48134436105426986\n",
            "Epoch: 333, Train loss: 0.4259738416515015, Val loss: 0.4782055463445814\n",
            "Epoch: 334, Train loss: 0.41189717250707436, Val loss: 0.4836246351662435\n",
            "Epoch: 335, Train loss: 0.41427242494404387, Val loss: 0.49914260130179555\n",
            "Epoch: 336, Train loss: 0.41550466332957936, Val loss: 0.49374792450352717\n",
            "Epoch: 337, Train loss: 0.4207904730747995, Val loss: 0.473456194134135\n",
            "Epoch: 338, Train loss: 0.41311767016197204, Val loss: 0.4886029770499782\n",
            "Epoch: 339, Train loss: 0.4166032881889283, Val loss: 0.49310154938384104\n",
            "Epoch: 340, Train loss: 0.4126359888632923, Val loss: 0.49539934765351445\n",
            "Epoch: 341, Train loss: 0.41532957022838385, Val loss: 0.4732608316760314\n",
            "Epoch: 342, Train loss: 0.41446295052909343, Val loss: 0.4760595924760166\n",
            "Epoch: 343, Train loss: 0.4114947286797572, Val loss: 0.4786155572847316\n",
            "Epoch: 344, Train loss: 0.4107918283593054, Val loss: 0.4896042397932002\n",
            "Epoch: 345, Train loss: 0.4138133317787694, Val loss: 0.4885449554575117\n",
            "Epoch: 346, Train loss: 0.40964959507071985, Val loss: 0.48755625321676854\n",
            "Epoch: 347, Train loss: 0.4101567554486645, Val loss: 0.5080596208572388\n",
            "Epoch: 348, Train loss: 0.40808439094772686, Val loss: 0.4849076043618353\n",
            "Epoch: 349, Train loss: 0.41801527922586146, Val loss: 0.47634509166604594\n",
            "Epoch: 350, Train loss: 0.411902969026052, Val loss: 0.4808345068442194\n",
            "Epoch: 351, Train loss: 0.41549693531486764, Val loss: 0.47744738467429815\n",
            "Epoch: 352, Train loss: 0.4122704053099462, Val loss: 0.48424672139318364\n",
            "Epoch: 353, Train loss: 0.4169420923689361, Val loss: 0.47344745695590973\n",
            "Epoch: 354, Train loss: 0.4097254190682825, Val loss: 0.48787020342914683\n",
            "Epoch: 355, Train loss: 0.4119903450834109, Val loss: 0.48879105676161616\n",
            "Epoch: 356, Train loss: 0.4136967282693306, Val loss: 0.4904017809190248\n",
            "Epoch: 357, Train loss: 0.4095116945766585, Val loss: 0.49500907565418045\n",
            "Epoch: 358, Train loss: 0.41542975079768363, Val loss: 0.49154424157581833\n",
            "Epoch: 359, Train loss: 0.41130922446432694, Val loss: 0.4934172336208193\n",
            "Epoch: 360, Train loss: 0.4116543942401762, Val loss: 0.47103214577624675\n",
            "Epoch: 361, Train loss: 0.40504329563679914, Val loss: 0.49457512560643646\n",
            "Epoch: 362, Train loss: 0.4122660753919113, Val loss: 0.4940280416294148\n",
            "Epoch: 363, Train loss: 0.41714954218789524, Val loss: 0.49742652946396876\n",
            "Epoch: 364, Train loss: 0.4041248983044176, Val loss: 0.4745387878072889\n",
            "Epoch: 365, Train loss: 0.4157503846598789, Val loss: 0.4778442190665948\n",
            "Epoch: 366, Train loss: 0.4143087958121329, Val loss: 0.486759068934541\n",
            "Epoch: 367, Train loss: 0.41349838228294733, Val loss: 0.47978514001557704\n",
            "Epoch: 368, Train loss: 0.40919232003816114, Val loss: 0.49594890561543015\n",
            "Epoch: 369, Train loss: 0.41491686503953273, Val loss: 0.48468933881897674\n",
            "Epoch: 370, Train loss: 0.4139386060470203, Val loss: 0.4799367491351931\n",
            "Epoch: 371, Train loss: 0.41210191956380443, Val loss: 0.4986338231124376\n",
            "Epoch: 372, Train loss: 0.41419192351563444, Val loss: 0.4806225613543862\n",
            "Epoch: 373, Train loss: 0.4115429118530958, Val loss: 0.4932148452652128\n",
            "Epoch: 374, Train loss: 0.4147666070420936, Val loss: 0.489401356170052\n",
            "Epoch: 375, Train loss: 0.40567242621737454, Val loss: 0.47458206979851975\n",
            "Epoch: 376, Train loss: 0.4145482110661785, Val loss: 0.4819700368925145\n",
            "Epoch: 377, Train loss: 0.40994459607016237, Val loss: 0.49070444585461365\n",
            "Epoch: 378, Train loss: 0.41039855783911416, Val loss: 0.496254416672807\n",
            "Epoch: 379, Train loss: 0.4105352880938356, Val loss: 0.49355317691439077\n",
            "Epoch: 380, Train loss: 0.4104157430623276, Val loss: 0.4781418591737747\n",
            "Epoch: 381, Train loss: 0.412760994263632, Val loss: 0.49305496953035655\n",
            "Epoch: 382, Train loss: 0.410397277626765, Val loss: 0.4711502331652139\n",
            "Epoch: 383, Train loss: 0.4071239417674607, Val loss: 0.48691488959287343\n",
            "Epoch: 384, Train loss: 0.4102820288347635, Val loss: 0.48852872887724325\n",
            "Epoch: 385, Train loss: 0.4157907087844909, Val loss: 0.4913591864077668\n",
            "Epoch: 386, Train loss: 0.4046087971362166, Val loss: 0.4968179444733419\n",
            "Epoch: 387, Train loss: 0.4132388629176969, Val loss: 0.48643030304657786\n",
            "Epoch: 388, Train loss: 0.41310737268354486, Val loss: 0.47969509347489003\n",
            "Epoch: 389, Train loss: 0.40862672050172455, Val loss: 0.48784980922937393\n",
            "Epoch: 390, Train loss: 0.4069675681673054, Val loss: 0.49378040785852234\n",
            "Epoch: 391, Train loss: 0.41042216560736705, Val loss: 0.47475156148797587\n",
            "Epoch: 392, Train loss: 0.4030866063324755, Val loss: 0.486068678529639\n",
            "Epoch: 393, Train loss: 0.406127644308534, Val loss: 0.48135502205083247\n",
            "Epoch: 394, Train loss: 0.4046319938210052, Val loss: 0.4848645858858761\n",
            "Epoch: 395, Train loss: 0.4122526541522391, Val loss: 0.4823605971116769\n",
            "Epoch: 396, Train loss: 0.4033864325995279, Val loss: 0.483258118754939\n",
            "Epoch: 397, Train loss: 0.40885623710907093, Val loss: 0.4828820267790242\n",
            "Epoch: 398, Train loss: 0.40145945376930436, Val loss: 0.49376340602573593\n",
            "Epoch: 399, Train loss: 0.4108201570748179, Val loss: 0.48888692847992243\n",
            "Epoch: 400, Train loss: 0.40946469810824354, Val loss: 0.48704186591662857\n",
            "Epoch: 401, Train loss: 0.4071112850536434, Val loss: 0.47898576487051814\n",
            "Epoch: 402, Train loss: 0.40840913428347764, Val loss: 0.48145451161422226\n",
            "Epoch: 403, Train loss: 0.40068238857897787, Val loss: 0.4799688493734912\n",
            "Epoch: 404, Train loss: 0.4097311763082452, Val loss: 0.47093845001961054\n",
            "Epoch: 405, Train loss: 0.41653233366084336, Val loss: 0.4767603831071603\n",
            "Epoch: 406, Train loss: 0.40736522005233644, Val loss: 0.47995386664804657\n",
            "Epoch: 407, Train loss: 0.40628791763234384, Val loss: 0.49140456122787374\n",
            "Epoch: 408, Train loss: 0.4030805232529704, Val loss: 0.479441956862023\n",
            "Epoch: 409, Train loss: 0.4053738360405095, Val loss: 0.48800705098792124\n",
            "Epoch: 410, Train loss: 0.40440577332597066, Val loss: 0.4809619925524059\n",
            "Epoch: 411, Train loss: 0.40486141335138653, Val loss: 0.4776583466874926\n",
            "Epoch: 412, Train loss: 0.4037073558598153, Val loss: 0.4939443359249516\n",
            "Epoch: 413, Train loss: 0.40965125358426097, Val loss: 0.4813531272505459\n",
            "Epoch: 414, Train loss: 0.41075922637558915, Val loss: 0.4851531476566666\n",
            "Epoch: 415, Train loss: 0.4058260474783749, Val loss: 0.49371566270527084\n",
            "Epoch: 416, Train loss: 0.4090734198798803, Val loss: 0.4874468833992356\n",
            "Epoch: 417, Train loss: 0.40639792088644044, Val loss: 0.48450122734433726\n",
            "Epoch: 418, Train loss: 0.4123185146806471, Val loss: 0.493902292690779\n",
            "Epoch: 419, Train loss: 0.4070327404009612, Val loss: 0.49052591504235016\n",
            "Epoch: 420, Train loss: 0.40455730369966103, Val loss: 0.47937346602741043\n",
            "Epoch: 421, Train loss: 0.4082762775743428, Val loss: 0.4748593925645477\n",
            "Epoch: 422, Train loss: 0.40449972810955537, Val loss: 0.47713323251197215\n",
            "Epoch: 423, Train loss: 0.4106556574144415, Val loss: 0.47837844687072856\n",
            "Epoch: 424, Train loss: 0.4017762560061155, Val loss: 0.48329224006125804\n",
            "Epoch: 425, Train loss: 0.40226152308784957, Val loss: 0.4844991992178716\n",
            "Epoch: 426, Train loss: 0.4056304888683461, Val loss: 0.4875483171720254\n",
            "Epoch: 427, Train loss: 0.4050430550825123, Val loss: 0.5058539556829553\n",
            "Epoch: 428, Train loss: 0.4069735185479164, Val loss: 0.4793170894447126\n",
            "Epoch: 429, Train loss: 0.4057077610601852, Val loss: 0.4811298078612277\n",
            "Epoch: 430, Train loss: 0.40072499174849474, Val loss: 0.4762779584056453\n",
            "Epoch: 431, Train loss: 0.40681334662306107, Val loss: 0.4868287330395297\n",
            "Epoch: 432, Train loss: 0.41012454956452776, Val loss: 0.48784965551213216\n",
            "Epoch: 433, Train loss: 0.39941004491941995, Val loss: 0.4837374702880257\n",
            "Epoch: 434, Train loss: 0.40621101008619576, Val loss: 0.47100100274148743\n",
            "Epoch: 435, Train loss: 0.4072469290272096, Val loss: 0.4923360845760295\n",
            "Epoch: 436, Train loss: 0.4058727930935216, Val loss: 0.49410635977983475\n",
            "Epoch: 437, Train loss: 0.3981735839941836, Val loss: 0.4816929694069059\n",
            "Epoch: 438, Train loss: 0.4096152833012602, Val loss: 0.4892324840551929\n",
            "Epoch: 439, Train loss: 0.4047297687669278, Val loss: 0.47715586306233154\n",
            "Epoch: 440, Train loss: 0.40230453132364746, Val loss: 0.4831588150639283\n",
            "Epoch: 441, Train loss: 0.4041561272894012, Val loss: 0.4787115046852513\n",
            "Epoch: 442, Train loss: 0.40186892864775936, Val loss: 0.48739966240368393\n",
            "Epoch: 443, Train loss: 0.40278201325882745, Val loss: 0.49710417146745484\n",
            "Epoch: 444, Train loss: 0.4059467407235318, Val loss: 0.47582396943318217\n",
            "Epoch: 445, Train loss: 0.40241655104037133, Val loss: 0.4807935239453065\n",
            "Epoch: 446, Train loss: 0.40505724188337094, Val loss: 0.4772946128719731\n",
            "Epoch: 447, Train loss: 0.4054054458509018, Val loss: 0.48082415406641205\n",
            "Epoch: 448, Train loss: 0.4054169823191933, Val loss: 0.49472445837761225\n",
            "Epoch: 449, Train loss: 0.407081511118596, Val loss: 0.48703454905434657\n",
            "Epoch: 450, Train loss: 0.3997560785658157, Val loss: 0.48617589669792277\n",
            "Epoch: 451, Train loss: 0.40508309018833333, Val loss: 0.4866598330830273\n",
            "Epoch: 452, Train loss: 0.4071432712862949, Val loss: 0.47628694419798095\n",
            "Epoch: 453, Train loss: 0.3998290184380455, Val loss: 0.4763534304342772\n",
            "Epoch: 454, Train loss: 0.4022802354558566, Val loss: 0.49180946146187027\n",
            "Epoch: 455, Train loss: 0.40795404252688156, Val loss: 0.4779223195816341\n",
            "Epoch: 456, Train loss: 0.40378335053912384, Val loss: 0.47670176742892517\n",
            "Epoch: 457, Train loss: 0.40512664849695185, Val loss: 0.4735899348008005\n",
            "Epoch: 458, Train loss: 0.3992232002540473, Val loss: 0.4844276034518292\n",
            "Epoch: 459, Train loss: 0.40629727881656413, Val loss: 0.4812720033683275\n",
            "Epoch: 460, Train loss: 0.4018972621260783, Val loss: 0.48544878787116\n",
            "Epoch: 461, Train loss: 0.40143254191702576, Val loss: 0.48220245461714895\n",
            "Epoch: 462, Train loss: 0.4057690602622079, Val loss: 0.49486070204722254\n",
            "Epoch: 463, Train loss: 0.4029573087161091, Val loss: 0.4793247427595289\n",
            "Epoch: 464, Train loss: 0.3992752994473182, Val loss: 0.47653219574376154\n",
            "Epoch: 465, Train loss: 0.40187244892479923, Val loss: 0.4722808207336225\n",
            "Epoch: 466, Train loss: 0.4041741151245963, Val loss: 0.4790942116003287\n",
            "Epoch: 467, Train loss: 0.41153123963985666, Val loss: 0.47268469945380565\n",
            "Epoch: 468, Train loss: 0.4062069881875039, Val loss: 0.476881784043814\n",
            "Epoch: 469, Train loss: 0.4088037067048349, Val loss: 0.48423230530400024\n",
            "Epoch: 470, Train loss: 0.4029305884539887, Val loss: 0.4765024710642664\n",
            "Epoch: 471, Train loss: 0.4025151761197725, Val loss: 0.4849687806869808\n",
            "Epoch: 472, Train loss: 0.3952465026834128, Val loss: 0.4820624754617089\n",
            "Epoch: 473, Train loss: 0.39611941097404485, Val loss: 0.4782823167349163\n",
            "Epoch: 474, Train loss: 0.3992417633842549, Val loss: 0.4666218628224574\n",
            "Epoch: 475, Train loss: 0.3990478457597032, Val loss: 0.4811122605675145\n",
            "Epoch: 476, Train loss: 0.4058720036598338, Val loss: 0.48396261154036774\n",
            "Epoch: 477, Train loss: 0.4039221025983814, Val loss: 0.4863726731977965\n",
            "Epoch: 478, Train loss: 0.40748996697361095, Val loss: 0.4859808518698341\n",
            "Epoch: 479, Train loss: 0.40159547045630706, Val loss: 0.48043889905277054\n",
            "Epoch: 480, Train loss: 0.4044068690232024, Val loss: 0.4587634637167579\n",
            "Epoch: 481, Train loss: 0.3960489762354618, Val loss: 0.4817284388761771\n",
            "Epoch: 482, Train loss: 0.3968017755697922, Val loss: 0.4801854075569856\n",
            "Epoch: 483, Train loss: 0.40395168661744646, Val loss: 0.47545336030031504\n",
            "Epoch: 484, Train loss: 0.40183874839221995, Val loss: 0.4947919398546219\n",
            "Epoch: 485, Train loss: 0.40219928085638645, Val loss: 0.4911509081721306\n",
            "Epoch: 486, Train loss: 0.3953816823425611, Val loss: 0.46892791555116053\n",
            "Epoch: 487, Train loss: 0.39843866132922007, Val loss: 0.49377801034011337\n",
            "Epoch: 488, Train loss: 0.40444452822815613, Val loss: 0.4822786650375316\n",
            "Epoch: 489, Train loss: 0.40020795121020686, Val loss: 0.4865117916151097\n",
            "Epoch: 490, Train loss: 0.4002611680130756, Val loss: 0.4735537461544338\n",
            "Epoch: 491, Train loss: 0.40279432358266126, Val loss: 0.4881916030457145\n",
            "Epoch: 492, Train loss: 0.4103161596472643, Val loss: 0.48823056056311254\n",
            "Epoch: 493, Train loss: 0.409715844359355, Val loss: 0.4681814261957219\n",
            "Epoch: 494, Train loss: 0.402397558437271, Val loss: 0.4804034687970814\n",
            "Epoch: 495, Train loss: 0.40279287375239126, Val loss: 0.47258243356880386\n",
            "Epoch: 496, Train loss: 0.4094328794764917, Val loss: 0.4921781306988315\n",
            "Epoch: 497, Train loss: 0.3940765808389046, Val loss: 0.4935262677700896\n",
            "Epoch: 498, Train loss: 0.4003009049786953, Val loss: 0.4771549701690674\n",
            "Epoch: 499, Train loss: 0.3997143636374459, Val loss: 0.4837458976789525\n",
            "Epoch: 500, Train loss: 0.40122804315003596, Val loss: 0.4763153520853896\n",
            "Epoch: 501, Train loss: 0.39723570259807217, Val loss: 0.47882629029060664\n",
            "Epoch: 502, Train loss: 0.4018762334167951, Val loss: 0.47012414195035634\n",
            "Epoch: 503, Train loss: 0.3996738695365818, Val loss: 0.48567704189764827\n",
            "Epoch: 504, Train loss: 0.40542941554403783, Val loss: 0.4884122645384387\n",
            "Epoch: 505, Train loss: 0.39625813283000505, Val loss: 0.4919545693617118\n",
            "Epoch: 506, Train loss: 0.39329108170183974, Val loss: 0.4750481373385379\n",
            "Epoch: 507, Train loss: 0.39749473792720064, Val loss: 0.4715709058861983\n",
            "Epoch: 508, Train loss: 0.4005035084753727, Val loss: 0.4747979037071529\n",
            "Epoch: 509, Train loss: 0.40090537364616075, Val loss: 0.48425353475307165\n",
            "Epoch: 510, Train loss: 0.39498884591495104, Val loss: 0.4889832535072377\n",
            "Epoch: 511, Train loss: 0.4005316059613006, Val loss: 0.4922748835463273\n",
            "Epoch: 512, Train loss: 0.39995655773194155, Val loss: 0.4876426987742123\n",
            "Epoch: 513, Train loss: 0.3999753399266777, Val loss: 0.49040113193424123\n",
            "Epoch: 514, Train loss: 0.3963927401878697, Val loss: 0.469510386648931\n",
            "Epoch: 515, Train loss: 0.40221082163949357, Val loss: 0.4913729837066249\n",
            "Epoch: 516, Train loss: 0.39619968099371894, Val loss: 0.48068303065864665\n",
            "Epoch: 517, Train loss: 0.3960400170827582, Val loss: 0.4860039808248219\n",
            "Epoch: 518, Train loss: 0.3999133665487031, Val loss: 0.483296027701152\n",
            "Epoch: 519, Train loss: 0.3992138300002794, Val loss: 0.4951980576703423\n",
            "Epoch: 520, Train loss: 0.39326426489745625, Val loss: 0.4905060408146758\n",
            "Epoch: 521, Train loss: 0.3941776839012627, Val loss: 0.4960235988623218\n",
            "Epoch: 522, Train loss: 0.39882588174028016, Val loss: 0.47674676658291565\n",
            "Epoch: 523, Train loss: 0.3996197863069759, Val loss: 0.48201706299656316\n",
            "Epoch: 524, Train loss: 0.4021597433603845, Val loss: 0.49977033193174164\n",
            "Epoch: 525, Train loss: 0.3993598471830734, Val loss: 0.4917515124145307\n",
            "Epoch: 526, Train loss: 0.40284013500012444, Val loss: 0.4837591475562045\n",
            "Epoch: 527, Train loss: 0.40053504777325805, Val loss: 0.4783329438222082\n",
            "Epoch: 528, Train loss: 0.3966317012311005, Val loss: 0.4908287187940196\n",
            "Epoch: 529, Train loss: 0.3974363744635357, Val loss: 0.48580237478017807\n",
            "Epoch: 530, Train loss: 0.4034749016959912, Val loss: 0.47245915547797557\n",
            "Epoch: 531, Train loss: 0.39594493347772236, Val loss: 0.47856845981196355\n",
            "Epoch: 532, Train loss: 0.40165825921149845, Val loss: 0.4909982869499608\n",
            "Epoch: 533, Train loss: 0.4019809982854681, Val loss: 0.4784594464458917\n",
            "Epoch: 534, Train loss: 0.3975886742317848, Val loss: 0.4911229998657578\n",
            "Epoch: 535, Train loss: 0.3971251399371816, Val loss: 0.482506557514793\n",
            "Epoch: 536, Train loss: 0.39706997926573423, Val loss: 0.47909975679297195\n",
            "Epoch: 537, Train loss: 0.39714855396317295, Val loss: 0.4814762586825772\n",
            "Epoch: 538, Train loss: 0.39090691675312866, Val loss: 0.48648620161571\n",
            "Epoch: 539, Train loss: 0.40176207419800325, Val loss: 0.48688761968361705\n",
            "Epoch: 540, Train loss: 0.3934962554972861, Val loss: 0.47929160532198456\n",
            "Epoch: 541, Train loss: 0.4016800760809829, Val loss: 0.4793819428274506\n",
            "Epoch: 542, Train loss: 0.4032666305167221, Val loss: 0.49195315375139836\n",
            "Epoch: 543, Train loss: 0.40335705776868874, Val loss: 0.4793140488235574\n",
            "Epoch: 544, Train loss: 0.39800509415693935, Val loss: 0.4943398437217662\n",
            "Epoch: 545, Train loss: 0.39192557873501793, Val loss: 0.49978695182423843\n",
            "Epoch: 546, Train loss: 0.39352014753436165, Val loss: 0.4775198917639883\n",
            "Epoch: 547, Train loss: 0.3939359251065252, Val loss: 0.4917880771191497\n",
            "Epoch: 548, Train loss: 0.3974350865038947, Val loss: 0.48829086909168645\n",
            "Epoch: 549, Train loss: 0.39158482895439317, Val loss: 0.47772247736391266\n",
            "Epoch: 550, Train loss: 0.39628539549314806, Val loss: 0.49188622793084696\n",
            "Epoch: 551, Train loss: 0.401368688090591, Val loss: 0.4964974448084831\n",
            "Epoch: 552, Train loss: 0.4002419681544876, Val loss: 0.4860681754193808\n",
            "Epoch: 553, Train loss: 0.39451716126892594, Val loss: 0.4720018388409364\n",
            "Epoch: 554, Train loss: 0.4003824742692326, Val loss: 0.49694309736552994\n",
            "Epoch: 555, Train loss: 0.395944847485626, Val loss: 0.488954333490447\n",
            "Epoch: 556, Train loss: 0.3924927668043711, Val loss: 0.4754942153629504\n",
            "Epoch: 557, Train loss: 0.3919264210323198, Val loss: 0.4806988368693151\n",
            "Epoch: 558, Train loss: 0.39375684562759367, Val loss: 0.48787789478113774\n",
            "Epoch: 559, Train loss: 0.3961608396557958, Val loss: 0.4893839386732955\n",
            "Epoch: 560, Train loss: 0.39614580881853895, Val loss: 0.477748308918978\n",
            "Epoch: 561, Train loss: 0.3997254094042956, Val loss: 0.49515750572869655\n",
            "Epoch: 562, Train loss: 0.39519084132700266, Val loss: 0.4840676878627978\n",
            "Epoch: 563, Train loss: 0.3984777019308819, Val loss: 0.4689436727448514\n",
            "Epoch: 564, Train loss: 0.389619342553738, Val loss: 0.4929504053373086\n",
            "Epoch: 565, Train loss: 0.39030743095090836, Val loss: 0.48875489438834946\n",
            "Epoch: 566, Train loss: 0.3992266601253003, Val loss: 0.48044770328622116\n",
            "Epoch: 567, Train loss: 0.3936757483135411, Val loss: 0.47547296120932225\n",
            "Epoch: 568, Train loss: 0.3979686280040646, Val loss: 0.4717971051209851\n",
            "Epoch: 569, Train loss: 0.4105197287441051, Val loss: 0.48270870274619054\n",
            "Epoch: 570, Train loss: 0.3980202481329461, Val loss: 0.4802107148264584\n",
            "Epoch: 571, Train loss: 0.39184872911995966, Val loss: 0.48389605040612976\n",
            "Epoch: 572, Train loss: 0.3984560241319177, Val loss: 0.4829501795925592\n",
            "Epoch: 573, Train loss: 0.39303272458026656, Val loss: 0.4895834914947811\n",
            "Epoch: 574, Train loss: 0.39351147321549157, Val loss: 0.4863285943865776\n",
            "Epoch: 575, Train loss: 0.395148332157597, Val loss: 0.4774390070846206\n",
            "Epoch: 576, Train loss: 0.39309377328447254, Val loss: 0.4875200614333153\n",
            "Epoch: 577, Train loss: 0.39437282186220923, Val loss: 0.4755749471093479\n",
            "Epoch: 578, Train loss: 0.4078095932021138, Val loss: 0.49702026263663646\n",
            "Epoch: 579, Train loss: 0.40370498069340305, Val loss: 0.4692284962848613\n",
            "Epoch: 580, Train loss: 0.393767943760861, Val loss: 0.47850107440823003\n",
            "Epoch: 581, Train loss: 0.400097083647552, Val loss: 0.48875931375905085\n",
            "Epoch: 582, Train loss: 0.39426708643895236, Val loss: 0.4920129838742708\n",
            "Epoch: 583, Train loss: 0.3966410725852015, Val loss: 0.48574056593995346\n",
            "Epoch: 584, Train loss: 0.3897712135773483, Val loss: 0.4839585043097797\n",
            "Epoch: 585, Train loss: 0.38885682796297777, Val loss: 0.4799293048287693\n",
            "Epoch: 586, Train loss: 0.3896968756785671, Val loss: 0.48841842577645656\n",
            "Epoch: 587, Train loss: 0.39317382034155013, Val loss: 0.48208004863638626\n",
            "Epoch: 588, Train loss: 0.3942066746470417, Val loss: 0.47367822731796067\n",
            "Epoch: 589, Train loss: 0.3998003633856697, Val loss: 0.4933809375292377\n",
            "Epoch: 590, Train loss: 0.401045309169355, Val loss: 0.47323416251885264\n",
            "Epoch: 591, Train loss: 0.39846645809744696, Val loss: 0.4839282310322711\n",
            "Epoch: 592, Train loss: 0.39496623346092796, Val loss: 0.48343072361067724\n",
            "Epoch: 593, Train loss: 0.39461060152474264, Val loss: 0.4674921435745139\n",
            "Epoch: 594, Train loss: 0.40363430788277094, Val loss: 0.48344520714722183\n",
            "Epoch: 595, Train loss: 0.3972241132955171, Val loss: 0.48458225280046463\n",
            "Epoch: 596, Train loss: 0.3991388627560003, Val loss: 0.4928920582721108\n",
            "Epoch: 597, Train loss: 0.3934059290736157, Val loss: 0.4853468709870389\n",
            "Epoch: 598, Train loss: 0.3869686609000732, Val loss: 0.4850124461870444\n",
            "Epoch: 599, Train loss: 0.3930480179177323, Val loss: 0.4804045514840829\n",
            "Epoch: 600, Train loss: 0.3879330816943044, Val loss: 0.49695253058483724\n",
            "Epoch: 601, Train loss: 0.39116425201317356, Val loss: 0.49641549234327514\n",
            "Epoch: 602, Train loss: 0.39792331762612404, Val loss: 0.48914315708373723\n",
            "Epoch: 603, Train loss: 0.393345864903378, Val loss: 0.49861732242923035\n",
            "Epoch: 604, Train loss: 0.38726014610684356, Val loss: 0.4900511981625306\n",
            "Epoch: 605, Train loss: 0.39749299871204097, Val loss: 0.47938375449494314\n",
            "Epoch: 606, Train loss: 0.3956798940863729, Val loss: 0.4736878350377083\n",
            "Epoch: 607, Train loss: 0.3936560358655205, Val loss: 0.48434144846702876\n",
            "Epoch: 608, Train loss: 0.383561630536725, Val loss: 0.4888540643610452\n",
            "Epoch: 609, Train loss: 0.40276352563159534, Val loss: 0.4781135952002124\n",
            "Epoch: 610, Train loss: 0.3871577627822644, Val loss: 0.4935487104873908\n",
            "Epoch: 611, Train loss: 0.39536591958415324, Val loss: 0.4882906581226148\n",
            "Epoch: 612, Train loss: 0.39535044907253347, Val loss: 0.49094904841561066\n",
            "Epoch: 613, Train loss: 0.3927841066997097, Val loss: 0.5001839379730978\n",
            "Epoch: 614, Train loss: 0.39119121479277247, Val loss: 0.48745943488259064\n",
            "Epoch: 615, Train loss: 0.3964631204665974, Val loss: 0.4898780760796447\n",
            "Epoch: 616, Train loss: 0.38829754278550355, Val loss: 0.4894878158443852\n",
            "Epoch: 617, Train loss: 0.3905386975683376, Val loss: 0.4871743278283822\n",
            "Epoch: 618, Train loss: 0.39108655732785824, Val loss: 0.4932688811891957\n",
            "Epoch: 619, Train loss: 0.39348708586784703, Val loss: 0.49037489412646545\n",
            "Epoch: 620, Train loss: 0.3982587665834167, Val loss: 0.4766962367453073\n",
            "Epoch: 621, Train loss: 0.39677860368374707, Val loss: 0.4846383470453714\n",
            "Epoch: 622, Train loss: 0.38836701973203075, Val loss: 0.49677582477268417\n",
            "Epoch: 623, Train loss: 0.3997723442587735, Val loss: 0.4776004113649067\n",
            "Epoch: 624, Train loss: 0.3857512552023809, Val loss: 0.49395230216415303\n",
            "Epoch: 625, Train loss: 0.39175578580216586, Val loss: 0.48744689790826096\n",
            "Epoch: 626, Train loss: 0.39305504519320267, Val loss: 0.48942248993798304\n",
            "Epoch: 627, Train loss: 0.3911772501987785, Val loss: 0.4964261596140109\n",
            "Epoch: 628, Train loss: 0.39555990730597934, Val loss: 0.4920703156998283\n",
            "Epoch: 629, Train loss: 0.3892309540287639, Val loss: 0.48912731087521505\n",
            "Epoch: 630, Train loss: 0.39624482438693964, Val loss: 0.4884270692342206\n",
            "Epoch: 631, Train loss: 0.395416262130395, Val loss: 0.48588614557918747\n",
            "Epoch: 632, Train loss: 0.38687489227193633, Val loss: 0.490704729760948\n",
            "Epoch: 633, Train loss: 0.39163625339555586, Val loss: 0.4832904472162849\n",
            "Epoch: 634, Train loss: 0.3951200490196253, Val loss: 0.48564781013287994\n",
            "Epoch: 635, Train loss: 0.39906658217833674, Val loss: 0.48172673308535624\n",
            "Epoch: 636, Train loss: 0.39598402498205665, Val loss: 0.4782503486463898\n",
            "Epoch: 637, Train loss: 0.3933248896656434, Val loss: 0.4829346721893863\n",
            "Epoch: 638, Train loss: 0.3976084795940811, Val loss: 0.48447825132231964\n",
            "Epoch: 639, Train loss: 0.3899672185834795, Val loss: 0.47997249035458817\n",
            "Epoch: 640, Train loss: 0.38107309491868746, Val loss: 0.4810306053412588\n",
            "Epoch: 641, Train loss: 0.39255002082216006, Val loss: 0.46903012379219655\n",
            "Epoch: 642, Train loss: 0.3911056636040742, Val loss: 0.4848355058776705\n",
            "Epoch: 643, Train loss: 0.39077914419473614, Val loss: 0.4910407709447961\n",
            "Epoch: 644, Train loss: 0.3952917081908964, Val loss: 0.48287022074586466\n",
            "Epoch: 645, Train loss: 0.3956835762000831, Val loss: 0.4841737390348786\n",
            "Epoch: 646, Train loss: 0.3946772473824044, Val loss: 0.48257486404556976\n",
            "Epoch: 647, Train loss: 0.3870787242990105, Val loss: 0.4822779172345212\n",
            "Epoch: 648, Train loss: 0.38936231225190815, Val loss: 0.48828094883968953\n",
            "Epoch: 649, Train loss: 0.3976296509800925, Val loss: 0.47596479913121775\n",
            "Epoch: 650, Train loss: 0.3832414072477467, Val loss: 0.47720623486920405\n",
            "Epoch: 651, Train loss: 0.39701287820078734, Val loss: 0.4853777348210937\n",
            "Epoch: 652, Train loss: 0.38428929967685166, Val loss: 0.4898800834229118\n",
            "Epoch: 653, Train loss: 0.3931749315437004, Val loss: 0.49541340297774267\n",
            "Epoch: 654, Train loss: 0.3928539278511842, Val loss: 0.47868902745999786\n",
            "Epoch: 655, Train loss: 0.3851017939479849, Val loss: 0.49151933742196935\n",
            "Epoch: 656, Train loss: 0.38256565360497935, Val loss: 0.4780538729146907\n",
            "Epoch: 657, Train loss: 0.3965062398733464, Val loss: 0.4807114471730433\n",
            "Epoch: 658, Train loss: 0.38319488549185887, Val loss: 0.48314023253164795\n",
            "Epoch: 659, Train loss: 0.3881185946715383, Val loss: 0.4958077237794274\n",
            "Epoch: 660, Train loss: 0.39293059706126526, Val loss: 0.4907784830582769\n",
            "Epoch: 661, Train loss: 0.3942412600704021, Val loss: 0.48550704631366226\n",
            "Epoch: 662, Train loss: 0.3917089822673693, Val loss: 0.4693250244385318\n",
            "Epoch: 663, Train loss: 0.39409659364449323, Val loss: 0.4818524702599174\n",
            "Epoch: 664, Train loss: 0.3847150413243988, Val loss: 0.4976903764825118\n",
            "Epoch: 665, Train loss: 0.39034104793753804, Val loss: 0.4925813569050086\n",
            "Epoch: 666, Train loss: 0.39415364371305145, Val loss: 0.48856314193261297\n",
            "Epoch: 667, Train loss: 0.3894580976237763, Val loss: 0.4776216149330139\n",
            "Epoch: 668, Train loss: 0.3916989221804757, Val loss: 0.48578094730251714\n",
            "Epoch: 669, Train loss: 0.39139914736055864, Val loss: 0.49463633330244766\n",
            "Epoch: 670, Train loss: 0.3919564925329998, Val loss: 0.48948097621139725\n",
            "Epoch: 671, Train loss: 0.39128214393450794, Val loss: 0.4791830264424023\n",
            "Epoch: 672, Train loss: 0.3898929209234406, Val loss: 0.48724193596526194\n",
            "Epoch: 673, Train loss: 0.40213023243954854, Val loss: 0.47987695469668035\n",
            "Epoch: 674, Train loss: 0.389953978390707, Val loss: 0.5014144456700275\n",
            "Epoch: 675, Train loss: 0.3909664901079633, Val loss: 0.4910002382178056\n",
            "Epoch: 676, Train loss: 0.38830532251738115, Val loss: 0.4848467107666166\n",
            "Epoch: 677, Train loss: 0.39294290663352727, Val loss: 0.48077238507960973\n",
            "Epoch: 678, Train loss: 0.38680455248647383, Val loss: 0.4923446319605175\n",
            "Epoch: 679, Train loss: 0.3896625481574926, Val loss: 0.4696948959639198\n",
            "Epoch: 680, Train loss: 0.39053697293926015, Val loss: 0.47891420871019363\n",
            "Epoch: 681, Train loss: 0.39001790810727466, Val loss: 0.4935654123362742\n",
            "Epoch: 682, Train loss: 0.3839730019446798, Val loss: 0.5080020502209663\n",
            "Epoch: 683, Train loss: 0.38972722698480566, Val loss: 0.48101494539725154\n",
            "Epoch: 684, Train loss: 0.39371263881765717, Val loss: 0.485206663608551\n",
            "Epoch: 685, Train loss: 0.3879342839645301, Val loss: 0.482119961396644\n",
            "Epoch: 686, Train loss: 0.3928181376518046, Val loss: 0.4947831928730011\n",
            "Epoch: 687, Train loss: 0.3846156600966298, Val loss: 0.49161692512662786\n",
            "Epoch: 688, Train loss: 0.3895535618167547, Val loss: 0.48274138099268865\n",
            "Epoch: 689, Train loss: 0.38829813701568316, Val loss: 0.4994061252004222\n",
            "Epoch: 690, Train loss: 0.38884824391589307, Val loss: 0.5073888893974455\n",
            "Epoch: 691, Train loss: 0.3941492030171934, Val loss: 0.5109096118494084\n",
            "Epoch: 692, Train loss: 0.3899862128107913, Val loss: 0.48376258266599553\n",
            "Epoch: 693, Train loss: 0.38174022095665905, Val loss: 0.4808410017898208\n",
            "Epoch: 694, Train loss: 0.3821151360757689, Val loss: 0.475122847055134\n",
            "Epoch: 695, Train loss: 0.38400366772917155, Val loss: 0.48221723539264577\n",
            "Epoch: 696, Train loss: 0.38869236327830825, Val loss: 0.49980566532988296\n",
            "Epoch: 697, Train loss: 0.38849848467467746, Val loss: 0.4941972772541799\n",
            "Epoch: 698, Train loss: 0.3892035617531283, Val loss: 0.486900622515302\n",
            "Epoch: 699, Train loss: 0.39258936317892695, Val loss: 0.4895675868580216\n",
            "Epoch: 700, Train loss: 0.38555566973647837, Val loss: 0.48675551108623805\n",
            "Epoch: 701, Train loss: 0.38034031775624116, Val loss: 0.5071439076411096\n",
            "Epoch: 702, Train loss: 0.38369345141791217, Val loss: 0.4912668160702053\n",
            "Epoch: 703, Train loss: 0.3838559701446229, Val loss: 0.48689533691657216\n",
            "Epoch: 704, Train loss: 0.37916765248148987, Val loss: 0.49344046806034286\n",
            "Epoch: 705, Train loss: 0.39129112599067767, Val loss: 0.4824768571477187\n",
            "Epoch: 706, Train loss: 0.39837551468238835, Val loss: 0.4968167543411255\n",
            "Epoch: 707, Train loss: 0.38406330748441597, Val loss: 0.4904948465133968\n",
            "Epoch: 708, Train loss: 0.38691256245083006, Val loss: 0.4763122691135657\n",
            "Epoch: 709, Train loss: 0.38974164290389113, Val loss: 0.49716747944292267\n",
            "Epoch: 710, Train loss: 0.3890359813087762, Val loss: 0.48229801811670003\n",
            "Epoch: 711, Train loss: 0.38138507728543614, Val loss: 0.4996800979501323\n",
            "Epoch: 712, Train loss: 0.38199691681610043, Val loss: 0.4885723081074263\n",
            "Epoch: 713, Train loss: 0.38792530295842215, Val loss: 0.48592672928383474\n",
            "Epoch: 714, Train loss: 0.3940245855789418, Val loss: 0.49250915019135727\n",
            "Epoch: 715, Train loss: 0.3898689254044991, Val loss: 0.49177149251887675\n",
            "Epoch: 716, Train loss: 0.3887141020254072, Val loss: 0.500062177840032\n",
            "Epoch: 717, Train loss: 0.3851674135608521, Val loss: 0.47984426664678675\n",
            "Epoch: 718, Train loss: 0.38858927848068, Val loss: 0.4871080564825158\n",
            "Epoch: 719, Train loss: 0.3877885267300896, Val loss: 0.5022079034855491\n",
            "Epoch: 720, Train loss: 0.3862523967894637, Val loss: 0.4863877029795396\n",
            "Epoch: 721, Train loss: 0.3816389980935024, Val loss: 0.48413690258013575\n",
            "Epoch: 722, Train loss: 0.3832719849991772, Val loss: 0.47916527484592636\n",
            "Epoch: 723, Train loss: 0.3910744549084336, Val loss: 0.4909610077738762\n",
            "Epoch: 724, Train loss: 0.3813885255896434, Val loss: 0.49851765914967183\n",
            "Epoch: 725, Train loss: 0.3825683572947021, Val loss: 0.49909245693369914\n",
            "Epoch: 726, Train loss: 0.39116898095152625, Val loss: 0.4830935436644052\n",
            "Epoch: 727, Train loss: 0.3899801953621972, Val loss: 0.49116880368245275\n",
            "Epoch: 728, Train loss: 0.3871823980201448, Val loss: 0.49087410537820114\n",
            "Epoch: 729, Train loss: 0.38080026087475755, Val loss: 0.48218983176507446\n",
            "Epoch: 730, Train loss: 0.3935869198559964, Val loss: 0.4868049127490897\n",
            "Epoch: 731, Train loss: 0.39250469877405975, Val loss: 0.49001591927126836\n",
            "Epoch: 732, Train loss: 0.3881419420435983, Val loss: 0.473524046963767\n",
            "Epoch: 733, Train loss: 0.39699202101532555, Val loss: 0.473673105632004\n",
            "Epoch: 734, Train loss: 0.38532796562792543, Val loss: 0.4800233464491995\n",
            "Epoch: 735, Train loss: 0.3925137339613631, Val loss: 0.4871860554343776\n",
            "Epoch: 736, Train loss: 0.38495041696278093, Val loss: 0.49128824746922445\n",
            "Epoch: 737, Train loss: 0.3855816067750119, Val loss: 0.48276813681188385\n",
            "Epoch: 738, Train loss: 0.3872126033470685, Val loss: 0.4807428776433593\n",
            "Epoch: 739, Train loss: 0.385876770841881, Val loss: 0.48981092007536636\n",
            "Epoch: 740, Train loss: 0.38669198505783825, Val loss: 0.4894757882544869\n",
            "Epoch: 741, Train loss: 0.3839298854670778, Val loss: 0.4907259803853537\n",
            "Epoch: 742, Train loss: 0.3899146555557634, Val loss: 0.48409821798926905\n",
            "Epoch: 743, Train loss: 0.38486757701607854, Val loss: 0.4861564620545036\n",
            "Epoch: 744, Train loss: 0.3812931681479206, Val loss: 0.48306146499357727\n",
            "Epoch: 745, Train loss: 0.3830584402795559, Val loss: 0.4844144088657279\n",
            "Epoch: 746, Train loss: 0.392821948298017, Val loss: 0.482587463369495\n",
            "Epoch: 747, Train loss: 0.38076455761905825, Val loss: 0.4876355395505303\n",
            "Epoch: 748, Train loss: 0.3864968375600002, Val loss: 0.4875654931131162\n",
            "Epoch: 749, Train loss: 0.3838758914518686, Val loss: 0.4858367596017687\n",
            "Epoch: 750, Train loss: 0.3865282374747185, Val loss: 0.47012094800409515\n",
            "Epoch: 751, Train loss: 0.37952784916321325, Val loss: 0.48371366764369766\n",
            "Epoch: 752, Train loss: 0.3884172411395165, Val loss: 0.4856531549441187\n",
            "Epoch: 753, Train loss: 0.3881285072199949, Val loss: 0.4928615371647634\n",
            "Epoch: 754, Train loss: 0.38005441709786253, Val loss: 0.4870076908877021\n",
            "Epoch: 755, Train loss: 0.39045639421971995, Val loss: 0.48567580431699753\n",
            "Epoch: 756, Train loss: 0.376380643976998, Val loss: 0.4749621517564121\n",
            "Epoch: 757, Train loss: 0.39016837173735025, Val loss: 0.4813491023684803\n",
            "Epoch: 758, Train loss: 0.38285635419426123, Val loss: 0.4957290131010507\n",
            "Epoch: 759, Train loss: 0.38384540261608935, Val loss: 0.48290984018852834\n",
            "Epoch: 760, Train loss: 0.38806570161376674, Val loss: 0.4823461198493054\n",
            "Epoch: 761, Train loss: 0.3861441104006857, Val loss: 0.491274593299941\n",
            "Epoch: 762, Train loss: 0.3865285369084386, Val loss: 0.4885586758977489\n",
            "Epoch: 763, Train loss: 0.38041642191202524, Val loss: 0.4981086736446933\n",
            "Epoch: 764, Train loss: 0.3843703944124128, Val loss: 0.4847938292904904\n",
            "Epoch: 765, Train loss: 0.3917715975031449, Val loss: 0.48547475392881195\n",
            "Epoch: 766, Train loss: 0.3847447017524985, Val loss: 0.48198490942779343\n",
            "Epoch: 767, Train loss: 0.37906180787424865, Val loss: 0.48025377526095037\n",
            "Epoch: 768, Train loss: 0.39027688272384714, Val loss: 0.48129730436362717\n",
            "Epoch: 769, Train loss: 0.3857379431179709, Val loss: 0.48607226186677027\n",
            "Epoch: 770, Train loss: 0.38348316093979057, Val loss: 0.4769336483196208\n",
            "Epoch: 771, Train loss: 0.39021819645393774, Val loss: 0.48831367257394287\n",
            "Epoch: 772, Train loss: 0.38624082265564974, Val loss: 0.47815363069898204\n",
            "Epoch: 773, Train loss: 0.3843735389171643, Val loss: 0.4879325094975923\n",
            "Epoch: 774, Train loss: 0.38533674162798437, Val loss: 0.493526076407809\n",
            "Epoch: 775, Train loss: 0.37426197959722485, Val loss: 0.4869916266516635\n",
            "Epoch: 776, Train loss: 0.38225750714899714, Val loss: 0.48792223357840586\n",
            "Epoch: 777, Train loss: 0.3871324458640818, Val loss: 0.4952516104829939\n",
            "Epoch: 778, Train loss: 0.387262248937817, Val loss: 0.48897675307173477\n",
            "Epoch: 779, Train loss: 0.38472182876040356, Val loss: 0.48521994211171804\n",
            "Epoch: 780, Train loss: 0.3936224285254797, Val loss: 0.49319756423172195\n",
            "Epoch: 781, Train loss: 0.38702153376186604, Val loss: 0.4726504805056672\n",
            "Epoch: 782, Train loss: 0.38268430588837377, Val loss: 0.48770764038751\n",
            "Epoch: 783, Train loss: 0.38610119060419, Val loss: 0.47961207714520004\n",
            "Epoch: 784, Train loss: 0.38400746501217337, Val loss: 0.4824120904269971\n",
            "Epoch: 785, Train loss: 0.377140407791454, Val loss: 0.4959554868309121\n",
            "Epoch: 786, Train loss: 0.3881211368407785, Val loss: 0.4807270443753192\n",
            "Epoch: 787, Train loss: 0.38556528585042343, Val loss: 0.4938323572278023\n",
            "Epoch: 788, Train loss: 0.384143356559801, Val loss: 0.4865804989086954\n",
            "Epoch: 789, Train loss: 0.3765025524214205, Val loss: 0.47453183876840693\n",
            "Epoch: 790, Train loss: 0.3877314213804908, Val loss: 0.48524597208750875\n",
            "Epoch: 791, Train loss: 0.39101388706421775, Val loss: 0.4783835626746479\n",
            "Epoch: 792, Train loss: 0.3838756201177459, Val loss: 0.4841056646485078\n",
            "Epoch: 793, Train loss: 0.3899357306264723, Val loss: 0.49369052954410253\n",
            "Epoch: 794, Train loss: 0.38747424903486616, Val loss: 0.48579848675351395\n",
            "Epoch: 795, Train loss: 0.3847180497810962, Val loss: 0.4919322752638867\n",
            "Epoch: 796, Train loss: 0.37686426203545176, Val loss: 0.4828900604655868\n",
            "Epoch: 797, Train loss: 0.3770018551331592, Val loss: 0.4878946299615659\n",
            "Epoch: 798, Train loss: 0.3795250051209326, Val loss: 0.49222804135397863\n",
            "Epoch: 799, Train loss: 0.3840337630766238, Val loss: 0.48793406039476395\n",
            "Epoch: 800, Train loss: 0.38550735287670923, Val loss: 0.48565754686531265\n",
            "Epoch: 801, Train loss: 0.39003981748505767, Val loss: 0.4988993884701478\n",
            "Epoch: 802, Train loss: 0.3775929193222952, Val loss: 0.49375527979511963\n",
            "Epoch: 803, Train loss: 0.3825762451754443, Val loss: 0.5068864959635233\n",
            "Epoch: 804, Train loss: 0.3801272618171061, Val loss: 0.49064518040732336\n",
            "Epoch: 805, Train loss: 0.3796954055820549, Val loss: 0.5027985141465539\n",
            "Epoch: 806, Train loss: 0.37738407181071254, Val loss: 0.5068686431960056\n",
            "Epoch: 807, Train loss: 0.3900196647765258, Val loss: 0.4759905973547383\n",
            "Epoch: 808, Train loss: 0.3813905663556227, Val loss: 0.4938428723498395\n",
            "Epoch: 809, Train loss: 0.3813823256243383, Val loss: 0.4883673493015139\n",
            "Epoch: 810, Train loss: 0.38877904831670124, Val loss: 0.4885118242941405\n",
            "Epoch: 811, Train loss: 0.3805409702343121, Val loss: 0.49490031952920716\n",
            "Epoch: 812, Train loss: 0.3809562522399906, Val loss: 0.49272578758628743\n",
            "Epoch: 813, Train loss: 0.3779250914021241, Val loss: 0.48690328982315567\n",
            "Epoch: 814, Train loss: 0.3838653468619415, Val loss: 0.48749753479894836\n",
            "Epoch: 815, Train loss: 0.38650908727272204, Val loss: 0.48968500369473505\n",
            "Epoch: 816, Train loss: 0.38787446515048385, Val loss: 0.4854029358217591\n",
            "Epoch: 817, Train loss: 0.37910809200486717, Val loss: 0.4952757295809294\n",
            "Epoch: 818, Train loss: 0.3847079117783018, Val loss: 0.48305529748138626\n",
            "Epoch: 819, Train loss: 0.37460909036086343, Val loss: 0.5021437900631052\n",
            "Epoch: 820, Train loss: 0.3862413342446837, Val loss: 0.483520282334403\n",
            "Epoch: 821, Train loss: 0.3862335647293631, Val loss: 0.48782286361644145\n",
            "Epoch: 822, Train loss: 0.37464564794288574, Val loss: 0.49351382020272705\n",
            "Epoch: 823, Train loss: 0.3858754822439234, Val loss: 0.4763697221090919\n",
            "Epoch: 824, Train loss: 0.38139745328726027, Val loss: 0.48226264746565567\n",
            "Epoch: 825, Train loss: 0.3845302744219966, Val loss: 0.48316403439170436\n",
            "Epoch: 826, Train loss: 0.38213311994156884, Val loss: 0.49208623366920573\n",
            "Epoch: 827, Train loss: 0.38287254994482367, Val loss: 0.4988641891824572\n",
            "Epoch: 828, Train loss: 0.3856725919731277, Val loss: 0.4932978517915073\n",
            "Epoch: 829, Train loss: 0.377890305112558, Val loss: 0.4947054719454364\n",
            "Epoch: 830, Train loss: 0.3727368332417063, Val loss: 0.4996681080052727\n",
            "Epoch: 831, Train loss: 0.380700289203073, Val loss: 0.4874109845412405\n",
            "Epoch: 832, Train loss: 0.3871759536756064, Val loss: 0.48792370761695664\n",
            "Epoch: 833, Train loss: 0.38410832866998357, Val loss: 0.497820998100858\n",
            "Epoch: 834, Train loss: 0.37929954167650765, Val loss: 0.48446704957046005\n",
            "Epoch: 835, Train loss: 0.3858971428548157, Val loss: 0.4941779955437309\n",
            "Epoch: 836, Train loss: 0.38268038978617147, Val loss: 0.4872953044740777\n",
            "Epoch: 837, Train loss: 0.39403108398915404, Val loss: 0.4917692208760663\n",
            "Epoch: 838, Train loss: 0.39603113752729047, Val loss: 0.4812602173340948\n",
            "Epoch: 839, Train loss: 0.3881205703663529, Val loss: 0.47709955078990834\n",
            "Epoch: 840, Train loss: 0.3874141756532687, Val loss: 0.4894754988582511\n",
            "Epoch: 841, Train loss: 0.38221136959978913, Val loss: 0.48283012525031443\n",
            "Epoch: 842, Train loss: 0.3786375204284434, Val loss: 0.4803956633335666\n",
            "Epoch: 843, Train loss: 0.3842843566535843, Val loss: 0.4906145447963162\n",
            "Epoch: 844, Train loss: 0.376807758928911, Val loss: 0.4949054863107832\n",
            "Epoch: 845, Train loss: 0.385679940113937, Val loss: 0.4754411452694943\n",
            "Epoch: 846, Train loss: 0.38173114102476297, Val loss: 0.4854706914017075\n",
            "Epoch: 847, Train loss: 0.37208054223640796, Val loss: 0.49537541678077296\n",
            "Epoch: 848, Train loss: 0.38915505319284344, Val loss: 0.47889178795249837\n",
            "Epoch: 849, Train loss: 0.38625363516949957, Val loss: 0.4858453885505074\n",
            "Epoch: 850, Train loss: 0.379951413788331, Val loss: 0.48637456486099645\n",
            "Epoch: 851, Train loss: 0.37874061487008426, Val loss: 0.4900531988394888\n",
            "Epoch: 852, Train loss: 0.378129812610225, Val loss: 0.4854625694845852\n",
            "Epoch: 853, Train loss: 0.3794285804695225, Val loss: 0.49929317871206685\n",
            "Epoch: 854, Train loss: 0.38650774054011316, Val loss: 0.4901476502418518\n",
            "Epoch: 855, Train loss: 0.3764484538039305, Val loss: 0.5095082513595882\n",
            "Epoch: 856, Train loss: 0.38905353377106094, Val loss: 0.48653304341592285\n",
            "Epoch: 857, Train loss: 0.3856607952739697, Val loss: 0.5073056460211152\n",
            "Epoch: 858, Train loss: 0.38571627511012424, Val loss: 0.48849384682743174\n",
            "Epoch: 859, Train loss: 0.38268651030916595, Val loss: 0.49924158461784063\n",
            "Epoch: 860, Train loss: 0.38410818771899236, Val loss: 0.4829048455545777\n",
            "Epoch: 861, Train loss: 0.3824093444296091, Val loss: 0.4902853287364307\n",
            "Epoch: 862, Train loss: 0.3802891839051152, Val loss: 0.48748630755826045\n",
            "Epoch: 863, Train loss: 0.3851860562136924, Val loss: 0.49879731355529083\n",
            "Epoch: 864, Train loss: 0.38368959044984335, Val loss: 0.47793437187608917\n",
            "Epoch: 865, Train loss: 0.383435600111247, Val loss: 0.4781995294125457\n",
            "Epoch: 866, Train loss: 0.3860137766680529, Val loss: 0.4854116122189321\n",
            "Epoch: 867, Train loss: 0.36861761386478525, Val loss: 0.48102703219965887\n",
            "Epoch: 868, Train loss: 0.3775503209220957, Val loss: 0.49019811459277807\n",
            "Epoch: 869, Train loss: 0.3852374424824265, Val loss: 0.4977960147355732\n",
            "Epoch: 870, Train loss: 0.3822192067908853, Val loss: 0.48689066226545136\n",
            "Epoch: 871, Train loss: 0.37945326879099694, Val loss: 0.4815859053479998\n",
            "Epoch: 872, Train loss: 0.39029555799523874, Val loss: 0.48664884935868413\n",
            "Epoch: 873, Train loss: 0.3794229546351286, Val loss: 0.48739978239724513\n",
            "Epoch: 874, Train loss: 0.38488199736769535, Val loss: 0.49162637363923223\n",
            "Epoch: 875, Train loss: 0.3892563328999426, Val loss: 0.5019898112667235\n",
            "Epoch: 876, Train loss: 0.37853666603005287, Val loss: 0.47519476045119136\n",
            "Epoch: 877, Train loss: 0.38327607890478255, Val loss: 0.48357937445766047\n",
            "Epoch: 878, Train loss: 0.37808196491852963, Val loss: 0.4800063697131057\n",
            "Epoch: 879, Train loss: 0.3826059460501021, Val loss: 0.495519577672607\n",
            "Epoch: 880, Train loss: 0.3854412398836596, Val loss: 0.47736859831370804\n",
            "Epoch: 881, Train loss: 0.38247131428724207, Val loss: 0.4946145576081778\n",
            "Epoch: 882, Train loss: 0.38323926745841524, Val loss: 0.4917968154737824\n",
            "Epoch: 883, Train loss: 0.3788223438668119, Val loss: 0.48600586935093526\n",
            "Epoch: 884, Train loss: 0.37720247521005146, Val loss: 0.47858916144622\n",
            "Epoch: 885, Train loss: 0.3896110864098663, Val loss: 0.49161122621674286\n",
            "Epoch: 886, Train loss: 0.38158632791778446, Val loss: 0.48269446351026235\n",
            "Epoch: 887, Train loss: 0.38173466018332225, Val loss: 0.4843609215397584\n",
            "Epoch: 888, Train loss: 0.376609520455069, Val loss: 0.48009638370651947\n",
            "Epoch: 889, Train loss: 0.3748069080433267, Val loss: 0.49559342429826136\n",
            "Epoch: 890, Train loss: 0.37846391680136326, Val loss: 0.4839708334521243\n",
            "Epoch: 891, Train loss: 0.37812050452417845, Val loss: 0.4836811815437518\n",
            "Epoch: 892, Train loss: 0.38105598073010555, Val loss: 0.48757965078479365\n",
            "Epoch: 893, Train loss: 0.37452943308855485, Val loss: 0.4767481919966246\n",
            "Epoch: 894, Train loss: 0.3771429394378226, Val loss: 0.4891036988088959\n",
            "Epoch: 895, Train loss: 0.3759675434668178, Val loss: 0.5007318952365926\n",
            "Epoch: 896, Train loss: 0.3865621783007637, Val loss: 0.4883107809643996\n",
            "Epoch: 897, Train loss: 0.37766121353410176, Val loss: 0.4849325489056738\n",
            "Epoch: 898, Train loss: 0.3832499083002676, Val loss: 0.47625654543700974\n",
            "Epoch: 899, Train loss: 0.38497819083855933, Val loss: 0.49792393689092834\n",
            "Epoch: 900, Train loss: 0.37919382556957276, Val loss: 0.4884827003667229\n",
            "Epoch: 901, Train loss: 0.38785793078172687, Val loss: 0.5015145759833487\n",
            "Epoch: 902, Train loss: 0.37692631119495645, Val loss: 0.4987260942396365\n",
            "Epoch: 903, Train loss: 0.3768205503711591, Val loss: 0.4898749699718074\n",
            "Epoch: 904, Train loss: 0.37600445475518046, Val loss: 0.4860179565454784\n",
            "Epoch: 905, Train loss: 0.38090969536553854, Val loss: 0.47510079961074025\n",
            "Epoch: 906, Train loss: 0.375689121823, Val loss: 0.4806952166714166\n",
            "Epoch: 907, Train loss: 0.3741752968739127, Val loss: 0.4867445704968352\n",
            "Epoch: 908, Train loss: 0.3780654798644432, Val loss: 0.477997918662272\n",
            "Epoch: 909, Train loss: 0.3830532387962928, Val loss: 0.49115973162023646\n",
            "Epoch: 910, Train loss: 0.3833599677631494, Val loss: 0.4830608293414116\n",
            "Epoch: 911, Train loss: 0.383201206961612, Val loss: 0.499542315147425\n",
            "Epoch: 912, Train loss: 0.37630810615366334, Val loss: 0.4874950302274604\n",
            "Epoch: 913, Train loss: 0.3785243228540467, Val loss: 0.4883540296240857\n",
            "Epoch: 914, Train loss: 0.3826450655739183, Val loss: 0.48704669702994197\n",
            "Epoch: 915, Train loss: 0.38055432927681865, Val loss: 0.48328219471793427\n",
            "Epoch: 916, Train loss: 0.38058504961354533, Val loss: 0.48625884479597997\n",
            "Epoch: 917, Train loss: 0.37584732172322455, Val loss: 0.4955708200210019\n",
            "Epoch: 918, Train loss: 0.37907856028263825, Val loss: 0.4896553025433892\n",
            "Epoch: 919, Train loss: 0.37854103098861613, Val loss: 0.49129895512994964\n",
            "Epoch: 920, Train loss: 0.38370875095342133, Val loss: 0.4874332653064477\n",
            "Epoch: 921, Train loss: 0.38132886054615556, Val loss: 0.4869961260180724\n",
            "Epoch: 922, Train loss: 0.38017185606519915, Val loss: 0.47731987661436986\n",
            "Epoch: 923, Train loss: 0.37945134992086066, Val loss: 0.5015126380481219\n",
            "Epoch: 924, Train loss: 0.37405759999487564, Val loss: 0.49840730977685826\n",
            "Epoch: 925, Train loss: 0.3708189461600712, Val loss: 0.5029753956355547\n",
            "Epoch: 926, Train loss: 0.3794667117793621, Val loss: 0.48609238862991333\n",
            "Epoch: 927, Train loss: 0.38445080579634106, Val loss: 0.4874286106542537\n",
            "Epoch: 928, Train loss: 0.379287432228608, Val loss: 0.49024580811199386\n",
            "Epoch: 929, Train loss: 0.38035330282478697, Val loss: 0.4919000883635722\n",
            "Epoch: 930, Train loss: 0.38005494603640916, Val loss: 0.4858408233052806\n",
            "Epoch: 931, Train loss: 0.3825438076281566, Val loss: 0.486371117594995\n",
            "Epoch: 932, Train loss: 0.37718750864273626, Val loss: 0.4926887131051013\n",
            "Epoch: 933, Train loss: 0.3765424981007467, Val loss: 0.47594509156126724\n",
            "Epoch: 934, Train loss: 0.37908839759672286, Val loss: 0.4830856793805173\n",
            "Epoch: 935, Train loss: 0.38594355148637627, Val loss: 0.48967436191282776\n",
            "Epoch: 936, Train loss: 0.3764195549285632, Val loss: 0.4856352527674876\n",
            "Epoch: 937, Train loss: 0.3807583140380451, Val loss: 0.4813364999074685\n",
            "Epoch: 938, Train loss: 0.38116210068277956, Val loss: 0.4858885030997427\n",
            "Epoch: 939, Train loss: 0.3717754873698773, Val loss: 0.4874871557480411\n",
            "Epoch: 940, Train loss: 0.37187898340211056, Val loss: 0.4966788005672003\n",
            "Epoch: 941, Train loss: 0.3743305512641568, Val loss: 0.4968635259490264\n",
            "Epoch: 942, Train loss: 0.37219826408743517, Val loss: 0.5006707812610426\n",
            "Epoch: 943, Train loss: 0.37547793383883793, Val loss: 0.4899569804731168\n",
            "Epoch: 944, Train loss: 0.3720517360838587, Val loss: 0.49770173195161316\n",
            "Epoch: 945, Train loss: 0.37076470622805213, Val loss: 0.5053273666846124\n",
            "Epoch: 946, Train loss: 0.37235760071021096, Val loss: 0.4942448382314883\n",
            "Epoch: 947, Train loss: 0.37104278551333353, Val loss: 0.48894235844674866\n",
            "Epoch: 948, Train loss: 0.3759356297282778, Val loss: 0.4946570455243713\n",
            "Epoch: 949, Train loss: 0.3839594545521977, Val loss: 0.4843606929245748\n",
            "Epoch: 950, Train loss: 0.375130796439184, Val loss: 0.48838175597943756\n",
            "Epoch: 951, Train loss: 0.37924198790618024, Val loss: 0.4821891502330178\n",
            "Epoch: 952, Train loss: 0.3796579308374483, Val loss: 0.49454138114264135\n",
            "Epoch: 953, Train loss: 0.37284560440680375, Val loss: 0.48165022817097214\n",
            "Epoch: 954, Train loss: 0.3736306118961753, Val loss: 0.48608378399359553\n",
            "Epoch: 955, Train loss: 0.3874738310198945, Val loss: 0.49369841735613973\n",
            "Epoch: 956, Train loss: 0.3641307144688435, Val loss: 0.4931110457370156\n",
            "Epoch: 957, Train loss: 0.38284410481214226, Val loss: 0.49151043437029185\n",
            "Epoch: 958, Train loss: 0.3787919718039035, Val loss: 0.4941989865742232\n",
            "Epoch: 959, Train loss: 0.37571744813733976, Val loss: 0.4955646293727975\n",
            "Epoch: 960, Train loss: 0.3729538788644404, Val loss: 0.49342601158116994\n",
            "Epoch: 961, Train loss: 0.3790962033895231, Val loss: 0.4898934940758504\n",
            "Epoch: 962, Train loss: 0.3725828220797933, Val loss: 0.49235704776487854\n",
            "Epoch: 963, Train loss: 0.37849309959524907, Val loss: 0.49254944920539856\n",
            "Epoch: 964, Train loss: 0.3751375651460243, Val loss: 0.481643903412317\n",
            "Epoch: 965, Train loss: 0.3768705834843123, Val loss: 0.486131635935683\n",
            "Epoch: 966, Train loss: 0.3712741484503406, Val loss: 0.49542623248539475\n",
            "Epoch: 967, Train loss: 0.3795486838939432, Val loss: 0.5070482692436168\n",
            "Epoch: 968, Train loss: 0.37623670423062916, Val loss: 0.4865221667446588\n",
            "Epoch: 969, Train loss: 0.38352696119798696, Val loss: 0.4899286863050963\n",
            "Epoch: 970, Train loss: 0.37658244868617663, Val loss: 0.48481569870522145\n",
            "Epoch: 971, Train loss: 0.37917156844298605, Val loss: 0.48813596132554504\n",
            "Epoch: 972, Train loss: 0.37387136221689105, Val loss: 0.4892366853983779\n",
            "Epoch: 973, Train loss: 0.3778699998794841, Val loss: 0.48627711793309764\n",
            "Epoch: 974, Train loss: 0.37363580043048933, Val loss: 0.48871357974253204\n",
            "Epoch: 975, Train loss: 0.3794735074500586, Val loss: 0.49400236379159124\n",
            "Epoch: 976, Train loss: 0.3807130495196381, Val loss: 0.4959457818614809\n",
            "Epoch: 977, Train loss: 0.3789820217236244, Val loss: 0.49533645024425105\n",
            "Epoch: 978, Train loss: 0.38041495137869363, Val loss: 0.49017060783348587\n",
            "Epoch: 979, Train loss: 0.36917068818511123, Val loss: 0.49085614634187597\n",
            "Epoch: 980, Train loss: 0.3751199000392309, Val loss: 0.4852982026181723\n",
            "Epoch: 981, Train loss: 0.3739541966489374, Val loss: 0.48966867045352336\n",
            "Epoch: 982, Train loss: 0.36461641965929725, Val loss: 0.502316132972115\n",
            "Epoch: 983, Train loss: 0.38423022303106075, Val loss: 0.4940752775261277\n",
            "Epoch: 984, Train loss: 0.38405145853055195, Val loss: 0.4831113191811662\n",
            "Epoch: 985, Train loss: 0.3729531306575507, Val loss: 0.505122872167512\n",
            "Epoch: 986, Train loss: 0.3744776903499395, Val loss: 0.48966740150200694\n",
            "Epoch: 987, Train loss: 0.3776891868336923, Val loss: 0.4999401757591649\n",
            "Epoch: 988, Train loss: 0.3772117920115954, Val loss: 0.4847717434167862\n",
            "Epoch: 989, Train loss: 0.3754341472329679, Val loss: 0.4943763829375568\n",
            "Epoch: 990, Train loss: 0.3812886520152081, Val loss: 0.496144600212574\n",
            "Epoch: 991, Train loss: 0.38056813819346924, Val loss: 0.4949363219110589\n",
            "Epoch: 992, Train loss: 0.37711637876094906, Val loss: 0.4843145710857291\n",
            "Epoch: 993, Train loss: 0.37685772610904394, Val loss: 0.4919394431145568\n",
            "Epoch: 994, Train loss: 0.37245247940044696, Val loss: 0.4895102522875133\n",
            "Epoch: 995, Train loss: 0.3821697465630532, Val loss: 0.5098255986445829\n",
            "Epoch: 996, Train loss: 0.38123040040347705, Val loss: 0.4878074452280998\n",
            "Epoch: 997, Train loss: 0.3768690311176771, Val loss: 0.4933767052073228\n",
            "Epoch: 998, Train loss: 0.37424688847248805, Val loss: 0.49449250180470317\n",
            "Epoch: 999, Train loss: 0.3763410831520916, Val loss: 0.4978336331091429\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXw0R-Tkucgw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_recalls(targets, predictions):\n",
        "  n_classes = 2\n",
        "  confusion_matrix, _, _ = np.histogram2d(targets.detach().cpu().numpy(), predictions.squeeze().detach().cpu().numpy(), bins=n_classes)\n",
        "  return np.diag(confusion_matrix) / (np.sum(confusion_matrix, axis=1))\n",
        "\n",
        "def gmean(classifier, dataloader):\n",
        "  with torch.no_grad():\n",
        "    classifier.eval()\n",
        "    recalls = np.zeros((2))\n",
        "    for inputs, targets in dataloader:\n",
        "      if torch.cuda.is_available():\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "      \n",
        "      outputs = classifier(inputs.float())\n",
        "      predictions = torch.round(outputs).int()\n",
        "      recalls += calculate_recalls(targets, predictions)\n",
        "\n",
        "    recalls = recalls / len(dataloader)\n",
        "    return mstats.gmean(recalls)    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlrNw0bEqAeh",
        "colab_type": "code",
        "outputId": "cbc53acf-c8c1-4807-e3ed-2a4dddb5f632",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "classifier = load_classifier()\n",
        "if torch.cuda.is_available():\n",
        "  classifier = classifier.cuda()\n",
        "train_accuracy = gmean(classifier, train_dataloader)\n",
        "test_accuracy = gmean(classifier, test_dataloader)\n",
        "print('Epochs: {}'.format(classifier.epochs))\n",
        "print('Train g-mean: {}'.format(train_accuracy.item()))\n",
        "print('Test g-mean: {}'.format(test_accuracy.item()))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epochs: 480\n",
            "Train g-mean: 0.8387829745989959\n",
            "Test g-mean: nan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in true_divide\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "918KM5F07hFl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}