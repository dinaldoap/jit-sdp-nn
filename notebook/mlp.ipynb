{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JIT-SDP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dinaldoap/jit-sdp-nn/blob/master/notebook/mlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXwvNEAQE6NO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import random\n",
        "from scipy.stats import mstats"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAupgTDfmJ_I",
        "colab_type": "code",
        "outputId": "18c56275-6ce6-4563-9178-bcd53ad0dafa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/dinaldoap/jit-sdp-data/master/jenkins.csv')\n",
        "df.head()"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fix</th>\n",
              "      <th>ns</th>\n",
              "      <th>nd</th>\n",
              "      <th>nf</th>\n",
              "      <th>entropy</th>\n",
              "      <th>la</th>\n",
              "      <th>ld</th>\n",
              "      <th>lt</th>\n",
              "      <th>ndev</th>\n",
              "      <th>age</th>\n",
              "      <th>nuc</th>\n",
              "      <th>exp</th>\n",
              "      <th>rexp</th>\n",
              "      <th>sexp</th>\n",
              "      <th>author_date_unix_timestamp</th>\n",
              "      <th>classification</th>\n",
              "      <th>contains_bug</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>False</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.641604</td>\n",
              "      <td>9.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>426.428571</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.000093</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5171.0</td>\n",
              "      <td>30.227271</td>\n",
              "      <td>1472.714286</td>\n",
              "      <td>1555326371</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>False</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.750000</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>426.428571</td>\n",
              "      <td>100.0</td>\n",
              "      <td>6.314775</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5170.0</td>\n",
              "      <td>29.227271</td>\n",
              "      <td>1471.714286</td>\n",
              "      <td>1555326363</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>False</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.906580</td>\n",
              "      <td>15.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>96.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.034722</td>\n",
              "      <td>2.0</td>\n",
              "      <td>629.0</td>\n",
              "      <td>14.828373</td>\n",
              "      <td>414.000000</td>\n",
              "      <td>1554971763</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>False</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.058824</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1554969774</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>False</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.662506</td>\n",
              "      <td>14.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>67.000000</td>\n",
              "      <td>6.0</td>\n",
              "      <td>21.280683</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.058824</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1554967752</td>\n",
              "      <td>Feature Addition</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     fix   ns   nd  ...  author_date_unix_timestamp    classification  contains_bug\n",
              "0  False  7.0  7.0  ...                  1555326371              None         False\n",
              "1  False  7.0  7.0  ...                  1555326363              None         False\n",
              "2  False  1.0  1.0  ...                  1554971763              None         False\n",
              "3  False  1.0  1.0  ...                  1554969774              None         False\n",
              "4  False  1.0  2.0  ...                  1554967752  Feature Addition         False\n",
              "\n",
              "[5 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alizaxdPD3El",
        "colab_type": "code",
        "outputId": "cb5268d6-5ef8-4c99-a148-742443ac6cb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "#df = df.sample(frac=1)\n",
        "label_col = 'contains_bug'\n",
        "features_cols = ['fix', 'ns', 'nd', 'nf', 'entropy', 'la', 'ld', 'lt', 'ndev', 'age', 'nuc', 'exp', 'rexp', 'sexp', 'classification']\n",
        "x = df[features_cols]\n",
        "x['fix'] = x['fix'].astype('int')\n",
        "df_classification = pd.get_dummies(x, columns=['classification'])\n",
        "x = pd.concat([x, df_classification], axis='columns')\n",
        "x = x.drop(['classification'], axis='columns')\n",
        "x = x.values\n",
        "y = df[label_col]\n",
        "y = y.astype('category')\n",
        "y = y.cat.codes\n",
        "y = y.values"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBD1-tp2ExzQ",
        "colab_type": "code",
        "outputId": "82c6c12a-78e9-4531-9e37-baa7746b9ac3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "print(x[:3])\n",
        "print(y[:3])"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.00000000e+00 7.00000000e+00 7.00000000e+00 7.00000000e+00\n",
            "  2.64160417e+00 9.00000000e+00 9.00000000e+00 4.26428571e+02\n",
            "  1.00000000e+02 9.25925926e-05 1.00000000e+00 5.17100000e+03\n",
            "  3.02272705e+01 1.47271429e+03 0.00000000e+00 7.00000000e+00\n",
            "  7.00000000e+00 7.00000000e+00 2.64160417e+00 9.00000000e+00\n",
            "  9.00000000e+00 4.26428571e+02 1.00000000e+02 9.25925926e-05\n",
            "  1.00000000e+00 5.17100000e+03 3.02272705e+01 1.47271429e+03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 7.00000000e+00 7.00000000e+00 7.00000000e+00\n",
            "  2.75000000e+00 8.00000000e+00 8.00000000e+00 4.26428571e+02\n",
            "  1.00000000e+02 6.31477513e+00 2.00000000e+00 5.17000000e+03\n",
            "  2.92272705e+01 1.47171429e+03 0.00000000e+00 7.00000000e+00\n",
            "  7.00000000e+00 7.00000000e+00 2.75000000e+00 8.00000000e+00\n",
            "  8.00000000e+00 4.26428571e+02 1.00000000e+02 6.31477513e+00\n",
            "  2.00000000e+00 5.17000000e+03 2.92272705e+01 1.47171429e+03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 1.00000000e+00 2.00000000e+00\n",
            "  9.06579548e-01 1.50000000e+01 4.40000000e+01 9.60000000e+01\n",
            "  4.00000000e+00 3.47222222e-02 2.00000000e+00 6.29000000e+02\n",
            "  1.48283729e+01 4.14000000e+02 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 2.00000000e+00 9.06579548e-01 1.50000000e+01\n",
            "  4.40000000e+01 9.60000000e+01 4.00000000e+00 3.47222222e-02\n",
            "  2.00000000e+00 6.29000000e+02 1.48283729e+01 4.14000000e+02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "[0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwCn1uNTMjfG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_index = int( len(x) * 0.8 )\n",
        "test_index = int( len(x) * 0.9 )\n",
        "x_train, y_train = x[:val_index], y[:val_index]\n",
        "x_val, y_val = x[val_index:test_index], y[val_index:test_index]\n",
        "x_test, y_test = x[test_index:], y[test_index:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSqnoapE5Poq",
        "colab_type": "code",
        "outputId": "9042e60b-9b6c-4df5-fad4-25fd19aea25d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(x_train[:3])\n",
        "print(x_val[:3])\n",
        "print(x_test[:3])"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.00000000e+00 7.00000000e+00 7.00000000e+00 7.00000000e+00\n",
            "  2.64160417e+00 9.00000000e+00 9.00000000e+00 4.26428571e+02\n",
            "  1.00000000e+02 9.25925926e-05 1.00000000e+00 5.17100000e+03\n",
            "  3.02272705e+01 1.47271429e+03 0.00000000e+00 7.00000000e+00\n",
            "  7.00000000e+00 7.00000000e+00 2.64160417e+00 9.00000000e+00\n",
            "  9.00000000e+00 4.26428571e+02 1.00000000e+02 9.25925926e-05\n",
            "  1.00000000e+00 5.17100000e+03 3.02272705e+01 1.47271429e+03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 7.00000000e+00 7.00000000e+00 7.00000000e+00\n",
            "  2.75000000e+00 8.00000000e+00 8.00000000e+00 4.26428571e+02\n",
            "  1.00000000e+02 6.31477513e+00 2.00000000e+00 5.17000000e+03\n",
            "  2.92272705e+01 1.47171429e+03 0.00000000e+00 7.00000000e+00\n",
            "  7.00000000e+00 7.00000000e+00 2.75000000e+00 8.00000000e+00\n",
            "  8.00000000e+00 4.26428571e+02 1.00000000e+02 6.31477513e+00\n",
            "  2.00000000e+00 5.17000000e+03 2.92272705e+01 1.47171429e+03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 1.00000000e+00 2.00000000e+00\n",
            "  9.06579548e-01 1.50000000e+01 4.40000000e+01 9.60000000e+01\n",
            "  4.00000000e+00 3.47222222e-02 2.00000000e+00 6.29000000e+02\n",
            "  1.48283729e+01 4.14000000e+02 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 2.00000000e+00 9.06579548e-01 1.50000000e+01\n",
            "  4.40000000e+01 9.60000000e+01 4.00000000e+00 3.47222222e-02\n",
            "  2.00000000e+00 6.29000000e+02 1.48283729e+01 4.14000000e+02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "[[0.00000000e+00 8.00000000e+00 8.00000000e+00 8.00000000e+00\n",
            "  2.73215889e+00 1.10000000e+01 1.10000000e+01 1.58625000e+02\n",
            "  5.00000000e+00 1.96759259e-04 1.00000000e+00 4.17300000e+03\n",
            "  2.45570735e+02 8.19125000e+02 0.00000000e+00 8.00000000e+00\n",
            "  8.00000000e+00 8.00000000e+00 2.73215889e+00 1.10000000e+01\n",
            "  1.10000000e+01 1.58625000e+02 5.00000000e+00 1.96759259e-04\n",
            "  1.00000000e+00 4.17300000e+03 2.45570735e+02 8.19125000e+02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 8.00000000e+00 8.00000000e+00 8.00000000e+00\n",
            "  2.73215889e+00 1.10000000e+01 1.10000000e+01 1.58625000e+02\n",
            "  5.00000000e+00 4.29604022e+00 5.00000000e+00 4.17200000e+03\n",
            "  2.44570735e+02 8.18125000e+02 0.00000000e+00 8.00000000e+00\n",
            "  8.00000000e+00 8.00000000e+00 2.73215889e+00 1.10000000e+01\n",
            "  1.10000000e+01 1.58625000e+02 5.00000000e+00 4.29604022e+00\n",
            "  5.00000000e+00 4.17200000e+03 2.44570735e+02 8.18125000e+02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 1.00000000e+00 1.07000000e+02\n",
            "  1.00000000e+00 5.29513889e-01 1.00000000e+00 4.17100000e+03\n",
            "  2.43572058e+02 8.40000000e+01 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.07000000e+02 1.00000000e+00 5.29513889e-01\n",
            "  1.00000000e+00 4.17100000e+03 2.43572058e+02 8.40000000e+01\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "[[0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 1.00000000e+00 2.31000000e+02\n",
            "  2.00000000e+00 8.17129630e-03 1.00000000e+00 2.29000000e+03\n",
            "  2.35138387e+02 1.96900000e+03 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 2.31000000e+02 2.00000000e+00 8.17129630e-03\n",
            "  1.00000000e+00 2.29000000e+03 2.35138387e+02 1.96900000e+03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 3.00000000e+00 4.00000000e+00\n",
            "  1.92417435e+00 1.00000000e+01 4.00000000e+00 7.95000000e+01\n",
            "  2.00000000e+00 7.59004630e-01 4.00000000e+00 2.28900000e+03\n",
            "  2.34145689e+02 1.96800000e+03 0.00000000e+00 1.00000000e+00\n",
            "  3.00000000e+00 4.00000000e+00 1.92417435e+00 1.00000000e+01\n",
            "  4.00000000e+00 7.95000000e+01 2.00000000e+00 7.59004630e-01\n",
            "  4.00000000e+00 2.28900000e+03 2.34145689e+02 1.96800000e+03\n",
            "  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 5.00000000e+00 4.00000000e+00 2.10000000e+01\n",
            "  1.00000000e+00 5.92943287e+00 1.00000000e+00 2.28800000e+03\n",
            "  2.33161190e+02 1.96700000e+03 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 5.00000000e+00\n",
            "  4.00000000e+00 2.10000000e+01 1.00000000e+00 5.92943287e+00\n",
            "  1.00000000e+00 2.28800000e+03 2.33161190e+02 1.96700000e+03\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46STsbti3AmT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean_train = x_train.mean(axis=0)\n",
        "std_train = x_train.std(axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5vvbrrK3PES",
        "colab_type": "code",
        "outputId": "193da28f-2d44-408a-f230-4ae7f98d8fcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "print(mean_train)\n",
        "print(std_train)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2.90871477e-01 1.61432635e+00 2.60858547e+00 4.18908715e+00\n",
            " 7.43390526e-01 5.00800621e+01 2.96331523e+01 8.11968929e+02\n",
            " 2.32963538e+01 9.76994760e+01 2.02296354e+00 1.73095790e+03\n",
            " 6.53773667e+01 8.28714959e+02 2.90871477e-01 1.61432635e+00\n",
            " 2.60858547e+00 4.18908715e+00 7.43390526e-01 5.00800621e+01\n",
            " 2.96331523e+01 8.11968929e+02 2.32963538e+01 9.76994760e+01\n",
            " 2.02296354e+00 1.73095790e+03 6.53773667e+01 8.28714959e+02\n",
            " 2.90871477e-01 1.47142488e-01 3.46521852e-02 4.71373157e-01\n",
            " 1.51538660e-02 4.08068270e-02]\n",
            "[4.54164354e-01 1.62736223e+00 6.93052365e+00 2.56616350e+01\n",
            " 1.13236779e+00 5.86250473e+02 6.19852440e+02 1.15236538e+03\n",
            " 2.84287684e+01 2.37802272e+02 7.67653474e+00 2.11471132e+03\n",
            " 6.96820390e+01 1.34456679e+03 4.54164354e-01 1.62736223e+00\n",
            " 6.93052365e+00 2.56616350e+01 1.13236779e+00 5.86250473e+02\n",
            " 6.19852440e+02 1.15236538e+03 2.84287684e+01 2.37802272e+02\n",
            " 7.67653474e+00 2.11471132e+03 6.96820390e+01 1.34456679e+03\n",
            " 4.54164354e-01 3.54247902e-01 1.82897270e-01 4.99179831e-01\n",
            " 1.22164751e-01 1.97842437e-01]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2IoFA5h425Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = (x_train - mean_train) / std_train\n",
        "x_val = (x_val - mean_train) / std_train\n",
        "x_test = (x_test - mean_train) / std_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1AoKNoU5V1w",
        "colab_type": "code",
        "outputId": "33029cca-6bd8-4641-953b-cf4375060ef5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(x_train[:3])\n",
        "print(x_val[:3])\n",
        "print(x_test[:3])"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-6.40454219e-01  3.30944983e+00  6.33633872e-01  1.09537559e-01\n",
            "   1.67632253e+00 -7.00725440e-02 -3.32872003e-02 -3.34564335e-01\n",
            "   2.69809951e+00 -4.10842935e-01 -1.33258504e-01  1.62671948e+00\n",
            "  -5.04435529e-01  4.78964178e-01 -6.40454219e-01  3.30944983e+00\n",
            "   6.33633872e-01  1.09537559e-01  1.67632253e+00 -7.00725440e-02\n",
            "  -3.32872003e-02 -3.34564335e-01  2.69809951e+00 -4.10842935e-01\n",
            "  -1.33258504e-01  1.62671948e+00 -5.04435529e-01  4.78964178e-01\n",
            "  -6.40454219e-01 -4.15365869e-01 -1.89462561e-01  1.05899079e+00\n",
            "  -1.24044505e-01 -2.06259221e-01]\n",
            " [-6.40454219e-01  3.30944983e+00  6.33633872e-01  1.09537559e-01\n",
            "   1.77204746e+00 -7.17782996e-02 -3.49004875e-02 -3.34564335e-01\n",
            "   2.69809951e+00 -3.84288594e-01 -2.99139370e-03  1.62624660e+00\n",
            "  -5.18786429e-01  4.78220444e-01 -6.40454219e-01  3.30944983e+00\n",
            "   6.33633872e-01  1.09537559e-01  1.77204746e+00 -7.17782996e-02\n",
            "  -3.49004875e-02 -3.34564335e-01  2.69809951e+00 -3.84288594e-01\n",
            "  -2.99139370e-03  1.62624660e+00 -5.18786429e-01  4.78220444e-01\n",
            "  -6.40454219e-01 -4.15365869e-01 -1.89462561e-01  1.05899079e+00\n",
            "  -1.24044505e-01 -2.06259221e-01]\n",
            " [-6.40454219e-01 -3.77498225e-01 -2.32101577e-01 -8.53058331e-02\n",
            "   1.44113090e-01 -5.98380107e-02  2.31778513e-02 -6.21303747e-01\n",
            "  -6.78761509e-01 -4.10697311e-01 -2.99139370e-03 -5.21091408e-01\n",
            "  -7.25423575e-01 -3.08437605e-01 -6.40454219e-01 -3.77498225e-01\n",
            "  -2.32101577e-01 -8.53058331e-02  1.44113090e-01 -5.98380107e-02\n",
            "   2.31778513e-02 -6.21303747e-01 -6.78761509e-01 -4.10697311e-01\n",
            "  -2.99139370e-03 -5.21091408e-01 -7.25423575e-01 -3.08437605e-01\n",
            "  -6.40454219e-01 -4.15365869e-01 -1.89462561e-01  1.05899079e+00\n",
            "  -1.24044505e-01 -2.06259221e-01]]\n",
            "[[-0.64045422  3.92394118  0.77792311  0.14850624  1.75629188 -0.06666103\n",
            "  -0.03006063 -0.566959   -0.64358587 -0.4108425  -0.1332585   1.15478745\n",
            "   2.58593708 -0.00713238 -0.64045422  3.92394118  0.77792311  0.14850624\n",
            "   1.75629188 -0.06666103 -0.03006063 -0.566959   -0.64358587 -0.4108425\n",
            "  -0.1332585   1.15478745  2.58593708 -0.00713238 -0.64045422 -0.41536587\n",
            "  -0.18946256  1.05899079 -0.1240445  -0.20625922]\n",
            " [-0.64045422  3.92394118  0.77792311  0.14850624  1.75629188 -0.06666103\n",
            "  -0.03006063 -0.566959   -0.64358587 -0.39277773  0.38780994  1.15431457\n",
            "   2.57158618 -0.00787611 -0.64045422  3.92394118  0.77792311  0.14850624\n",
            "   1.75629188 -0.06666103 -0.03006063 -0.566959   -0.64358587 -0.39277773\n",
            "   0.38780994  1.15431457  2.57158618 -0.00787611 -0.64045422 -0.41536587\n",
            "  -0.18946256  1.05899079 -0.1240445  -0.20625922]\n",
            " [-0.64045422 -0.37749822 -0.23210158 -0.12427451 -0.65649211 -0.08371859\n",
            "  -0.0461935  -0.61175816 -0.78428842 -0.40861663 -0.1332585   1.1538417\n",
            "   2.55725426 -0.55386982 -0.64045422 -0.37749822 -0.23210158 -0.12427451\n",
            "  -0.65649211 -0.08371859 -0.0461935  -0.61175816 -0.78428842 -0.40861663\n",
            "  -0.1332585   1.1538417   2.55725426 -0.55386982 -0.64045422 -0.41536587\n",
            "  -0.18946256  1.05899079 -0.1240445  -0.20625922]]\n",
            "[[-0.64045422 -0.37749822 -0.23210158 -0.12427451 -0.65649211 -0.08371859\n",
            "  -0.0461935  -0.5041534  -0.74911278 -0.41080896 -0.1332585   0.26435859\n",
            "   2.43622349  0.84806872 -0.64045422 -0.37749822 -0.23210158 -0.12427451\n",
            "  -0.65649211 -0.08371859 -0.0461935  -0.5041534  -0.74911278 -0.41080896\n",
            "  -0.1332585   0.26435859  2.43622349  0.84806872 -0.64045422 -0.41536587\n",
            "  -0.18946256  1.05899079 -0.1240445  -0.20625922]\n",
            " [-0.64045422 -0.37749822  0.05647691 -0.00736848  1.04275646 -0.06836679\n",
            "  -0.04135364 -0.63562212 -0.74911278 -0.40765158  0.25754283  0.26388571\n",
            "   2.42197738  0.84732499 -0.64045422 -0.37749822  0.05647691 -0.00736848\n",
            "   1.04275646 -0.06836679 -0.04135364 -0.63562212 -0.74911278 -0.40765158\n",
            "   0.25754283  0.26388571  2.42197738  0.84732499 -0.64045422  2.40751606\n",
            "  -0.18946256 -0.94429528 -0.1240445  -0.20625922]\n",
            " [-0.64045422 -0.37749822 -0.23210158 -0.12427451 -0.65649211 -0.07689557\n",
            "  -0.04135364 -0.68638727 -0.78428842 -0.38590903 -0.1332585   0.26341283\n",
            "   2.40784893  0.84658126 -0.64045422 -0.37749822 -0.23210158 -0.12427451\n",
            "  -0.65649211 -0.07689557 -0.04135364 -0.68638727 -0.78428842 -0.38590903\n",
            "  -0.1332585   0.26341283  2.40784893  0.84658126 -0.64045422 -0.41536587\n",
            "   5.27808762 -0.94429528 -0.1240445  -0.20625922]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNcuSWBQp2Ua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_sampler(y):\n",
        "  _, counts = np.unique(y, return_counts=True)\n",
        "  n_samples = len(y)\n",
        "  class_weights = n_samples / counts\n",
        "  weights = class_weights[y]\n",
        "  fade = reversed(range(n_samples))\n",
        "  fading_factor = 1 #0.999999\n",
        "  fade = [fading_factor**x for x in (fade)]\n",
        "  weights = weights * fade\n",
        "  return data.WeightedRandomSampler(weights=weights, num_samples=n_samples, replacement=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fh6O6Nz6uC51",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sampler = create_sampler(y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iP6L6GN22_bR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, y_train = torch.from_numpy(x_train), torch.from_numpy(y_train)\n",
        "x_val, y_val = torch.from_numpy(x_val), torch.from_numpy(y_val)\n",
        "x_test, y_test = torch.from_numpy(x_test), torch.from_numpy(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XnnrzRjL3IP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = data.TensorDataset(x_train, y_train)\n",
        "val_dataset = data.TensorDataset(x_val, y_val)\n",
        "test_dataset = data.TensorDataset(x_test, y_test)\n",
        "\n",
        "train_dataloader = data.DataLoader(train_dataset, batch_size=512, sampler=sampler)\n",
        "val_dataloader = data.DataLoader(val_dataset, batch_size=32)\n",
        "test_dataloader = data.DataLoader(test_dataset, batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHoY9-JcORo-",
        "colab_type": "code",
        "outputId": "92990f28-ef67-445a-86d4-64b289e0554d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "source": [
        "inputs, targets = next(iter(train_dataloader))\n",
        "print(inputs[:3])\n",
        "print(targets[:3])\n",
        "\n",
        "inputs, targets = next(iter(val_dataloader))\n",
        "print(inputs[:3])\n",
        "print(targets[:3])\n",
        "\n",
        "inputs, targets = next(iter(test_dataloader))\n",
        "print(inputs[:3])\n",
        "print(targets[:3])"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 1.5614, -0.3775,  0.2008,  0.1095,  1.5703, -0.0360,  0.0054,  0.5914,\n",
            "          2.1353, -0.0578,  0.5181, -0.5745, -0.7845, -0.3813,  1.5614, -0.3775,\n",
            "          0.2008,  0.1095,  1.5703, -0.0360,  0.0054,  0.5914,  2.1353, -0.0578,\n",
            "          0.5181, -0.5745, -0.7845, -0.3813,  1.5614, -0.4154, -0.1895, -0.9443,\n",
            "         -0.1240, -0.2063],\n",
            "        [-0.6405, -0.3775,  0.9222,  2.2918,  4.4708,  0.2216,  0.2426, -0.6804,\n",
            "         -0.2918,  1.2221,  1.6905, -0.6828,  0.3564, -0.4854, -0.6405, -0.3775,\n",
            "          0.9222,  2.2918,  4.4708,  0.2216,  0.2426, -0.6804, -0.2918,  1.2221,\n",
            "          1.6905, -0.6828,  0.3564, -0.4854, -0.6405, -0.4154, -0.1895,  1.0590,\n",
            "         -0.1240, -0.2063],\n",
            "        [ 1.5614,  0.2370,  0.0565, -0.0463,  0.3892, -0.0462, -0.0430,  0.6694,\n",
            "          0.6227, -0.3754,  0.1273, -0.2199,  1.2378, -0.1798,  1.5614,  0.2370,\n",
            "          0.0565, -0.0463,  0.3892, -0.0462, -0.0430,  0.6694,  0.6227, -0.3754,\n",
            "          0.1273, -0.2199,  1.2378, -0.1798,  1.5614, -0.4154, -0.1895, -0.9443,\n",
            "         -0.1240, -0.2063]], dtype=torch.float64)\n",
            "tensor([1, 0, 1], dtype=torch.int8)\n",
            "tensor([[-0.6405,  3.9239,  0.7779,  0.1485,  1.7563, -0.0667, -0.0301, -0.5670,\n",
            "         -0.6436, -0.4108, -0.1333,  1.1548,  2.5859, -0.0071, -0.6405,  3.9239,\n",
            "          0.7779,  0.1485,  1.7563, -0.0667, -0.0301, -0.5670, -0.6436, -0.4108,\n",
            "         -0.1333,  1.1548,  2.5859, -0.0071, -0.6405, -0.4154, -0.1895,  1.0590,\n",
            "         -0.1240, -0.2063],\n",
            "        [-0.6405,  3.9239,  0.7779,  0.1485,  1.7563, -0.0667, -0.0301, -0.5670,\n",
            "         -0.6436, -0.3928,  0.3878,  1.1543,  2.5716, -0.0079, -0.6405,  3.9239,\n",
            "          0.7779,  0.1485,  1.7563, -0.0667, -0.0301, -0.5670, -0.6436, -0.3928,\n",
            "          0.3878,  1.1543,  2.5716, -0.0079, -0.6405, -0.4154, -0.1895,  1.0590,\n",
            "         -0.1240, -0.2063],\n",
            "        [-0.6405, -0.3775, -0.2321, -0.1243, -0.6565, -0.0837, -0.0462, -0.6118,\n",
            "         -0.7843, -0.4086, -0.1333,  1.1538,  2.5573, -0.5539, -0.6405, -0.3775,\n",
            "         -0.2321, -0.1243, -0.6565, -0.0837, -0.0462, -0.6118, -0.7843, -0.4086,\n",
            "         -0.1333,  1.1538,  2.5573, -0.5539, -0.6405, -0.4154, -0.1895,  1.0590,\n",
            "         -0.1240, -0.2063]], dtype=torch.float64)\n",
            "tensor([0, 0, 0], dtype=torch.int8)\n",
            "tensor([[-0.6405, -0.3775, -0.2321, -0.1243, -0.6565, -0.0837, -0.0462, -0.5042,\n",
            "         -0.7491, -0.4108, -0.1333,  0.2644,  2.4362,  0.8481, -0.6405, -0.3775,\n",
            "         -0.2321, -0.1243, -0.6565, -0.0837, -0.0462, -0.5042, -0.7491, -0.4108,\n",
            "         -0.1333,  0.2644,  2.4362,  0.8481, -0.6405, -0.4154, -0.1895,  1.0590,\n",
            "         -0.1240, -0.2063],\n",
            "        [-0.6405, -0.3775,  0.0565, -0.0074,  1.0428, -0.0684, -0.0414, -0.6356,\n",
            "         -0.7491, -0.4077,  0.2575,  0.2639,  2.4220,  0.8473, -0.6405, -0.3775,\n",
            "          0.0565, -0.0074,  1.0428, -0.0684, -0.0414, -0.6356, -0.7491, -0.4077,\n",
            "          0.2575,  0.2639,  2.4220,  0.8473, -0.6405,  2.4075, -0.1895, -0.9443,\n",
            "         -0.1240, -0.2063],\n",
            "        [-0.6405, -0.3775, -0.2321, -0.1243, -0.6565, -0.0769, -0.0414, -0.6864,\n",
            "         -0.7843, -0.3859, -0.1333,  0.2634,  2.4078,  0.8466, -0.6405, -0.3775,\n",
            "         -0.2321, -0.1243, -0.6565, -0.0769, -0.0414, -0.6864, -0.7843, -0.3859,\n",
            "         -0.1333,  0.2634,  2.4078,  0.8466, -0.6405, -0.4154,  5.2781, -0.9443,\n",
            "         -0.1240, -0.2063]], dtype=torch.float64)\n",
            "tensor([1, 1, 0], dtype=torch.int8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-GctbL8E1oA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Classifier(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, drop_prob, epoch=None, val_gmean=None):\n",
        "    super(Classifier, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.drop_prob = drop_prob\n",
        "    self.epoch = epoch\n",
        "    self.val_gmean = val_gmean\n",
        "    self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "    self.fcout = nn.Linear(hidden_size, 1)\n",
        "    self.dropout = nn.Dropout(drop_prob)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = torch.relu(self.fc1(x))\n",
        "    x = self.dropout(x)\n",
        "    x = torch.sigmoid(self.fcout(x))\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inpSePkGLTIs",
        "colab_type": "code",
        "outputId": "b3c56983-b430-43a3-9646-6f7621732dfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "classifier = Classifier(input_size=x.shape[1], hidden_size=x.shape[1], drop_prob=0.5)\n",
        "classifier"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Classifier(\n",
              "  (fc1): Linear(in_features=34, out_features=34, bias=True)\n",
              "  (fcout): Linear(in_features=34, out_features=1, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsT5eIrNoXAh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = 'classifier.cpt'\n",
        "def save_classifier(classifier):\n",
        "  checkpoint = {\n",
        "      'input_size': classifier.input_size,\n",
        "      'hidden_size': classifier.hidden_size,\n",
        "      'drop_prob': classifier.drop_prob,\n",
        "      'val_gmean': classifier.val_gmean,\n",
        "      'epoch': classifier.epoch,\n",
        "      'state_dict': classifier.state_dict()\n",
        "  }\n",
        "  with open(filename, 'wb') as f:\n",
        "    torch.save(checkpoint, f)\n",
        "\n",
        "def load_classifier():\n",
        "  with open(filename, 'rb') as f:\n",
        "    checkpoint = torch.load(f)\n",
        "  classifier = Classifier(checkpoint['input_size'], checkpoint['hidden_size'], checkpoint['drop_prob'], checkpoint['epoch'], checkpoint['val_gmean'])\n",
        "  classifier.load_state_dict(checkpoint['state_dict'])\n",
        "  return classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LX_eofKUq0ok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_classifier(classifier)\n",
        "classifier = load_classifier()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PR8s5ZA5rX-V",
        "colab_type": "code",
        "outputId": "818ffc81-37d6-4860-ecb2-68b953066c15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  classifier = classifier.cuda()\n",
        "  inputs = inputs.cuda()\n",
        "\n",
        "classifier(inputs[:3].float())"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.5375],\n",
              "        [0.6136],\n",
              "        [0.5338]], device='cuda:0', grad_fn=<SigmoidBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arYOr5lMLYy7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(params=classifier.parameters(), lr=0.003)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXw0R-Tkucgw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_loss(classifier, dataloader):\n",
        "  with torch.no_grad():\n",
        "    classifier.eval()\n",
        "    loss = 0\n",
        "    for inputs, targets in dataloader:\n",
        "      if torch.cuda.is_available():\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "      outputs = classifier(inputs.float())\n",
        "      batch_loss = criterion(outputs.squeeze(), targets.float())\n",
        "      loss += batch_loss.item()\n",
        "\n",
        "    return loss / len(dataloader)\n",
        "\n",
        "def calc_recalls(targets, predictions):\n",
        "  n_classes = 2\n",
        "  confusion_matrix, _, _ = np.histogram2d(targets.detach().cpu().numpy(), predictions.squeeze().detach().cpu().numpy(), bins=n_classes)\n",
        "  recalls = np.diag(confusion_matrix) / ((np.sum(confusion_matrix, axis=1)) + 1e-12)\n",
        "  return recalls\n",
        "\n",
        "def calc_gmean(classifier, dataloader):\n",
        "  with torch.no_grad():\n",
        "    classifier.eval()\n",
        "    recalls = np.zeros((2))    \n",
        "    for inputs, targets in dataloader:\n",
        "      if torch.cuda.is_available():\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "      \n",
        "      outputs = classifier(inputs.float())\n",
        "      predictions = torch.round(outputs).int()\n",
        "      recalls += calc_recalls(targets, predictions)\n",
        "    recalls = recalls / len(dataloader)\n",
        "    return mstats.gmean(recalls)    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFy03p96Lza1",
        "colab_type": "code",
        "outputId": "9fea2dfc-d558-42e1-ad76-d886833d7739",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_loss = 0\n",
        "epochs = 200\n",
        "for epoch in range(epochs):\n",
        "  classifier.train()\n",
        "  for inputs, targets in train_dataloader:\n",
        "    if torch.cuda.is_available():\n",
        "      inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "    outputs = classifier(inputs.float())\n",
        "    loss = criterion(outputs.squeeze(), targets.float())\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  train_loss = train_loss / len(train_dataloader)\n",
        "  val_loss = calc_loss(classifier, val_dataloader)\n",
        "  val_gmean = calc_gmean(classifier, val_dataloader)\n",
        "  print('Epoch: {}, Train loss: {}, Val loss: {}, Val g-mean: {}'.format(epoch, train_loss, val_loss, val_gmean))\n",
        "\n",
        "  if classifier.val_gmean is None or val_gmean > classifier.val_gmean:\n",
        "    classifier.epoch = epoch\n",
        "    classifier.val_gmean = val_gmean\n",
        "    save_classifier(classifier)\n",
        "\n",
        "classifier.epoch = epoch\n",
        "classifier.val_gmean = val_gmean"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Train loss: 0.5020032493691695, Val loss: 0.4594741722470836, Val g-mean: 0.7482994546651928\n",
            "Epoch: 1, Train loss: 0.5183972825303962, Val loss: 0.4690330032455294, Val g-mean: 0.7476285293160934\n",
            "Epoch: 2, Train loss: 0.5174984676003855, Val loss: 0.4513895974347466, Val g-mean: 0.7454338174499954\n",
            "Epoch: 3, Train loss: 0.520419270695198, Val loss: 0.4668129488433662, Val g-mean: 0.7475068763372864\n",
            "Epoch: 4, Train loss: 0.5163343086289981, Val loss: 0.4630037667719941, Val g-mean: 0.7595038189193808\n",
            "Epoch: 5, Train loss: 0.5200797581736655, Val loss: 0.5095403486568677, Val g-mean: 0.7400057963637188\n",
            "Epoch: 6, Train loss: 0.5167559894136807, Val loss: 0.45896482036301967, Val g-mean: 0.7512806666757215\n",
            "Epoch: 7, Train loss: 0.5176199429187133, Val loss: 0.5084556813694929, Val g-mean: 0.7427583984555794\n",
            "Epoch: 8, Train loss: 0.5120209245121086, Val loss: 0.5047286121468795, Val g-mean: 0.750406097204685\n",
            "Epoch: 9, Train loss: 0.5140449901114316, Val loss: 0.5043431967496872, Val g-mean: 0.7452600130213314\n",
            "Epoch: 10, Train loss: 0.5102899636607693, Val loss: 0.451140394140231, Val g-mean: 0.7583119299055102\n",
            "Epoch: 11, Train loss: 0.5149879760537788, Val loss: 0.47878816841464295, Val g-mean: 0.7525709997004418\n",
            "Epoch: 12, Train loss: 0.5135196180902925, Val loss: 0.4696725697109574, Val g-mean: 0.7557996290059287\n",
            "Epoch: 13, Train loss: 0.517667276377644, Val loss: 0.47729584300204325, Val g-mean: 0.7510253243785754\n",
            "Epoch: 14, Train loss: 0.519671730637424, Val loss: 0.5120367513675439, Val g-mean: 0.7479562009649965\n",
            "Epoch: 15, Train loss: 0.5146661532144764, Val loss: 0.46937888272498784, Val g-mean: 0.7561488857497267\n",
            "Epoch: 16, Train loss: 0.5219574542741531, Val loss: 0.491120310794366, Val g-mean: 0.7511940638290469\n",
            "Epoch: 17, Train loss: 0.5168860201038303, Val loss: 0.4721969234708108, Val g-mean: 0.7495903990144264\n",
            "Epoch: 18, Train loss: 0.520301032861067, Val loss: 0.46980089772688716, Val g-mean: 0.7550753822970657\n",
            "Epoch: 19, Train loss: 0.5193909850768874, Val loss: 0.4594487610616182, Val g-mean: 0.751630458038605\n",
            "Epoch: 20, Train loss: 0.5123498835768947, Val loss: 0.4561036121296255, Val g-mean: 0.7519159988625062\n",
            "Epoch: 21, Train loss: 0.5144999790573503, Val loss: 0.487309156083747, Val g-mean: 0.7474696864980682\n",
            "Epoch: 22, Train loss: 0.5207997078654651, Val loss: 0.46014167897795377, Val g-mean: 0.7567368427253455\n",
            "Epoch: 23, Train loss: 0.5228356996517366, Val loss: 0.48896681576182965, Val g-mean: 0.7485642114817197\n",
            "Epoch: 24, Train loss: 0.5161193420472401, Val loss: 0.4725901827608284, Val g-mean: 0.7511764896434913\n",
            "Epoch: 25, Train loss: 0.5176301868076457, Val loss: 0.5126791561120435, Val g-mean: 0.7370758299630676\n",
            "Epoch: 26, Train loss: 0.52191812655203, Val loss: 0.5070587386818308, Val g-mean: 0.747360064120639\n",
            "Epoch: 27, Train loss: 0.5149893786459828, Val loss: 0.47491202660297094, Val g-mean: 0.7510325125314901\n",
            "Epoch: 28, Train loss: 0.513636942824642, Val loss: 0.47882266381853505, Val g-mean: 0.7517914727553606\n",
            "Epoch: 29, Train loss: 0.5162372509670079, Val loss: 0.49863761800684425, Val g-mean: 0.7486674528438657\n",
            "Epoch: 30, Train loss: 0.521748173968917, Val loss: 0.4597912924854379, Val g-mean: 0.7516290042689919\n",
            "Epoch: 31, Train loss: 0.5165726782652612, Val loss: 0.4735277565686326, Val g-mean: 0.7525534056390912\n",
            "Epoch: 32, Train loss: 0.5176524683095198, Val loss: 0.4580226062159789, Val g-mean: 0.7589859313275709\n",
            "Epoch: 33, Train loss: 0.5210964519315048, Val loss: 0.49181962287739706, Val g-mean: 0.7514438261824836\n",
            "Epoch: 34, Train loss: 0.5181934648923003, Val loss: 0.47999336139151927, Val g-mean: 0.753479236000118\n",
            "Epoch: 35, Train loss: 0.5209455642108644, Val loss: 0.4626920266371024, Val g-mean: 0.7603174567490757\n",
            "Epoch: 36, Train loss: 0.5190105850053579, Val loss: 0.4689694811639033, Val g-mean: 0.7439711316124095\n",
            "Epoch: 37, Train loss: 0.5216242226972725, Val loss: 0.48797757178545, Val g-mean: 0.7531050116740662\n",
            "Epoch: 38, Train loss: 0.5172970487177753, Val loss: 0.4600634270984876, Val g-mean: 0.75821799562736\n",
            "Epoch: 39, Train loss: 0.5176297015618347, Val loss: 0.4776926342593996, Val g-mean: 0.7538368640794176\n",
            "Epoch: 40, Train loss: 0.512607545148571, Val loss: 0.4759757265840706, Val g-mean: 0.7543151290659986\n",
            "Epoch: 41, Train loss: 0.5136623594450124, Val loss: 0.4576558747181767, Val g-mean: 0.7506585825211767\n",
            "Epoch: 42, Train loss: 0.5115900782464031, Val loss: 0.4905359556800441, Val g-mean: 0.7456367643718828\n",
            "Epoch: 43, Train loss: 0.5191799437469508, Val loss: 0.4771921173914483, Val g-mean: 0.751206152168182\n",
            "Epoch: 44, Train loss: 0.5242909107696232, Val loss: 0.46187420522696093, Val g-mean: 0.7472133134545018\n",
            "Epoch: 45, Train loss: 0.5169074791933653, Val loss: 0.47168247558568654, Val g-mean: 0.7482215934213249\n",
            "Epoch: 46, Train loss: 0.5187761162934218, Val loss: 0.4498497402589572, Val g-mean: 0.7512722556864251\n",
            "Epoch: 47, Train loss: 0.5185137484555117, Val loss: 0.4801265538523072, Val g-mean: 0.7505214680565981\n",
            "Epoch: 48, Train loss: 0.5204939805470137, Val loss: 0.45624439555563423, Val g-mean: 0.7481082815050875\n",
            "Epoch: 49, Train loss: 0.51668656882226, Val loss: 0.4754830230223505, Val g-mean: 0.755051279185985\n",
            "Epoch: 50, Train loss: 0.5144484281060412, Val loss: 0.4852804216115098, Val g-mean: 0.7469359762711244\n",
            "Epoch: 51, Train loss: 0.5140700417116999, Val loss: 0.47771504090020533, Val g-mean: 0.749434398573498\n",
            "Epoch: 52, Train loss: 0.5181985021091232, Val loss: 0.4848670130105395, Val g-mean: 0.7435189312307201\n",
            "Epoch: 53, Train loss: 0.5142853686695996, Val loss: 0.47917444592243746, Val g-mean: 0.7471694003729208\n",
            "Epoch: 54, Train loss: 0.5154289457606791, Val loss: 0.48118724497525317, Val g-mean: 0.7489918539686473\n",
            "Epoch: 55, Train loss: 0.5183701558421775, Val loss: 0.4809674730426387, Val g-mean: 0.7568289048777654\n",
            "Epoch: 56, Train loss: 0.520541737852156, Val loss: 0.5010583400726318, Val g-mean: 0.7463767572294966\n",
            "Epoch: 57, Train loss: 0.5203270405106811, Val loss: 0.4836425367546709, Val g-mean: 0.747817562864634\n",
            "Epoch: 58, Train loss: 0.5130526057188266, Val loss: 0.44868440043769386, Val g-mean: 0.7612810349536924\n",
            "Epoch: 59, Train loss: 0.5170108948655937, Val loss: 0.49794915671411316, Val g-mean: 0.7488772704572173\n",
            "Epoch: 60, Train loss: 0.5177835609685643, Val loss: 0.5177371368596428, Val g-mean: 0.7413093932584643\n",
            "Epoch: 61, Train loss: 0.5175172665301416, Val loss: 0.46935801953077316, Val g-mean: 0.7557906291192442\n",
            "Epoch: 62, Train loss: 0.5209621731612334, Val loss: 0.48793765745664897, Val g-mean: 0.7460053794379222\n",
            "Epoch: 63, Train loss: 0.5182842673548828, Val loss: 0.49937120119207784, Val g-mean: 0.7469714820626606\n",
            "Epoch: 64, Train loss: 0.5140112843996829, Val loss: 0.4807020525791143, Val g-mean: 0.7554176797466537\n",
            "Epoch: 65, Train loss: 0.5199976678660212, Val loss: 0.4703697868083653, Val g-mean: 0.750246258861255\n",
            "Epoch: 66, Train loss: 0.5212982943078162, Val loss: 0.4683146214014606, Val g-mean: 0.7588753809444508\n",
            "Epoch: 67, Train loss: 0.5186431973922155, Val loss: 0.46236351563742284, Val g-mean: 0.745124114776706\n",
            "Epoch: 68, Train loss: 0.5198524475875338, Val loss: 0.4641670557229142, Val g-mean: 0.749715152895523\n",
            "Epoch: 69, Train loss: 0.5212649966561554, Val loss: 0.45058606192469597, Val g-mean: 0.7478483044150362\n",
            "Epoch: 70, Train loss: 0.5115948977315734, Val loss: 0.4688607366069367, Val g-mean: 0.7480594051981158\n",
            "Epoch: 71, Train loss: 0.5192334049685072, Val loss: 0.5000851907228169, Val g-mean: 0.7421590287395824\n",
            "Epoch: 72, Train loss: 0.5193725008550005, Val loss: 0.45166902930328723, Val g-mean: 0.7493905855433896\n",
            "Epoch: 73, Train loss: 0.5243213351326624, Val loss: 0.4708253792242, Val g-mean: 0.7505579089926868\n",
            "Epoch: 74, Train loss: 0.5199819243157052, Val loss: 0.49432883372432307, Val g-mean: 0.7523749943498897\n",
            "Epoch: 75, Train loss: 0.5170195323234172, Val loss: 0.49993135623241725, Val g-mean: 0.7427524790102796\n",
            "Epoch: 76, Train loss: 0.5159947548571526, Val loss: 0.48860608962805646, Val g-mean: 0.7493549517228896\n",
            "Epoch: 77, Train loss: 0.5156057735134199, Val loss: 0.4614264827809836, Val g-mean: 0.7554667803814036\n",
            "Epoch: 78, Train loss: 0.5201120728459511, Val loss: 0.49945308503351715, Val g-mean: 0.7472999819884775\n",
            "Epoch: 79, Train loss: 0.5146472392746402, Val loss: 0.4675685349655779, Val g-mean: 0.7449312835108391\n",
            "Epoch: 80, Train loss: 0.5198683454756899, Val loss: 0.4547743246351418, Val g-mean: 0.7517109643763843\n",
            "Epoch: 81, Train loss: 0.518241589827927, Val loss: 0.4396226539423591, Val g-mean: 0.7584896017367747\n",
            "Epoch: 82, Train loss: 0.5173892223407195, Val loss: 0.5220841494830031, Val g-mean: 0.7375212350454324\n",
            "Epoch: 83, Train loss: 0.5134623462264196, Val loss: 0.4821155255562381, Val g-mean: 0.7477176787225007\n",
            "Epoch: 84, Train loss: 0.5169317937062091, Val loss: 0.47742735672938197, Val g-mean: 0.7551515685162865\n",
            "Epoch: 85, Train loss: 0.516882271139679, Val loss: 0.4589509267948176, Val g-mean: 0.7624766375712895\n",
            "Epoch: 86, Train loss: 0.5160183585152516, Val loss: 0.4688663710104792, Val g-mean: 0.7564665246739125\n",
            "Epoch: 87, Train loss: 0.5107999098313103, Val loss: 0.4603498717279811, Val g-mean: 0.748969613115496\n",
            "Epoch: 88, Train loss: 0.5183022969961486, Val loss: 0.46559592905013186, Val g-mean: 0.7485228306391508\n",
            "Epoch: 89, Train loss: 0.5215929285319236, Val loss: 0.46461209694021627, Val g-mean: 0.749780129160839\n",
            "Epoch: 90, Train loss: 0.5189270702256226, Val loss: 0.4753787662637861, Val g-mean: 0.7489572374661098\n",
            "Epoch: 91, Train loss: 0.5185872207274914, Val loss: 0.4551474361827499, Val g-mean: 0.7512527507774394\n",
            "Epoch: 92, Train loss: 0.5242905166854746, Val loss: 0.4720233263153779, Val g-mean: 0.7495706481005747\n",
            "Epoch: 93, Train loss: 0.5185268509469281, Val loss: 0.463102678327184, Val g-mean: 0.7526950855794112\n",
            "Epoch: 94, Train loss: 0.5156362887748549, Val loss: 0.47104442296059507, Val g-mean: 0.7526039206793622\n",
            "Epoch: 95, Train loss: 0.5244701887341144, Val loss: 0.49177907662172066, Val g-mean: 0.7543977163381558\n",
            "Epoch: 96, Train loss: 0.5188132871796473, Val loss: 0.48423522789227336, Val g-mean: 0.7484131083877323\n",
            "Epoch: 97, Train loss: 0.5208561594277353, Val loss: 0.501277042062659, Val g-mean: 0.7430099164225353\n",
            "Epoch: 98, Train loss: 0.5160066701732962, Val loss: 0.4678019393039377, Val g-mean: 0.7569133248309423\n",
            "Epoch: 99, Train loss: 0.5066181999491048, Val loss: 0.4589360983748185, Val g-mean: 0.7532673130524058\n",
            "Epoch: 100, Train loss: 0.5149573589156762, Val loss: 0.48187598978218277, Val g-mean: 0.7515585650548365\n",
            "Epoch: 101, Train loss: 0.5203887629529643, Val loss: 0.4658532191656138, Val g-mean: 0.7635102196141329\n",
            "Epoch: 102, Train loss: 0.5190437038949206, Val loss: 0.4515071472055034, Val g-mean: 0.7494052827781215\n",
            "Epoch: 103, Train loss: 0.515377767515316, Val loss: 0.4759771849371885, Val g-mean: 0.7521430197047403\n",
            "Epoch: 104, Train loss: 0.5157813604367443, Val loss: 0.48591311119104685, Val g-mean: 0.7444349724220801\n",
            "Epoch: 105, Train loss: 0.5176680241647851, Val loss: 0.44827277958393097, Val g-mean: 0.7553863514005575\n",
            "Epoch: 106, Train loss: 0.5131414889625904, Val loss: 0.4528095686906262, Val g-mean: 0.753217238702868\n",
            "Epoch: 107, Train loss: 0.5139922846287285, Val loss: 0.4531826392600411, Val g-mean: 0.7515122903630995\n",
            "Epoch: 108, Train loss: 0.5200031040241054, Val loss: 0.4748047990234275, Val g-mean: 0.7506652078500022\n",
            "Epoch: 109, Train loss: 0.5143095386606814, Val loss: 0.497989938251282, Val g-mean: 0.7485377810982307\n",
            "Epoch: 110, Train loss: 0.5086299913823144, Val loss: 0.48840848748621185, Val g-mean: 0.7509168512327494\n",
            "Epoch: 111, Train loss: 0.5142758311872407, Val loss: 0.4826701156010753, Val g-mean: 0.7446282925542196\n",
            "Epoch: 112, Train loss: 0.5175031003252579, Val loss: 0.4581216612928792, Val g-mean: 0.7548366837253356\n",
            "Epoch: 113, Train loss: 0.5118384318835273, Val loss: 0.45401684312443985, Val g-mean: 0.7499277108621738\n",
            "Epoch: 114, Train loss: 0.5150683238770007, Val loss: 0.47747365640182243, Val g-mean: 0.7482397169084475\n",
            "Epoch: 115, Train loss: 0.5123072580670721, Val loss: 0.4770311172071256, Val g-mean: 0.7476127500880956\n",
            "Epoch: 116, Train loss: 0.5061397803880023, Val loss: 0.4731099625167094, Val g-mean: 0.7462127808055863\n",
            "Epoch: 117, Train loss: 0.5204332853959769, Val loss: 0.4775197192242271, Val g-mean: 0.7491652524381284\n",
            "Epoch: 118, Train loss: 0.5143490043795077, Val loss: 0.464590118511727, Val g-mean: 0.7595851816590367\n",
            "Epoch: 119, Train loss: 0.5187982969313232, Val loss: 0.47700482567674235, Val g-mean: 0.7596650995739213\n",
            "Epoch: 120, Train loss: 0.5236850209738847, Val loss: 0.48039817319888817, Val g-mean: 0.7455018943726701\n",
            "Epoch: 121, Train loss: 0.5186011480859133, Val loss: 0.49228470674470853, Val g-mean: 0.7457369598918229\n",
            "Epoch: 122, Train loss: 0.5213140844973849, Val loss: 0.46952170427692563, Val g-mean: 0.7566883073429657\n",
            "Epoch: 123, Train loss: 0.5215210702849923, Val loss: 0.4645343901295411, Val g-mean: 0.7460798894874495\n",
            "Epoch: 124, Train loss: 0.5144468234475074, Val loss: 0.4938211670439494, Val g-mean: 0.7473692359591171\n",
            "Epoch: 125, Train loss: 0.5179048619419222, Val loss: 0.46888170822670583, Val g-mean: 0.7500813980133137\n",
            "Epoch: 126, Train loss: 0.5188035388310486, Val loss: 0.45892003943261345, Val g-mean: 0.7558438270443354\n",
            "Epoch: 127, Train loss: 0.5177799267714477, Val loss: 0.4485949779811658, Val g-mean: 0.7650928713654186\n",
            "Epoch: 128, Train loss: 0.5266365772358818, Val loss: 0.45373192194261047, Val g-mean: 0.759985432456235\n",
            "Epoch: 129, Train loss: 0.512614439364777, Val loss: 0.46674321964383125, Val g-mean: 0.7605805782433773\n",
            "Epoch: 130, Train loss: 0.5180277850658356, Val loss: 0.4676896959151092, Val g-mean: 0.7513748681103853\n",
            "Epoch: 131, Train loss: 0.5145807486522372, Val loss: 0.4784026357688402, Val g-mean: 0.7516573709664679\n",
            "Epoch: 132, Train loss: 0.5179656091012144, Val loss: 0.4615320367248435, Val g-mean: 0.7497298738977725\n",
            "Epoch: 133, Train loss: 0.5172749748425463, Val loss: 0.47041897514933034, Val g-mean: 0.7538724298793805\n",
            "Epoch: 134, Train loss: 0.5160227025564218, Val loss: 0.47954460939294413, Val g-mean: 0.7539065771066912\n",
            "Epoch: 135, Train loss: 0.5142953573742867, Val loss: 0.4711721106187293, Val g-mean: 0.7539434597676131\n",
            "Epoch: 136, Train loss: 0.5202353115120462, Val loss: 0.47932008457811254, Val g-mean: 0.749577974171427\n",
            "Epoch: 137, Train loss: 0.5114027100049585, Val loss: 0.4723701292746945, Val g-mean: 0.7522087585214763\n",
            "Epoch: 138, Train loss: 0.5127898395512269, Val loss: 0.46380237058589335, Val g-mean: 0.7546410761444202\n",
            "Epoch: 139, Train loss: 0.518986046942843, Val loss: 0.461251956459723, Val g-mean: 0.7482231054473718\n",
            "Epoch: 140, Train loss: 0.5242235166627266, Val loss: 0.4620595811621139, Val g-mean: 0.7615307301797559\n",
            "Epoch: 141, Train loss: 0.5192177416012949, Val loss: 0.48418710890569183, Val g-mean: 0.7470572824901092\n",
            "Epoch: 142, Train loss: 0.5140839143832688, Val loss: 0.45943465946536316, Val g-mean: 0.7481129981948677\n",
            "Epoch: 143, Train loss: 0.515530436248738, Val loss: 0.47120783576055575, Val g-mean: 0.7602598326461749\n",
            "Epoch: 144, Train loss: 0.5153308667376156, Val loss: 0.4765267360367273, Val g-mean: 0.7502251929321313\n",
            "Epoch: 145, Train loss: 0.5160523280700949, Val loss: 0.4718137920687073, Val g-mean: 0.756479484592105\n",
            "Epoch: 146, Train loss: 0.5138942785566337, Val loss: 0.49838736947429807, Val g-mean: 0.7420181528834273\n",
            "Epoch: 147, Train loss: 0.5155063637189313, Val loss: 0.46327642919985873, Val g-mean: 0.7556348378751405\n",
            "Epoch: 148, Train loss: 0.5174401976771842, Val loss: 0.4736399509404835, Val g-mean: 0.7496198217520113\n",
            "Epoch: 149, Train loss: 0.5157655735761127, Val loss: 0.4808159919553681, Val g-mean: 0.7492088441555655\n",
            "Epoch: 150, Train loss: 0.5169434326781259, Val loss: 0.4745604695850297, Val g-mean: 0.7531756372494012\n",
            "Epoch: 151, Train loss: 0.5164060429990642, Val loss: 0.46693220205212893, Val g-mean: 0.7428099546501534\n",
            "Epoch: 152, Train loss: 0.5242485517374789, Val loss: 0.4644126609752053, Val g-mean: 0.7509730806122129\n",
            "Epoch: 153, Train loss: 0.52091246088898, Val loss: 0.44485232841811684, Val g-mean: 0.7491183866176805\n",
            "Epoch: 154, Train loss: 0.5196985427853715, Val loss: 0.4672157952660008, Val g-mean: 0.7547302288977701\n",
            "Epoch: 155, Train loss: 0.5218039660190209, Val loss: 0.47499072904649536, Val g-mean: 0.7488094508494048\n",
            "Epoch: 156, Train loss: 0.5136323741353778, Val loss: 0.4749627003544255, Val g-mean: 0.7515163904850073\n",
            "Epoch: 157, Train loss: 0.5180817941073713, Val loss: 0.44834094533794805, Val g-mean: 0.7691076689455674\n",
            "Epoch: 158, Train loss: 0.518677478033315, Val loss: 0.4648287143362196, Val g-mean: 0.754879052891606\n",
            "Epoch: 159, Train loss: 0.5117100988301795, Val loss: 0.45735774640189975, Val g-mean: 0.7543857938314297\n",
            "Epoch: 160, Train loss: 0.5171558203782772, Val loss: 0.4640906625672391, Val g-mean: 0.7502197337001734\n",
            "Epoch: 161, Train loss: 0.514087020416234, Val loss: 0.4816398140239088, Val g-mean: 0.7503179921015357\n",
            "Epoch: 162, Train loss: 0.5125904898648507, Val loss: 0.4490971884837276, Val g-mean: 0.7641124035997484\n",
            "Epoch: 163, Train loss: 0.5138885557149717, Val loss: 0.46673764229605075, Val g-mean: 0.7586077844504642\n",
            "Epoch: 164, Train loss: 0.50885311600277, Val loss: 0.4667806023437726, Val g-mean: 0.7546214567025732\n",
            "Epoch: 165, Train loss: 0.5173477971858907, Val loss: 0.4687733195329967, Val g-mean: 0.7530843705467316\n",
            "Epoch: 166, Train loss: 0.5146112234035791, Val loss: 0.49167588392370626, Val g-mean: 0.7449837839866194\n",
            "Epoch: 167, Train loss: 0.5144001028272134, Val loss: 0.491943768764797, Val g-mean: 0.7480766454306833\n",
            "Epoch: 168, Train loss: 0.5163311913382919, Val loss: 0.5070888035391506, Val g-mean: 0.7390987186426675\n",
            "Epoch: 169, Train loss: 0.51452488479586, Val loss: 0.48333399507560226, Val g-mean: 0.7499606167704354\n",
            "Epoch: 170, Train loss: 0.5129117081504045, Val loss: 0.48101092129945755, Val g-mean: 0.7527583613419865\n",
            "Epoch: 171, Train loss: 0.5155179265558223, Val loss: 0.45947683367289993, Val g-mean: 0.7474784010144584\n",
            "Epoch: 172, Train loss: 0.5187868140335133, Val loss: 0.47980115994026784, Val g-mean: 0.7543024923773697\n",
            "Epoch: 173, Train loss: 0.5121041911793258, Val loss: 0.4861766023463325, Val g-mean: 0.7506244834082402\n",
            "Epoch: 174, Train loss: 0.5170995163759445, Val loss: 0.4899480260516468, Val g-mean: 0.7489364670816762\n",
            "Epoch: 175, Train loss: 0.504421710622522, Val loss: 0.4859580401527254, Val g-mean: 0.7524969965193588\n",
            "Epoch: 176, Train loss: 0.5145691531178255, Val loss: 0.4664146770772181, Val g-mean: 0.7590660742379504\n",
            "Epoch: 177, Train loss: 0.5210328562146658, Val loss: 0.45299232653097105, Val g-mean: 0.756564655357893\n",
            "Epoch: 178, Train loss: 0.5148949548944209, Val loss: 0.4754076017753074, Val g-mean: 0.7514200565888968\n",
            "Epoch: 179, Train loss: 0.5126636363574925, Val loss: 0.4585240304862198, Val g-mean: 0.7543409565475178\n",
            "Epoch: 180, Train loss: 0.516338660626843, Val loss: 0.467336019599124, Val g-mean: 0.7526728820774332\n",
            "Epoch: 181, Train loss: 0.5176154253333607, Val loss: 0.45094290434529904, Val g-mean: 0.7523504990594452\n",
            "Epoch: 182, Train loss: 0.5154965995107037, Val loss: 0.47965365707090024, Val g-mean: 0.7501105060714638\n",
            "Epoch: 183, Train loss: 0.5201416349681688, Val loss: 0.47962404905181183, Val g-mean: 0.7494401299650691\n",
            "Epoch: 184, Train loss: 0.509472328676133, Val loss: 0.4599613661044522, Val g-mean: 0.7515008686849334\n",
            "Epoch: 185, Train loss: 0.5161982057976447, Val loss: 0.4685342764776004, Val g-mean: 0.7644177594038534\n",
            "Epoch: 186, Train loss: 0.5221445172985596, Val loss: 0.4537670061384377, Val g-mean: 0.7574649172123157\n",
            "Epoch: 187, Train loss: 0.518533429581764, Val loss: 0.4676535209934962, Val g-mean: 0.75414150076094\n",
            "Epoch: 188, Train loss: 0.5155106051208378, Val loss: 0.45902184769511223, Val g-mean: 0.7582441532743318\n",
            "Epoch: 189, Train loss: 0.5182052494231071, Val loss: 0.47757747024297714, Val g-mean: 0.748695285276423\n",
            "Epoch: 190, Train loss: 0.517578230722806, Val loss: 0.45787510981685237, Val g-mean: 0.7614293921079607\n",
            "Epoch: 191, Train loss: 0.515521631858434, Val loss: 0.4847817562128368, Val g-mean: 0.7411332125951018\n",
            "Epoch: 192, Train loss: 0.5161775252856101, Val loss: 0.4762204916853654, Val g-mean: 0.7483945064804807\n",
            "Epoch: 193, Train loss: 0.5116709836785137, Val loss: 0.48146056285814237, Val g-mean: 0.7486215803243186\n",
            "Epoch: 194, Train loss: 0.5137439246461688, Val loss: 0.47828785545731844, Val g-mean: 0.7487212789376858\n",
            "Epoch: 195, Train loss: 0.5200962687675522, Val loss: 0.48028188767401797, Val g-mean: 0.7505002359498315\n",
            "Epoch: 196, Train loss: 0.5099644832578258, Val loss: 0.46263601297610685, Val g-mean: 0.7511474695015695\n",
            "Epoch: 197, Train loss: 0.517869814897752, Val loss: 0.46112857896246406, Val g-mean: 0.7608317498076886\n",
            "Epoch: 198, Train loss: 0.5135443645785036, Val loss: 0.4492644738209875, Val g-mean: 0.766836330185627\n",
            "Epoch: 199, Train loss: 0.5108505498894207, Val loss: 0.4590230690021264, Val g-mean: 0.7568062728394659\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlrNw0bEqAeh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(classifier):\n",
        "  if torch.cuda.is_available():\n",
        "    classifier = classifier.cuda()\n",
        "  train_gmean = gmean(classifier, train_dataloader)\n",
        "  test_gmean = gmean(classifier, test_dataloader)\n",
        "  print('Epoch: {}'.format(classifier.epoch))\n",
        "  print('Train g-mean: {}'.format(train_gmean.item()))\n",
        "  print('Val g-mean: {}'.format(classifier.val_gmean))\n",
        "  print('Test g-mean: {}'.format(test_gmean.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFoBTASdvqRv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "bf270501-5580-4a3a-a61a-91d5671a9045"
      },
      "source": [
        "print('Best classifier')\n",
        "evaluate(load_classifier())"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best classifier\n",
            "Epoch: 157\n",
            "Train g-mean: 0.7662956504243538\n",
            "Val g-mean: 0.7691076689455674\n",
            "Test g-mean: 0.7367720310994109\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "918KM5F07hFl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "d908ad04-e13e-4aaa-ed83-ef2ac9d27fc3"
      },
      "source": [
        "print('Last classifier')\n",
        "evaluate(classifier)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Last classifier\n",
            "Epoch: 199\n",
            "Train g-mean: 0.773016252273258\n",
            "Val g-mean: 0.7568062728394659\n",
            "Test g-mean: 0.7241882423046799\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_t4Gh43GYWl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}