{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JIT-SDP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dinaldoap/jit-sdp-nn/blob/master/notebook/mlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import random\n",
        "from scipy.stats import mstats\n",
        "\n",
        "from jitsdp import metrics\n",
        "from jitsdp.pipeline import Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fix</th>\n      <th>ns</th>\n      <th>nd</th>\n      <th>nf</th>\n      <th>entropy</th>\n      <th>la</th>\n      <th>ld</th>\n      <th>lt</th>\n      <th>ndev</th>\n      <th>age</th>\n      <th>nuc</th>\n      <th>exp</th>\n      <th>rexp</th>\n      <th>sexp</th>\n      <th>author_date_unix_timestamp</th>\n      <th>classification</th>\n      <th>contains_bug</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>False</td>\n      <td>7.0</td>\n      <td>7.0</td>\n      <td>7.0</td>\n      <td>2.641604</td>\n      <td>9.0</td>\n      <td>9.0</td>\n      <td>426.428571</td>\n      <td>100.0</td>\n      <td>0.000093</td>\n      <td>1.0</td>\n      <td>5171.0</td>\n      <td>30.227271</td>\n      <td>1472.714286</td>\n      <td>1555326371</td>\n      <td>None</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>False</td>\n      <td>7.0</td>\n      <td>7.0</td>\n      <td>7.0</td>\n      <td>2.750000</td>\n      <td>8.0</td>\n      <td>8.0</td>\n      <td>426.428571</td>\n      <td>100.0</td>\n      <td>6.314775</td>\n      <td>2.0</td>\n      <td>5170.0</td>\n      <td>29.227271</td>\n      <td>1471.714286</td>\n      <td>1555326363</td>\n      <td>None</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0.906580</td>\n      <td>15.0</td>\n      <td>44.0</td>\n      <td>96.000000</td>\n      <td>4.0</td>\n      <td>0.034722</td>\n      <td>2.0</td>\n      <td>629.0</td>\n      <td>14.828373</td>\n      <td>414.000000</td>\n      <td>1554971763</td>\n      <td>None</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>40.000000</td>\n      <td>1.0</td>\n      <td>0.000012</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>3.058824</td>\n      <td>3.000000</td>\n      <td>1554969774</td>\n      <td>None</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>4.0</td>\n      <td>1.662506</td>\n      <td>14.0</td>\n      <td>10.0</td>\n      <td>67.000000</td>\n      <td>6.0</td>\n      <td>21.280683</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>2.058824</td>\n      <td>2.000000</td>\n      <td>1554967752</td>\n      <td>Feature Addition</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "     fix   ns   nd   nf   entropy    la    ld          lt   ndev        age  \\\n0  False  7.0  7.0  7.0  2.641604   9.0   9.0  426.428571  100.0   0.000093   \n1  False  7.0  7.0  7.0  2.750000   8.0   8.0  426.428571  100.0   6.314775   \n2  False  1.0  1.0  2.0  0.906580  15.0  44.0   96.000000    4.0   0.034722   \n3  False  1.0  1.0  1.0  0.000000   0.0   0.0   40.000000    1.0   0.000012   \n4  False  1.0  2.0  4.0  1.662506  14.0  10.0   67.000000    6.0  21.280683   \n\n   nuc     exp       rexp         sexp  author_date_unix_timestamp  \\\n0  1.0  5171.0  30.227271  1472.714286                  1555326371   \n1  2.0  5170.0  29.227271  1471.714286                  1555326363   \n2  2.0   629.0  14.828373   414.000000                  1554971763   \n3  1.0     4.0   3.058824     3.000000                  1554969774   \n4  4.0     3.0   2.058824     2.000000                  1554967752   \n\n     classification  contains_bug  \n0              None         False  \n1              None         False  \n2              None         False  \n3              None         False  \n4  Feature Addition         False  "
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/dinaldoap/jit-sdp-data/master/jenkins.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "/home/pytorch/.conda/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  \"\"\"\n"
        }
      ],
      "source": [
        "#df = df.sample(frac=1)\n",
        "label_col = 'contains_bug'\n",
        "features_cols = ['fix', 'ns', 'nd', 'nf', 'entropy', 'la', 'ld', 'lt', 'ndev', 'age', 'nuc', 'exp', 'rexp', 'sexp', 'classification']\n",
        "X = df[features_cols]\n",
        "X['fix'] = X['fix'].astype('int')\n",
        "df_classification = pd.get_dummies(X, columns=['classification'])\n",
        "X = pd.concat([X, df_classification], axis='columns')\n",
        "X = X.drop(['classification'], axis='columns')\n",
        "X = X.values\n",
        "y = df[label_col]\n",
        "y = y.astype('category')\n",
        "y = y.cat.codes\n",
        "y = y.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "[[0.00000000e+00 7.00000000e+00 7.00000000e+00 7.00000000e+00\n  2.64160417e+00 9.00000000e+00 9.00000000e+00 4.26428571e+02\n  1.00000000e+02 9.25925926e-05 1.00000000e+00 5.17100000e+03\n  3.02272705e+01 1.47271429e+03 0.00000000e+00 7.00000000e+00\n  7.00000000e+00 7.00000000e+00 2.64160417e+00 9.00000000e+00\n  9.00000000e+00 4.26428571e+02 1.00000000e+02 9.25925926e-05\n  1.00000000e+00 5.17100000e+03 3.02272705e+01 1.47271429e+03\n  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n  0.00000000e+00 0.00000000e+00]]\n[0]\n"
        }
      ],
      "source": [
        "print(X[:1])\n",
        "print(y[:1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_index = int( len(X) * 0.9 )\n",
        "X_train, y_train = X[:test_index], y[:test_index]\n",
        "X_test, y_test = X[test_index:], y[test_index:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "[[0.00000000e+00 7.00000000e+00 7.00000000e+00 7.00000000e+00\n  2.64160417e+00 9.00000000e+00 9.00000000e+00 4.26428571e+02\n  1.00000000e+02 9.25925926e-05 1.00000000e+00 5.17100000e+03\n  3.02272705e+01 1.47271429e+03 0.00000000e+00 7.00000000e+00\n  7.00000000e+00 7.00000000e+00 2.64160417e+00 9.00000000e+00\n  9.00000000e+00 4.26428571e+02 1.00000000e+02 9.25925926e-05\n  1.00000000e+00 5.17100000e+03 3.02272705e+01 1.47271429e+03\n  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n  0.00000000e+00 0.00000000e+00]]\n[[0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n  0.00000000e+00 1.00000000e+00 1.00000000e+00 2.31000000e+02\n  2.00000000e+00 8.17129630e-03 1.00000000e+00 2.29000000e+03\n  2.35138387e+02 1.96900000e+03 0.00000000e+00 1.00000000e+00\n  1.00000000e+00 1.00000000e+00 0.00000000e+00 1.00000000e+00\n  1.00000000e+00 2.31000000e+02 2.00000000e+00 8.17129630e-03\n  1.00000000e+00 2.29000000e+03 2.35138387e+02 1.96900000e+03\n  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n  0.00000000e+00 0.00000000e+00]]\n"
        }
      ],
      "source": [
        "print(X_train[:1])\n",
        "print(X_test[:1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "mean_train = X_train.mean(axis=0)\n",
        "std_train = X_train.std(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "[2.84157779e-01 1.61084038e+00 2.58937109e+00 4.11079441e+00\n 7.49641664e-01 4.94600037e+01 2.76527676e+01 7.55224305e+02\n 2.09663019e+01 9.03703380e+01 1.96731335e+00 1.81907199e+03\n 7.69168636e+01 9.19113438e+02 2.84157779e-01 1.61084038e+00\n 2.58937109e+00 4.11079441e+00 7.49641664e-01 4.94600037e+01\n 2.76527676e+01 7.55224305e+02 2.09663019e+01 9.03703380e+01\n 1.96731335e+00 1.81907199e+03 7.69168636e+01 9.19113438e+02\n 2.84157779e-01 1.50606841e-01 3.42497242e-02 4.78576683e-01\n 1.47572637e-02 3.76517102e-02]\n[4.51012345e-01 1.63796058e+00 6.67531787e+00 2.43475764e+01\n 1.13389245e+00 5.71750268e+02 5.85715662e+02 1.10935403e+03\n 2.76076700e+01 2.26346855e+02 7.26033575e+00 2.06378941e+03\n 7.91356267e+01 1.36351174e+03 4.51012345e-01 1.63796058e+00\n 6.67531787e+00 2.43475764e+01 1.13389245e+00 5.71750268e+02\n 5.85715662e+02 1.10935403e+03 2.76076700e+01 2.26346855e+02\n 7.26033575e+00 2.06378941e+03 7.91356267e+01 1.36351174e+03\n 4.51012345e-01 3.57665235e-01 1.81869955e-01 4.99540831e-01\n 1.20579795e-01 1.90352460e-01]\n"
        }
      ],
      "source": [
        "print(mean_train)\n",
        "print(std_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train = (X_train - mean_train) / std_train\n",
        "X_test = (X_test - mean_train) / std_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "[[-0.63004435  3.29016442  0.66073691  0.11866502  1.66855552 -0.07076517\n  -0.03184611 -0.29638485  2.86274423 -0.39925558 -0.13323259  1.62416184\n  -0.58999461  0.40601106 -0.63004435  3.29016442  0.66073691  0.11866502\n   1.66855552 -0.07076517 -0.03184611 -0.29638485  2.86274423 -0.39925558\n  -0.13323259  1.62416184 -0.58999461  0.40601106 -0.63004435 -0.42108325\n  -0.18831986  1.0438052  -0.12238588 -0.19779997]]\n[[-0.63004435 -0.3729274  -0.23809669 -0.12776608 -0.66112237 -0.08475729\n  -0.04550462 -0.47254915 -0.68699394 -0.39921989 -0.13323259  0.22818608\n   1.99937159  0.76998718 -0.63004435 -0.3729274  -0.23809669 -0.12776608\n  -0.66112237 -0.08475729 -0.04550462 -0.47254915 -0.68699394 -0.39921989\n  -0.13323259  0.22818608  1.99937159  0.76998718 -0.63004435 -0.42108325\n  -0.18831986  1.0438052  -0.12238588 -0.19779997]]\n"
        }
      ],
      "source": [
        "print(X_train[:1])\n",
        "print(X_test[:1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Classifier(nn.Module):\n",
        "  FILENAME = 'models/classifier.cpt'\n",
        "  def __init__(self, input_size, hidden_size, drop_prob, epoch=None, val_gmean=None):\n",
        "    super(Classifier, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.drop_prob = drop_prob\n",
        "    self.epoch = epoch\n",
        "    self.val_gmean = val_gmean\n",
        "    self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "    self.fcout = nn.Linear(hidden_size, 1)\n",
        "    self.dropout = nn.Dropout(drop_prob)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = torch.relu(self.fc1(x))\n",
        "    x = self.dropout(x)\n",
        "    x = torch.sigmoid(self.fcout(x))\n",
        "    return x\n",
        "\n",
        "  def save(self):\n",
        "    checkpoint = {\n",
        "        'input_size': self.input_size,\n",
        "        'hidden_size': self.hidden_size,\n",
        "        'drop_prob': self.drop_prob,\n",
        "        'val_gmean': self.val_gmean,\n",
        "        'epoch': self.epoch,\n",
        "        'state_dict': self.state_dict()\n",
        "    }\n",
        "    with open(Classifier.FILENAME, 'wb') as f:\n",
        "      torch.save(checkpoint, f)\n",
        "\n",
        "  def load(self):\n",
        "    with open(Classifier.FILENAME, 'rb') as f:\n",
        "      checkpoint = torch.load(f)\n",
        "      self.input_size = checkpoint['input_size']\n",
        "      self.hidden_size = checkpoint['hidden_size']\n",
        "      self.drop_prob = checkpoint['drop_prob']\n",
        "      self.epoch = checkpoint['epoch']\n",
        "      self.val_gmean = checkpoint['val_gmean']\n",
        "      self.load_state_dict(checkpoint['state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": "Classifier(\n  (fc1): Linear(in_features=34, out_features=34, bias=True)\n  (fcout): Linear(in_features=34, out_features=1, bias=True)\n  (dropout): Dropout(p=0.5, inplace=False)\n)"
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classifier = Classifier(input_size=X.shape[1], hidden_size=X.shape[1], drop_prob=0.5)\n",
        "classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": "Classifier(\n  (fc1): Linear(in_features=34, out_features=34, bias=True)\n  (fcout): Linear(in_features=34, out_features=1, bias=True)\n  (dropout): Dropout(p=0.5, inplace=False)\n)"
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classifier.save()\n",
        "classifier.load()\n",
        "classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(params=classifier.parameters(), lr=0.003)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "rain loss: 0.5531444301101011, Train g-mean: 0.7041673842410323, Val g-mean: 0.7354159034053153\nEpoch: 16, Train loss: 0.5452996027517839, Train g-mean: 0.7097098312810494, Val g-mean: 0.7415447320878773\nEpoch: 17, Train loss: 0.5496033655070048, Train g-mean: 0.7037022432688945, Val g-mean: 0.7496923972539953\nEpoch: 18, Train loss: 0.5377251954834247, Train g-mean: 0.710941313505955, Val g-mean: 0.7473618514079278\nEpoch: 19, Train loss: 0.5383792890237962, Train g-mean: 0.7089247713746331, Val g-mean: 0.7442710953382234\nEpoch: 20, Train loss: 0.5397580970890684, Train g-mean: 0.7082231661548223, Val g-mean: 0.7451103279664498\nEpoch: 21, Train loss: 0.5468890516920155, Train g-mean: 0.7086659802260893, Val g-mean: 0.7378379259401373\nEpoch: 22, Train loss: 0.5410651545467623, Train g-mean: 0.7047279462929005, Val g-mean: 0.7522709292727274\nEpoch: 23, Train loss: 0.5434670638853226, Train g-mean: 0.7086951652758462, Val g-mean: 0.7520782265370977\nEpoch: 24, Train loss: 0.5363920167771514, Train g-mean: 0.7054073701527348, Val g-mean: 0.75035388514214\nEpoch: 25, Train loss: 0.5404270294784672, Train g-mean: 0.7114203776468127, Val g-mean: 0.7494415454892144\nEpoch: 26, Train loss: 0.542760233857385, Train g-mean: 0.7129873880751381, Val g-mean: 0.7455982362402873\nEpoch: 27, Train loss: 0.5365602315689293, Train g-mean: 0.7057668866524407, Val g-mean: 0.7463389182684835\nEpoch: 28, Train loss: 0.5357901223226111, Train g-mean: 0.7136213410736268, Val g-mean: 0.7532885719313086\nEpoch: 29, Train loss: 0.530182182197193, Train g-mean: 0.7121712560714226, Val g-mean: 0.7534994163185397\nEpoch: 30, Train loss: 0.5344710189410871, Train g-mean: 0.7148527350394656, Val g-mean: 0.7516514900309558\nEpoch: 31, Train loss: 0.5350422962055982, Train g-mean: 0.7216183281637022, Val g-mean: 0.7658321365561966\nEpoch: 32, Train loss: 0.5327264997784561, Train g-mean: 0.7206249133787122, Val g-mean: 0.7518121710057413\nEpoch: 33, Train loss: 0.5359907731883146, Train g-mean: 0.7172426697896017, Val g-mean: 0.7551710281517794\nEpoch: 34, Train loss: 0.5384847513770253, Train g-mean: 0.717307125941612, Val g-mean: 0.7543231164759819\nEpoch: 35, Train loss: 0.5268382085998221, Train g-mean: 0.7161971467302904, Val g-mean: 0.7570961793056494\nEpoch: 36, Train loss: 0.5347411730698014, Train g-mean: 0.7200343590661616, Val g-mean: 0.7577597824267712\nEpoch: 37, Train loss: 0.5309587352775388, Train g-mean: 0.7157349131636218, Val g-mean: 0.7553631534825918\nEpoch: 38, Train loss: 0.5277362578783862, Train g-mean: 0.721018503898733, Val g-mean: 0.7539341102166275\nEpoch: 39, Train loss: 0.5336968168073088, Train g-mean: 0.7164888462837176, Val g-mean: 0.7514451960044143\nEpoch: 40, Train loss: 0.533151454305173, Train g-mean: 0.7233750678487999, Val g-mean: 0.7600340286512443\nEpoch: 41, Train loss: 0.5316650181391178, Train g-mean: 0.7127970000076967, Val g-mean: 0.7661946320184562\nEpoch: 42, Train loss: 0.5307841321580897, Train g-mean: 0.7226912081026827, Val g-mean: 0.7608333053219837\nEpoch: 43, Train loss: 0.5318170091045038, Train g-mean: 0.7197253388588082, Val g-mean: 0.7604776057542625\nEpoch: 44, Train loss: 0.5319336856545855, Train g-mean: 0.7226432513981726, Val g-mean: 0.7626132952230248\nEpoch: 45, Train loss: 0.5279449932726578, Train g-mean: 0.7185580693966016, Val g-mean: 0.7636306499684543\nEpoch: 46, Train loss: 0.5303069248998072, Train g-mean: 0.7216125193885033, Val g-mean: 0.7635785198868454\nEpoch: 47, Train loss: 0.5308786459845455, Train g-mean: 0.7267253915169831, Val g-mean: 0.7650853837467206\nEpoch: 48, Train loss: 0.5221534917193654, Train g-mean: 0.7201616649342102, Val g-mean: 0.7622363969674807\nEpoch: 49, Train loss: 0.5308716410486056, Train g-mean: 0.7249264173263206, Val g-mean: 0.7588084894988367\nEpoch: 50, Train loss: 0.5346941308946899, Train g-mean: 0.7277260113791583, Val g-mean: 0.7640007859431583\nEpoch: 51, Train loss: 0.5268183930225135, Train g-mean: 0.7247432245109215, Val g-mean: 0.7571453040213432\nEpoch: 52, Train loss: 0.5236557866862365, Train g-mean: 0.7267421609672932, Val g-mean: 0.7634220671866578\nEpoch: 53, Train loss: 0.5201642471137488, Train g-mean: 0.7282901887713576, Val g-mean: 0.7637091368770254\nEpoch: 54, Train loss: 0.5253761998037569, Train g-mean: 0.7238228478495935, Val g-mean: 0.7601878367064281\nEpoch: 55, Train loss: 0.5198347988577482, Train g-mean: 0.7296365786113708, Val g-mean: 0.7652595448360848\nEpoch: 56, Train loss: 0.5243546331612773, Train g-mean: 0.7230379934396021, Val g-mean: 0.7587235667784531\nEpoch: 57, Train loss: 0.5294890708512947, Train g-mean: 0.7300123016313637, Val g-mean: 0.7676094462568352\nEpoch: 58, Train loss: 0.5276199174405848, Train g-mean: 0.7273560433945657, Val g-mean: 0.770645160628321\nEpoch: 59, Train loss: 0.5233957673048406, Train g-mean: 0.7292618170062019, Val g-mean: 0.7656326351484306\nEpoch: 60, Train loss: 0.5191752166955274, Train g-mean: 0.72839346152608, Val g-mean: 0.766419643660596\nEpoch: 61, Train loss: 0.5249825062695951, Train g-mean: 0.7307727465693478, Val g-mean: 0.7652742410984401\nEpoch: 62, Train loss: 0.5249318242132761, Train g-mean: 0.7332577414782268, Val g-mean: 0.7594913041296406\nEpoch: 63, Train loss: 0.5226645975542377, Train g-mean: 0.7289651994871419, Val g-mean: 0.7665734118524297\nEpoch: 64, Train loss: 0.5271748011673422, Train g-mean: 0.7283384175170822, Val g-mean: 0.7654506010715166\nEpoch: 65, Train loss: 0.5307944455344578, Train g-mean: 0.735047239925483, Val g-mean: 0.7681062633724225\nEpoch: 66, Train loss: 0.5239134485359862, Train g-mean: 0.7302233168751229, Val g-mean: 0.7707653199004145\nEpoch: 67, Train loss: 0.5161626615453884, Train g-mean: 0.729027672753892, Val g-mean: 0.7665144152156202\nEpoch: 68, Train loss: 0.5226291975361901, Train g-mean: 0.7336186060064532, Val g-mean: 0.7691680496485434\nEpoch: 69, Train loss: 0.5206065795894943, Train g-mean: 0.7230185707122494, Val g-mean: 0.7740614685568503\nEpoch: 70, Train loss: 0.5237235840112645, Train g-mean: 0.7259005095390815, Val g-mean: 0.7684767611826907\nEpoch: 71, Train loss: 0.518606157170276, Train g-mean: 0.7283276244007428, Val g-mean: 0.7731653646708173\nEpoch: 72, Train loss: 0.5146043094576561, Train g-mean: 0.7327858446302429, Val g-mean: 0.7684538369495435\nEpoch: 73, Train loss: 0.5187272500599072, Train g-mean: 0.7335131055429089, Val g-mean: 0.7746680145465974\nEpoch: 74, Train loss: 0.5278711794574155, Train g-mean: 0.7292913586606865, Val g-mean: 0.7677706570622819\nEpoch: 75, Train loss: 0.5222813675005188, Train g-mean: 0.7314293159296494, Val g-mean: 0.7662999090418098\nEpoch: 76, Train loss: 0.5271838541448599, Train g-mean: 0.7279387247815344, Val g-mean: 0.775048553281349\nEpoch: 77, Train loss: 0.5161736476004136, Train g-mean: 0.7330093117149541, Val g-mean: 0.7697578872149672\nEpoch: 78, Train loss: 0.5101008106184312, Train g-mean: 0.7332036928714373, Val g-mean: 0.7724902271165579\nEpoch: 79, Train loss: 0.5238340985798232, Train g-mean: 0.7325631766301054, Val g-mean: 0.7760517512940407\nEpoch: 80, Train loss: 0.5228264191518058, Train g-mean: 0.7357793710947476, Val g-mean: 0.7749621949713061\nEpoch: 81, Train loss: 0.5172095661967478, Train g-mean: 0.7368514977999981, Val g-mean: 0.7729604599197587\nEpoch: 82, Train loss: 0.5200075279586577, Train g-mean: 0.7320737420282193, Val g-mean: 0.7820562688807831\nEpoch: 83, Train loss: 0.5191883943297747, Train g-mean: 0.7336052597440142, Val g-mean: 0.7700009457377169\nEpoch: 84, Train loss: 0.5214582816704172, Train g-mean: 0.7373911916918969, Val g-mean: 0.7781722090981746\nEpoch: 85, Train loss: 0.5206803461322013, Train g-mean: 0.7313505567200541, Val g-mean: 0.7845031725203099\nEpoch: 86, Train loss: 0.5167025887569017, Train g-mean: 0.7335736565618928, Val g-mean: 0.7770550595236001\nEpoch: 87, Train loss: 0.5216005229634103, Train g-mean: 0.7373454434194459, Val g-mean: 0.7777037770768365\nEpoch: 88, Train loss: 0.5197971648733892, Train g-mean: 0.7369762114964786, Val g-mean: 0.780513951363503\nEpoch: 89, Train loss: 0.5247876732949545, Train g-mean: 0.7310939194775536, Val g-mean: 0.7708686691375962\nEpoch: 90, Train loss: 0.510086873630693, Train g-mean: 0.7340973320597203, Val g-mean: 0.774670121222311\nEpoch: 91, Train loss: 0.514306835115975, Train g-mean: 0.7340952501549419, Val g-mean: 0.7768496726026128\nEpoch: 92, Train loss: 0.5167553239342755, Train g-mean: 0.7366247498201108, Val g-mean: 0.7747295667288446\nEpoch: 93, Train loss: 0.5225973781695661, Train g-mean: 0.7344181112425814, Val g-mean: 0.7746379040160061\nEpoch: 94, Train loss: 0.5259652551849445, Train g-mean: 0.7304277479556357, Val g-mean: 0.7648248433767696\nEpoch: 95, Train loss: 0.5195413968421448, Train g-mean: 0.7347825351656067, Val g-mean: 0.781908833902963\nEpoch: 96, Train loss: 0.514723245510325, Train g-mean: 0.7309225718209952, Val g-mean: 0.7707032255451908\nEpoch: 97, Train loss: 0.5178141587363569, Train g-mean: 0.73730492005845, Val g-mean: 0.7758172383273381\nEpoch: 98, Train loss: 0.5179574642257478, Train g-mean: 0.7357440250527832, Val g-mean: 0.7729253400228778\nEpoch: 99, Train loss: 0.519354671631299, Train g-mean: 0.7317519756559819, Val g-mean: 0.7654123798658425\nEpoch: 100, Train loss: 0.5121285059613561, Train g-mean: 0.7392516168572721, Val g-mean: 0.7696773876676439\nEpoch: 101, Train loss: 0.5218904149291927, Train g-mean: 0.7392253355011701, Val g-mean: 0.7715199184488281\nEpoch: 102, Train loss: 0.5203067824161919, Train g-mean: 0.7352519488734409, Val g-mean: 0.777106420690674\nEpoch: 103, Train loss: 0.5155635801261227, Train g-mean: 0.7301539591741141, Val g-mean: 0.7699086862437096\nEpoch: 104, Train loss: 0.5126330723296515, Train g-mean: 0.7311320049634008, Val g-mean: 0.7813779820012074\nEpoch: 105, Train loss: 0.5136247075264548, Train g-mean: 0.7382604124819904, Val g-mean: 0.7732028989656935\nEpoch: 106, Train loss: 0.5152091928951376, Train g-mean: 0.7351950503688696, Val g-mean: 0.7763796785616301\nEpoch: 107, Train loss: 0.5144757313825953, Train g-mean: 0.7319443222454126, Val g-mean: 0.7760663121563491\nEpoch: 108, Train loss: 0.5114127611077196, Train g-mean: 0.7327362101648398, Val g-mean: 0.775742950924367\nEpoch: 109, Train loss: 0.5174704963706406, Train g-mean: 0.7364419893549187, Val g-mean: 0.7804178681024937\nEpoch: 110, Train loss: 0.5281988463206742, Train g-mean: 0.735061361031448, Val g-mean: 0.7748187241120992\nEpoch: 111, Train loss: 0.5225055823504509, Train g-mean: 0.7328580891335869, Val g-mean: 0.7627242809585089\nEpoch: 112, Train loss: 0.5152619550110428, Train g-mean: 0.736453065194439, Val g-mean: 0.7773461847691884\nEpoch: 113, Train loss: 0.5214037828197885, Train g-mean: 0.736552174363641, Val g-mean: 0.773080215991741\nEpoch: 114, Train loss: 0.5098026591808134, Train g-mean: 0.7331305362773252, Val g-mean: 0.7741711390173736\nEpoch: 115, Train loss: 0.5172017489189534, Train g-mean: 0.7422636827501865, Val g-mean: 0.7770112975509484\nEpoch: 116, Train loss: 0.5148099599086184, Train g-mean: 0.7402956247860014, Val g-mean: 0.7770847525607273\nEpoch: 117, Train loss: 0.5190037525561195, Train g-mean: 0.7385519053617429, Val g-mean: 0.7809282294968782\nEpoch: 118, Train loss: 0.5186571968846513, Train g-mean: 0.7320441914297827, Val g-mean: 0.7734139414506119\nEpoch: 119, Train loss: 0.5131729441148934, Train g-mean: 0.7326087654943296, Val g-mean: 0.7633442005872509\nEpoch: 120, Train loss: 0.5114354412200688, Train g-mean: 0.7344731892998877, Val g-mean: 0.7650637727736408\nEpoch: 121, Train loss: 0.5179006317753125, Train g-mean: 0.7363558484774274, Val g-mean: 0.7756942035862496\nEpoch: 122, Train loss: 0.515729731880443, Train g-mean: 0.7378293805623564, Val g-mean: 0.7743324104659244\nEpoch: 123, Train loss: 0.5152087144505981, Train g-mean: 0.738572406840919, Val g-mean: 0.7735613591624692\nEpoch: 124, Train loss: 0.5130528125815208, Train g-mean: 0.7378669642966618, Val g-mean: 0.7723413030101697\nEpoch: 125, Train loss: 0.5133749236756222, Train g-mean: 0.7384306285680955, Val g-mean: 0.7798063417578381\nEpoch: 126, Train loss: 0.514523938655073, Train g-mean: 0.7363863702395453, Val g-mean: 0.7821295771036367\nEpoch: 127, Train loss: 0.5192975772588481, Train g-mean: 0.7375536144415497, Val g-mean: 0.7790809847139052\nEpoch: 128, Train loss: 0.5113191935067732, Train g-mean: 0.7356313898060299, Val g-mean: 0.7719828529783141\nEpoch: 129, Train loss: 0.5075667088679915, Train g-mean: 0.7382851918355885, Val g-mean: 0.7770997405223208\nEpoch: 130, Train loss: 0.5192701339860565, Train g-mean: 0.7386227332021913, Val g-mean: 0.7805226468003824\nEpoch: 131, Train loss: 0.514379607867328, Train g-mean: 0.7413119582159302, Val g-mean: 0.7813652108290817\nEpoch: 132, Train loss: 0.5119965577541031, Train g-mean: 0.7375545275393279, Val g-mean: 0.7875368291662909\nEpoch: 133, Train loss: 0.514922237305606, Train g-mean: 0.7348887736392564, Val g-mean: 0.7710526440905581\nEpoch: 134, Train loss: 0.5210029048774172, Train g-mean: 0.7419799794396238, Val g-mean: 0.7792293883440837\nEpoch: 135, Train loss: 0.512583036868627, Train g-mean: 0.7368906717312234, Val g-mean: 0.7777781283655214\nEpoch: 136, Train loss: 0.5167778941721196, Train g-mean: 0.7413600630793322, Val g-mean: 0.7802362986683793\nEpoch: 137, Train loss: 0.522512725308233, Train g-mean: 0.7367327074698786, Val g-mean: 0.7741663504140869\nEpoch: 138, Train loss: 0.5112602402846704, Train g-mean: 0.740448323316658, Val g-mean: 0.777546154395873\nEpoch: 139, Train loss: 0.5182698773779438, Train g-mean: 0.7404979224103875, Val g-mean: 0.7805118200580876\nEpoch: 140, Train loss: 0.5111757657146809, Train g-mean: 0.7431374587860238, Val g-mean: 0.7816671961821772\nEpoch: 141, Train loss: 0.5126852381464161, Train g-mean: 0.7410284256999766, Val g-mean: 0.7822170418937913\nEpoch: 142, Train loss: 0.5093697439101933, Train g-mean: 0.7390141087189752, Val g-mean: 0.7722007255432198\nEpoch: 143, Train loss: 0.5171090367779456, Train g-mean: 0.739061110700715, Val g-mean: 0.7797627757383186\nEpoch: 144, Train loss: 0.5115512899948627, Train g-mean: 0.7442955139372986, Val g-mean: 0.7812340579379768\nEpoch: 145, Train loss: 0.5110862973215067, Train g-mean: 0.7389968176011215, Val g-mean: 0.7758021803633165\nEpoch: 146, Train loss: 0.5124096743644859, Train g-mean: 0.736105332301584, Val g-mean: 0.7797476192023955\nEpoch: 147, Train loss: 0.517390832361965, Train g-mean: 0.7318578985283115, Val g-mean: 0.7716015596130783\nEpoch: 148, Train loss: 0.5160001864050542, Train g-mean: 0.7373371557120073, Val g-mean: 0.7766864021494784\nEpoch: 149, Train loss: 0.5136739269026699, Train g-mean: 0.7413872911241655, Val g-mean: 0.7809720720973824\nEpoch: 150, Train loss: 0.5203807872460657, Train g-mean: 0.7392222877874536, Val g-mean: 0.7759208996539609\nEpoch: 151, Train loss: 0.5180269195022909, Train g-mean: 0.735844795360781, Val g-mean: 0.7713604282228326\nEpoch: 152, Train loss: 0.5182678394169449, Train g-mean: 0.7420994509063163, Val g-mean: 0.7820705884933794\nEpoch: 153, Train loss: 0.5156912207416255, Train g-mean: 0.7392484109913893, Val g-mean: 0.7836050554064626\nEpoch: 154, Train loss: 0.5130717356994915, Train g-mean: 0.7425397739534925, Val g-mean: 0.7819934794567897\nEpoch: 155, Train loss: 0.511336176568614, Train g-mean: 0.7416475254625932, Val g-mean: 0.782527570033051\nEpoch: 156, Train loss: 0.5064343122074325, Train g-mean: 0.738504148961206, Val g-mean: 0.7808865686562564\nEpoch: 157, Train loss: 0.5142323815604288, Train g-mean: 0.7416385091799944, Val g-mean: 0.7790556614509409\nEpoch: 158, Train loss: 0.5128675817520706, Train g-mean: 0.7431504070890868, Val g-mean: 0.7803218677219893\nEpoch: 159, Train loss: 0.5185482031236125, Train g-mean: 0.7404943760722287, Val g-mean: 0.7756274328837749\nEpoch: 160, Train loss: 0.5141612546722351, Train g-mean: 0.7457278726450604, Val g-mean: 0.7775260111005595\nEpoch: 161, Train loss: 0.5161541859743948, Train g-mean: 0.7406945717607554, Val g-mean: 0.7778103773269919\nEpoch: 162, Train loss: 0.5121620202177558, Train g-mean: 0.7410296066063861, Val g-mean: 0.780064638066046\nEpoch: 163, Train loss: 0.5101534026405593, Train g-mean: 0.738921828260142, Val g-mean: 0.7825950243970399\nEpoch: 164, Train loss: 0.5160423843207048, Train g-mean: 0.7431576999934794, Val g-mean: 0.7826965883014039\nEpoch: 165, Train loss: 0.509044099384246, Train g-mean: 0.7448526265071479, Val g-mean: 0.7727721853597526\nEpoch: 166, Train loss: 0.5178706345021649, Train g-mean: 0.740787030786312, Val g-mean: 0.77360661860876\nEpoch: 167, Train loss: 0.5167166300234121, Train g-mean: 0.7427981123659894, Val g-mean: 0.783602338603413\nEpoch: 168, Train loss: 0.5142237108681368, Train g-mean: 0.7413175480585295, Val g-mean: 0.7812928015872839\nEpoch: 169, Train loss: 0.5110953828762071, Train g-mean: 0.7416715947231382, Val g-mean: 0.7765430732228273\nEpoch: 170, Train loss: 0.514453844588715, Train g-mean: 0.7434678544494192, Val g-mean: 0.7758448472576774\nEpoch: 171, Train loss: 0.511206385069467, Train g-mean: 0.7411264180086632, Val g-mean: 0.7835674878993257\nEpoch: 172, Train loss: 0.5166348676542283, Train g-mean: 0.7404684238635457, Val g-mean: 0.7762227718670345\nEpoch: 173, Train loss: 0.5128694318315409, Train g-mean: 0.7385232448160518, Val g-mean: 0.7785420055907305\nEpoch: 174, Train loss: 0.5144042460393218, Train g-mean: 0.7441433847360596, Val g-mean: 0.7839876120228583\nEpoch: 175, Train loss: 0.5120926600956287, Train g-mean: 0.7423823028161068, Val g-mean: 0.784678733462607\nEpoch: 176, Train loss: 0.5177934378697917, Train g-mean: 0.7364045651609941, Val g-mean: 0.7773867869999748\nEpoch: 177, Train loss: 0.5187393699709025, Train g-mean: 0.7422058477159272, Val g-mean: 0.7841553271459575\nEpoch: 178, Train loss: 0.5165351452853523, Train g-mean: 0.7428259440879239, Val g-mean: 0.7806021645986426\nEpoch: 179, Train loss: 0.5143191504234591, Train g-mean: 0.7394123679894351, Val g-mean: 0.7789584631351856\nEpoch: 180, Train loss: 0.5099363143315543, Train g-mean: 0.7448588994225871, Val g-mean: 0.7808351048566808\nEpoch: 181, Train loss: 0.5085771514693658, Train g-mean: 0.745799117572084, Val g-mean: 0.7802637103697079\nEpoch: 182, Train loss: 0.5118347646683704, Train g-mean: 0.7395512778658118, Val g-mean: 0.7819443267760385\nEpoch: 183, Train loss: 0.5109343511984141, Train g-mean: 0.7454607906975056, Val g-mean: 0.7845693567458972\nEpoch: 184, Train loss: 0.5046209272236024, Train g-mean: 0.7422703219289032, Val g-mean: 0.771854105273559\nEpoch: 185, Train loss: 0.5141766870543752, Train g-mean: 0.7432359829815038, Val g-mean: 0.779663734550543\nEpoch: 186, Train loss: 0.5117214625323058, Train g-mean: 0.7469509943421274, Val g-mean: 0.7802183546065171\nEpoch: 187, Train loss: 0.5087428926026828, Train g-mean: 0.740275280726337, Val g-mean: 0.774785596143413\nEpoch: 188, Train loss: 0.51070190071702, Train g-mean: 0.7424356732138337, Val g-mean: 0.7863766103058855\nEpoch: 189, Train loss: 0.5120916681411022, Train g-mean: 0.7436169903417781, Val g-mean: 0.7816153783462656\nEpoch: 190, Train loss: 0.5100221680191902, Train g-mean: 0.7437624853478916, Val g-mean: 0.777305338834539\nEpoch: 191, Train loss: 0.5122379126879592, Train g-mean: 0.7383211998691475, Val g-mean: 0.7816871373123586\nEpoch: 192, Train loss: 0.5096438391155999, Train g-mean: 0.7405797434487933, Val g-mean: 0.7798041220158106\nEpoch: 193, Train loss: 0.5168399088992701, Train g-mean: 0.7414957239597183, Val g-mean: 0.7767845494066515\nEpoch: 194, Train loss: 0.51162451855498, Train g-mean: 0.7428631114362488, Val g-mean: 0.7790521912717752\nEpoch: 195, Train loss: 0.5112200983956731, Train g-mean: 0.7457358754715625, Val g-mean: 0.7764584357545706\nEpoch: 196, Train loss: 0.507121585904521, Train g-mean: 0.7431096632731528, Val g-mean: 0.7784437886037251\nEpoch: 197, Train loss: 0.5070859528275372, Train g-mean: 0.7430938986810091, Val g-mean: 0.7883063412772956\nEpoch: 198, Train loss: 0.5111471828148203, Train g-mean: 0.7422026857581837, Val g-mean: 0.7857232217196316\nEpoch: 199, Train loss: 0.5167131946543482, Train g-mean: 0.7429215842386792, Val g-mean: 0.7856706079195179\n"
        }
      ],
      "source": [
        "pipeline = Pipeline(steps=[], classifier=classifier, optimizer=optimizer, criterion=criterion, max_epochs=200, fading_factor=0.9999)\n",
        "pipeline.train(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "  def evaluate(pipeline):\n",
        "    train_gmean, train_recalls = pipeline.evaluate(X_train, y_train)\n",
        "    test_gmean, test_recalls = pipeline.evaluate(X_test, y_test)\n",
        "    print('Epoch: {}'.format(pipeline.epoch))\n",
        "    print('Train g-mean: {}, recalls: {}'.format(train_gmean.item(), train_recalls))\n",
        "    print('Test g-mean: {}, recalls: {}'.format(test_gmean.item(), test_recalls))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Last classifier\nEpoch: 199\nTrain g-mean: 0.7461736734404283, recalls: [0.73941845 0.75299061]\nTest g-mean: 0.7407253009874877, recalls: [0.78547772 0.69852264]\n"
        }
      ],
      "source": [
        "print('Last classifier')\n",
        "evaluate(pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Best classifier\nEpoch: 197\nTrain g-mean: 0.7466168316428287, recalls: [0.72771996 0.7660044 ]\nTest g-mean: 0.7418633688268692, recalls: [0.77461644 0.7104952 ]\n"
        }
      ],
      "source": [
        "print('Best classifier')\n",
        "pipeline.load()\n",
        "evaluate(pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "L_t4Gh43GYWl"
      },
      "outputs": [],
      "source": []
    }
  ]
}