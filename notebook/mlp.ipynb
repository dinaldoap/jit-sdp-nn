{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JIT-SDP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dinaldoap/jit-sdp-nn/blob/master/notebook/mlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import random\n",
        "from scipy.stats import mstats\n",
        "\n",
        "from jitsdp import metrics\n",
        "from jitsdp.pipeline import Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fix</th>\n      <th>ns</th>\n      <th>nd</th>\n      <th>nf</th>\n      <th>entrophy</th>\n      <th>la</th>\n      <th>ld</th>\n      <th>lt</th>\n      <th>ndev</th>\n      <th>age</th>\n      <th>nuc</th>\n      <th>exp</th>\n      <th>rexp</th>\n      <th>sexp</th>\n      <th>contains_bug</th>\n      <th>author_date_unix_timestamp</th>\n      <th>commit_type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>False</td>\n      <td>2</td>\n      <td>5</td>\n      <td>23</td>\n      <td>3.630787</td>\n      <td>3754</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>11.0</td>\n      <td>0.000000</td>\n      <td>21</td>\n      <td>False</td>\n      <td>1323292816</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>False</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0.811278</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>23.5</td>\n      <td>0.000000</td>\n      <td>22</td>\n      <td>False</td>\n      <td>1323292845</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>True</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.000000</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>0.019688</td>\n      <td>1</td>\n      <td>25.0</td>\n      <td>51.793651</td>\n      <td>2</td>\n      <td>False</td>\n      <td>1323294546</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>False</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.000000</td>\n      <td>2</td>\n      <td>3</td>\n      <td>31.0</td>\n      <td>1</td>\n      <td>0.035972</td>\n      <td>1</td>\n      <td>26.0</td>\n      <td>28.799228</td>\n      <td>3</td>\n      <td>False</td>\n      <td>1323295924</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>False</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0.000000</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>0.5</td>\n      <td>0.000000</td>\n      <td>1</td>\n      <td>False</td>\n      <td>1323301755</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "     fix  ns  nd  nf  entrophy    la  ld    lt  ndev       age  nuc   exp  \\\n0  False   2   5  23  3.630787  3754   0   0.0     1  0.000000    0  11.0   \n1  False   2   2   2  0.811278     4   0   0.0     1  0.000000    0  23.5   \n2   True   1   1   1  0.000000     1   1   3.0     1  0.019688    1  25.0   \n3  False   1   1   1  0.000000     2   3  31.0     1  0.035972    1  26.0   \n4  False   1   1   2  0.000000     5   0   0.0     1  0.000000    0   0.5   \n\n        rexp  sexp  contains_bug  author_date_unix_timestamp  commit_type  \n0   0.000000    21         False                  1323292816            0  \n1   0.000000    22         False                  1323292845            0  \n2  51.793651     2         False                  1323294546            0  \n3  28.799228     3         False                  1323295924            0  \n4   0.000000     1         False                  1323301755            0  "
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/dinaldoap/jit-sdp-data/master/brackets.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "/home/pytorch/.conda/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  \n"
        }
      ],
      "source": [
        "df = df[df['commit_type'] != 3]\n",
        "#df = df.sample(frac=1)\n",
        "label_col = 'contains_bug'\n",
        "features_cols = ['fix', 'ns', 'nd', 'nf', 'entrophy', 'la', 'ld', 'lt', 'ndev', 'age', 'nuc', 'exp', 'rexp', 'sexp']\n",
        "X = df[features_cols]\n",
        "X['fix'] = X['fix'].astype('int')\n",
        "X = X.values\n",
        "y = df[label_col]\n",
        "y = y.astype('int')\n",
        "y = y.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "[[0.000000e+00 2.000000e+00 5.000000e+00 2.300000e+01 3.630787e+00\n  3.754000e+03 0.000000e+00 0.000000e+00 1.000000e+00 0.000000e+00\n  0.000000e+00 1.100000e+01 0.000000e+00 2.100000e+01]]\n[0]\n"
        }
      ],
      "source": [
        "print(X[:1])\n",
        "print(y[:1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_index = int( len(X) * 0.9 )\n",
        "X_train, y_train = X[:test_index], y[:test_index]\n",
        "X_test, y_test = X[test_index:], y[test_index:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "[[0.000000e+00 2.000000e+00 5.000000e+00 2.300000e+01 3.630787e+00\n  3.754000e+03 0.000000e+00 0.000000e+00 1.000000e+00 0.000000e+00\n  0.000000e+00 1.100000e+01 0.000000e+00 2.100000e+01]]\n[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
        }
      ],
      "source": [
        "print(X_train[:1])\n",
        "print(X_test[:1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "mean_train = X_train.mean(axis=0)\n",
        "std_train = X_train.std(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "[1.58931896e-01 7.46068425e-01 1.16252648e+00 1.78008858e+00\n 3.20738417e-01 8.42387830e+01 3.79735541e+01 4.22212467e+02\n 2.67096091e+01 7.55832129e+00 8.86575518e+01 4.79215900e+02\n 3.01487924e+01 2.58808011e+02]\n[3.65612566e-01 5.92085003e-01 2.99453343e+00 1.42250140e+01\n 6.57629456e-01 1.84675239e+03 9.80224735e+02 1.59479852e+03\n 2.69620082e+01 3.21379111e+01 1.94257009e+02 9.22457304e+02\n 2.24081690e+02 4.61634212e+02]\n"
        }
      ],
      "source": [
        "print(mean_train)\n",
        "print(std_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train = (X_train - mean_train) / std_train\n",
        "X_test = (X_test - mean_train) / std_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "[[-0.43470031  2.11782357  1.28149297  1.4917322   5.03330341  1.98714307\n  -0.03873964 -0.26474345 -0.95354949 -0.23518396 -0.45639306 -0.5075746\n  -0.13454376 -0.51514382]]\n[[-0.43470031 -1.26006979 -0.38821623 -0.12513791 -0.48771906 -0.04561455\n  -0.03873964 -0.26474345 -0.99063871 -0.23518396 -0.45639306 -0.51949927\n  -0.13454376 -0.56063438]]\n"
        }
      ],
      "source": [
        "print(X_train[:1])\n",
        "print(X_test[:1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Classifier(nn.Module):\n",
        "  FILENAME = 'models/classifier.cpt'\n",
        "  def __init__(self, input_size, hidden_size, drop_prob, epoch=None, val_gmean=None):\n",
        "    super(Classifier, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.drop_prob = drop_prob\n",
        "    self.epoch = epoch\n",
        "    self.val_gmean = val_gmean\n",
        "    self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "    self.fcout = nn.Linear(hidden_size, 1)\n",
        "    self.dropout = nn.Dropout(drop_prob)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = torch.relu(self.fc1(x))\n",
        "    x = self.dropout(x)\n",
        "    x = torch.sigmoid(self.fcout(x))\n",
        "    return x\n",
        "\n",
        "  def save(self):\n",
        "    checkpoint = {\n",
        "        'input_size': self.input_size,\n",
        "        'hidden_size': self.hidden_size,\n",
        "        'drop_prob': self.drop_prob,\n",
        "        'val_gmean': self.val_gmean,\n",
        "        'epoch': self.epoch,\n",
        "        'state_dict': self.state_dict()\n",
        "    }\n",
        "    with open(Classifier.FILENAME, 'wb') as f:\n",
        "      torch.save(checkpoint, f)\n",
        "\n",
        "  def load(self):\n",
        "    with open(Classifier.FILENAME, 'rb') as f:\n",
        "      checkpoint = torch.load(f)\n",
        "      self.input_size = checkpoint['input_size']\n",
        "      self.hidden_size = checkpoint['hidden_size']\n",
        "      self.drop_prob = checkpoint['drop_prob']\n",
        "      self.epoch = checkpoint['epoch']\n",
        "      self.val_gmean = checkpoint['val_gmean']\n",
        "      self.load_state_dict(checkpoint['state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": "Classifier(\n  (fc1): Linear(in_features=14, out_features=14, bias=True)\n  (fcout): Linear(in_features=14, out_features=1, bias=True)\n  (dropout): Dropout(p=0.5, inplace=False)\n)"
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classifier = Classifier(input_size=X.shape[1], hidden_size=X.shape[1], drop_prob=0.5)\n",
        "classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": "Classifier(\n  (fc1): Linear(in_features=14, out_features=14, bias=True)\n  (fcout): Linear(in_features=14, out_features=1, bias=True)\n  (dropout): Dropout(p=0.5, inplace=False)\n)"
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classifier.save()\n",
        "classifier.load()\n",
        "classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(params=classifier.parameters(), lr=0.003)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "4943927092668, Val g-mean: 0.7063259078411576\nEpoch: 16, Train loss: 0.533887132342696, Train g-mean: 0.7148166078730956, Val g-mean: 0.6853484394299167\nEpoch: 17, Train loss: 0.5388492127378796, Train g-mean: 0.7153038982132565, Val g-mean: 0.7008888779396586\nEpoch: 18, Train loss: 0.5437692832383372, Train g-mean: 0.7189026585160458, Val g-mean: 0.7050902375901523\nEpoch: 19, Train loss: 0.5305109337377862, Train g-mean: 0.7194146687022798, Val g-mean: 0.7007549640853871\nEpoch: 20, Train loss: 0.525141663904021, Train g-mean: 0.7199964590435036, Val g-mean: 0.7041071601712149\nEpoch: 21, Train loss: 0.5179915877876903, Train g-mean: 0.7223226349934385, Val g-mean: 0.6991460142921941\nEpoch: 22, Train loss: 0.5366711228117284, Train g-mean: 0.7279367746845405, Val g-mean: 0.7040823682746312\nEpoch: 23, Train loss: 0.5320198905220519, Train g-mean: 0.7266820529672722, Val g-mean: 0.7117198241746766\nEpoch: 24, Train loss: 0.5221068382680093, Train g-mean: 0.7373415904234798, Val g-mean: 0.7166140069349527\nEpoch: 25, Train loss: 0.5300765414338015, Train g-mean: 0.7346094641631722, Val g-mean: 0.7065351539183741\nEpoch: 26, Train loss: 0.5278755833680456, Train g-mean: 0.733580237707262, Val g-mean: 0.724056702425357\nEpoch: 27, Train loss: 0.5181320909587224, Train g-mean: 0.7346537329796443, Val g-mean: 0.7289098204771011\nEpoch: 28, Train loss: 0.5214167567375433, Train g-mean: 0.7377221519586753, Val g-mean: 0.7150050956538815\nEpoch: 29, Train loss: 0.5198387079021872, Train g-mean: 0.7410634871746505, Val g-mean: 0.73311353915126\nEpoch: 30, Train loss: 0.5269506334995284, Train g-mean: 0.7394963339864955, Val g-mean: 0.7087725891009996\nEpoch: 31, Train loss: 0.5244646319311823, Train g-mean: 0.7465180037848707, Val g-mean: 0.7319339879906193\nEpoch: 32, Train loss: 0.5177050369430225, Train g-mean: 0.7434218105012295, Val g-mean: 0.7343923409882307\nEpoch: 33, Train loss: 0.5199625028650272, Train g-mean: 0.7415221119000598, Val g-mean: 0.7235960861573568\nEpoch: 34, Train loss: 0.525862974189152, Train g-mean: 0.7436388649095683, Val g-mean: 0.7207969438104053\nEpoch: 35, Train loss: 0.5177706047985678, Train g-mean: 0.7404023247692738, Val g-mean: 0.7208403455487913\nEpoch: 36, Train loss: 0.5144973147663819, Train g-mean: 0.7443816582902868, Val g-mean: 0.7291236683009938\nEpoch: 37, Train loss: 0.5200758077869577, Train g-mean: 0.7434618728215476, Val g-mean: 0.7223449785400344\nEpoch: 38, Train loss: 0.5065830938807901, Train g-mean: 0.7468661776468857, Val g-mean: 0.735993460580053\nEpoch: 39, Train loss: 0.5002932218312925, Train g-mean: 0.7475069159081484, Val g-mean: 0.7253404528134202\nEpoch: 40, Train loss: 0.5098775985964696, Train g-mean: 0.7477147813938007, Val g-mean: 0.7231369735182369\nEpoch: 41, Train loss: 0.5117473668396777, Train g-mean: 0.7431807088195158, Val g-mean: 0.7443389308986086\nEpoch: 42, Train loss: 0.5113321928273366, Train g-mean: 0.7484746088508841, Val g-mean: 0.7249180585139728\nEpoch: 43, Train loss: 0.5124269439638042, Train g-mean: 0.7431312967524442, Val g-mean: 0.7237726591708937\nEpoch: 44, Train loss: 0.5131806465623648, Train g-mean: 0.7500852128974101, Val g-mean: 0.7236503478158411\nEpoch: 45, Train loss: 0.5105520275115891, Train g-mean: 0.7507137919521939, Val g-mean: 0.7347871654162076\nEpoch: 46, Train loss: 0.5144346040163719, Train g-mean: 0.7508735331714629, Val g-mean: 0.730004008673206\nEpoch: 47, Train loss: 0.5071950362611182, Train g-mean: 0.7482255091973338, Val g-mean: 0.7258083881958758\nEpoch: 48, Train loss: 0.5134531412326451, Train g-mean: 0.750829730855052, Val g-mean: 0.7251640011616425\nEpoch: 49, Train loss: 0.5098257474736201, Train g-mean: 0.7481450214069182, Val g-mean: 0.7453418365534545\nEpoch: 50, Train loss: 0.5095839770515612, Train g-mean: 0.753457834230434, Val g-mean: 0.7288238563954559\nEpoch: 51, Train loss: 0.5213556448330476, Train g-mean: 0.7518255573122272, Val g-mean: 0.741506829149843\nEpoch: 52, Train loss: 0.5107587038750804, Train g-mean: 0.7516263869011806, Val g-mean: 0.7623712684862664\nEpoch: 53, Train loss: 0.512946196110743, Train g-mean: 0.7494339296257442, Val g-mean: 0.7572673501262878\nEpoch: 54, Train loss: 0.5139758940861231, Train g-mean: 0.7516090317039764, Val g-mean: 0.7594073911408593\nEpoch: 55, Train loss: 0.515749998590512, Train g-mean: 0.7505850587094257, Val g-mean: 0.7437957047605465\nEpoch: 56, Train loss: 0.5052785790995885, Train g-mean: 0.7532690462163599, Val g-mean: 0.7474629054820863\nEpoch: 57, Train loss: 0.5110135650245778, Train g-mean: 0.7505067278506324, Val g-mean: 0.7471919478670946\nEpoch: 58, Train loss: 0.509689413988904, Train g-mean: 0.7565408228661583, Val g-mean: 0.7473013085959587\nEpoch: 59, Train loss: 0.5005509180499087, Train g-mean: 0.7509356614438125, Val g-mean: 0.7512351008986996\nEpoch: 60, Train loss: 0.5089822972909076, Train g-mean: 0.7527728218522223, Val g-mean: 0.7532430170746774\nEpoch: 61, Train loss: 0.5150122737071893, Train g-mean: 0.7525413232347448, Val g-mean: 0.7500011974807529\nEpoch: 62, Train loss: 0.5071546012985635, Train g-mean: 0.7557005999812604, Val g-mean: 0.7393133105756756\nEpoch: 63, Train loss: 0.5065244942720019, Train g-mean: 0.7578860278275558, Val g-mean: 0.7455906543384626\nEpoch: 64, Train loss: 0.495346944991376, Train g-mean: 0.7509199712549138, Val g-mean: 0.7432861686326143\nEpoch: 65, Train loss: 0.5054814628002057, Train g-mean: 0.7525046069941277, Val g-mean: 0.7385953143057409\nEpoch: 66, Train loss: 0.5087201205436566, Train g-mean: 0.7555440348022062, Val g-mean: 0.7479740231686082\nEpoch: 67, Train loss: 0.5117665951718325, Train g-mean: 0.7589164945827819, Val g-mean: 0.73526607718319\nEpoch: 68, Train loss: 0.49940745896267547, Train g-mean: 0.7583890874636271, Val g-mean: 0.733497008458934\nEpoch: 69, Train loss: 0.4996084396563001, Train g-mean: 0.7584257141039411, Val g-mean: 0.7486205680957061\nEpoch: 70, Train loss: 0.5046965282038636, Train g-mean: 0.7559445802423986, Val g-mean: 0.7489770835843174\nEpoch: 71, Train loss: 0.5109439314861943, Train g-mean: 0.7504218405512469, Val g-mean: 0.7543117523944834\nEpoch: 72, Train loss: 0.5093804932118167, Train g-mean: 0.761388550243221, Val g-mean: 0.7334876452450977\nEpoch: 73, Train loss: 0.5100542978746575, Train g-mean: 0.7549260170061846, Val g-mean: 0.7590331791402961\nEpoch: 74, Train loss: 0.4988709275790016, Train g-mean: 0.7573295133067361, Val g-mean: 0.7513427354710377\nEpoch: 75, Train loss: 0.5068393239125892, Train g-mean: 0.7585259231293112, Val g-mean: 0.7442875430051399\nEpoch: 76, Train loss: 0.4993950965459336, Train g-mean: 0.7535261399168505, Val g-mean: 0.7559035885968219\nEpoch: 77, Train loss: 0.5039121900373101, Train g-mean: 0.7589906421033292, Val g-mean: 0.7479409536490579\nEpoch: 78, Train loss: 0.49906783690897266, Train g-mean: 0.7598539427999164, Val g-mean: 0.7534826257190037\nEpoch: 79, Train loss: 0.5013140278170675, Train g-mean: 0.7611344073481371, Val g-mean: 0.7485259549017527\nEpoch: 80, Train loss: 0.5050569511492699, Train g-mean: 0.7600002242610002, Val g-mean: 0.7309848265350709\nEpoch: 81, Train loss: 0.5010241431034734, Train g-mean: 0.758417290336048, Val g-mean: 0.7560401589428881\nEpoch: 82, Train loss: 0.5012776367793379, Train g-mean: 0.7591817634808085, Val g-mean: 0.749013787948092\nEpoch: 83, Train loss: 0.5016587157083761, Train g-mean: 0.7591842763390687, Val g-mean: 0.753129124103079\nEpoch: 84, Train loss: 0.5028869044178768, Train g-mean: 0.764309904957661, Val g-mean: 0.744719924657501\nEpoch: 85, Train loss: 0.5060831217233412, Train g-mean: 0.7607085999634402, Val g-mean: 0.7522843646935506\nEpoch: 86, Train loss: 0.5065726687804009, Train g-mean: 0.7594272733720953, Val g-mean: 0.7552111024147654\nEpoch: 87, Train loss: 0.5002226597152123, Train g-mean: 0.7626531595183661, Val g-mean: 0.749565910311478\nEpoch: 88, Train loss: 0.4945009299902759, Train g-mean: 0.7630250045645282, Val g-mean: 0.7442697619233742\nEpoch: 89, Train loss: 0.5068675437455682, Train g-mean: 0.7572141433899716, Val g-mean: 0.7553232248909413\nEpoch: 90, Train loss: 0.5030673960805128, Train g-mean: 0.7596045294407889, Val g-mean: 0.7511044423562991\nEpoch: 91, Train loss: 0.5017649315650982, Train g-mean: 0.7637604906757095, Val g-mean: 0.749861547074852\nEpoch: 92, Train loss: 0.49061412819388267, Train g-mean: 0.7612214576268219, Val g-mean: 0.7406616590112832\nEpoch: 93, Train loss: 0.509319288818667, Train g-mean: 0.7609701449766645, Val g-mean: 0.7536079116695026\nEpoch: 94, Train loss: 0.500403254631326, Train g-mean: 0.756872724610085, Val g-mean: 0.746861181424616\nEpoch: 95, Train loss: 0.502837982539732, Train g-mean: 0.7650652491317705, Val g-mean: 0.7457592403901786\nEpoch: 96, Train loss: 0.5049954790006222, Train g-mean: 0.7602496219010677, Val g-mean: 0.7581677045941093\nEpoch: 97, Train loss: 0.5052237728455597, Train g-mean: 0.7602826271909914, Val g-mean: 0.7525864547059675\nEpoch: 98, Train loss: 0.5064929752804657, Train g-mean: 0.7580019086385643, Val g-mean: 0.7523620600457211\nEpoch: 99, Train loss: 0.501414681223382, Train g-mean: 0.7660211644024988, Val g-mean: 0.7499754707128644\nEpoch: 100, Train loss: 0.49849983818287474, Train g-mean: 0.7618924880425536, Val g-mean: 0.7428698929684927\nEpoch: 101, Train loss: 0.49452959737854163, Train g-mean: 0.7653616351014652, Val g-mean: 0.7372202536581878\nEpoch: 102, Train loss: 0.49696374338357113, Train g-mean: 0.7638662446980607, Val g-mean: 0.7435953722458752\nEpoch: 103, Train loss: 0.500033072205345, Train g-mean: 0.7642272617119125, Val g-mean: 0.755990583368842\nEpoch: 104, Train loss: 0.5012756540509902, Train g-mean: 0.7650096671010349, Val g-mean: 0.7442398977298147\nEpoch: 105, Train loss: 0.49892404765647375, Train g-mean: 0.7631542536616018, Val g-mean: 0.7586046190288336\nEpoch: 106, Train loss: 0.493053777973837, Train g-mean: 0.7657093923425948, Val g-mean: 0.749501853325722\nEpoch: 107, Train loss: 0.49896293821650556, Train g-mean: 0.7589697294801566, Val g-mean: 0.755879598181215\nEpoch: 108, Train loss: 0.5001564834595854, Train g-mean: 0.7630129655617581, Val g-mean: 0.7515154069524587\nEpoch: 109, Train loss: 0.5081048626065294, Train g-mean: 0.7576192147959209, Val g-mean: 0.7592343325834167\nEpoch: 110, Train loss: 0.5087989026533707, Train g-mean: 0.7671457761747236, Val g-mean: 0.7482925195804674\nEpoch: 111, Train loss: 0.49421169947381227, Train g-mean: 0.769719830796855, Val g-mean: 0.7416919822236616\nEpoch: 112, Train loss: 0.4994534199403849, Train g-mean: 0.7646254222555737, Val g-mean: 0.7605320535807673\nEpoch: 113, Train loss: 0.5040434181866444, Train g-mean: 0.7684115375235729, Val g-mean: 0.7535214285319551\nEpoch: 114, Train loss: 0.49664727343043186, Train g-mean: 0.7653116680550742, Val g-mean: 0.7543076200629159\nEpoch: 115, Train loss: 0.5044842821831222, Train g-mean: 0.7639767729888373, Val g-mean: 0.7598799939209735\nEpoch: 116, Train loss: 0.5002905263875025, Train g-mean: 0.7590383246810043, Val g-mean: 0.7523524880224987\nEpoch: 117, Train loss: 0.5029063729813201, Train g-mean: 0.7623090632405355, Val g-mean: 0.7470734221214923\nEpoch: 118, Train loss: 0.5040141149233157, Train g-mean: 0.7650639690557882, Val g-mean: 0.7549411432770137\nEpoch: 119, Train loss: 0.49454273639644314, Train g-mean: 0.767323083619079, Val g-mean: 0.7504719787214441\nEpoch: 120, Train loss: 0.49060250139609873, Train g-mean: 0.7680974885734247, Val g-mean: 0.7457328392687432\nEpoch: 121, Train loss: 0.4998546479771449, Train g-mean: 0.7665444820490714, Val g-mean: 0.7344060054322519\nEpoch: 122, Train loss: 0.4943758970876177, Train g-mean: 0.7640627115907627, Val g-mean: 0.7450912823477085\nEpoch: 123, Train loss: 0.498401697641255, Train g-mean: 0.7670754955615781, Val g-mean: 0.7420927111242496\nEpoch: 124, Train loss: 0.5055331032959351, Train g-mean: 0.7645188678789472, Val g-mean: 0.7549423995591382\nEpoch: 125, Train loss: 0.5009343274345275, Train g-mean: 0.7650796347370638, Val g-mean: 0.7608657342283189\nEpoch: 126, Train loss: 0.5059303852344917, Train g-mean: 0.7682116761009394, Val g-mean: 0.7589440364803609\nEpoch: 127, Train loss: 0.5016802514324067, Train g-mean: 0.7644246137257136, Val g-mean: 0.7435375324342555\nEpoch: 128, Train loss: 0.5000420147632186, Train g-mean: 0.7670763875767562, Val g-mean: 0.7449800333789927\nEpoch: 129, Train loss: 0.49931182050440504, Train g-mean: 0.7670114724028717, Val g-mean: 0.7410999834601895\nEpoch: 130, Train loss: 0.4914900229742604, Train g-mean: 0.7675532567864385, Val g-mean: 0.7574142504956511\nEpoch: 131, Train loss: 0.4863761222304705, Train g-mean: 0.7679684318643095, Val g-mean: 0.7504954594715346\nEpoch: 132, Train loss: 0.5016972566500371, Train g-mean: 0.7699532687938733, Val g-mean: 0.7638558941828245\nEpoch: 133, Train loss: 0.49960185000178026, Train g-mean: 0.7706698641424115, Val g-mean: 0.7380893260660997\nEpoch: 134, Train loss: 0.49839235235852747, Train g-mean: 0.7698901375652876, Val g-mean: 0.7444074095730829\nEpoch: 135, Train loss: 0.49850375875461767, Train g-mean: 0.7688548911521212, Val g-mean: 0.7586948234373672\nEpoch: 136, Train loss: 0.4983162576331371, Train g-mean: 0.7700390448898004, Val g-mean: 0.751352581478147\nEpoch: 137, Train loss: 0.49047085853692207, Train g-mean: 0.7677462478447797, Val g-mean: 0.7580494374895936\nEpoch: 138, Train loss: 0.4905070929343772, Train g-mean: 0.7702725007317104, Val g-mean: 0.7524427541455726\nEpoch: 139, Train loss: 0.4904040597330079, Train g-mean: 0.7726558642056721, Val g-mean: 0.7499717392016455\nEpoch: 140, Train loss: 0.4918757801341533, Train g-mean: 0.7708315826526572, Val g-mean: 0.7544004796201337\nEpoch: 141, Train loss: 0.49486599374770945, Train g-mean: 0.7675427361058907, Val g-mean: 0.7559896563759976\nEpoch: 142, Train loss: 0.49998826675789687, Train g-mean: 0.7712645643402635, Val g-mean: 0.7589373624893266\nEpoch: 143, Train loss: 0.4981658259648571, Train g-mean: 0.7690710852916474, Val g-mean: 0.7464886103850626\nEpoch: 144, Train loss: 0.4979134705791108, Train g-mean: 0.7646175819465703, Val g-mean: 0.7615595690113877\nEpoch: 145, Train loss: 0.4957614803850978, Train g-mean: 0.7689164169383437, Val g-mean: 0.7574346071660476\nEpoch: 146, Train loss: 0.5028775812066039, Train g-mean: 0.7647239592959755, Val g-mean: 0.7596324165221742\nEpoch: 147, Train loss: 0.4940158592834233, Train g-mean: 0.7733616171745563, Val g-mean: 0.7580161051056807\nEpoch: 148, Train loss: 0.49739128516960057, Train g-mean: 0.7717888442904353, Val g-mean: 0.759648134869846\nEpoch: 149, Train loss: 0.4990126288954871, Train g-mean: 0.7716129422697378, Val g-mean: 0.7533180826093047\nEpoch: 150, Train loss: 0.4996432708027266, Train g-mean: 0.7709462997193891, Val g-mean: 0.7445410991448913\nEpoch: 151, Train loss: 0.4942858891342449, Train g-mean: 0.773427133954756, Val g-mean: 0.7555536673454388\nEpoch: 152, Train loss: 0.49122503746355706, Train g-mean: 0.7724212384911266, Val g-mean: 0.7568809227284647\nEpoch: 153, Train loss: 0.49121425419648673, Train g-mean: 0.7679977813738036, Val g-mean: 0.7618620710138667\nEpoch: 154, Train loss: 0.4980175945267792, Train g-mean: 0.7703630049535363, Val g-mean: 0.7506346288014973\nEpoch: 155, Train loss: 0.48933158549916783, Train g-mean: 0.7757514603394925, Val g-mean: 0.7548488859972348\nEpoch: 156, Train loss: 0.49260518317891444, Train g-mean: 0.7679129125304294, Val g-mean: 0.7640143856893661\nEpoch: 157, Train loss: 0.4819415000100481, Train g-mean: 0.7707166652781927, Val g-mean: 0.7538544695360637\nEpoch: 158, Train loss: 0.49274740079690854, Train g-mean: 0.7746916043763477, Val g-mean: 0.7471962186887547\nEpoch: 159, Train loss: 0.49017779226091995, Train g-mean: 0.7726462879184867, Val g-mean: 0.7533454823718111\nEpoch: 160, Train loss: 0.48904507010997317, Train g-mean: 0.7714466489596717, Val g-mean: 0.769948991343284\nEpoch: 161, Train loss: 0.49002974751798206, Train g-mean: 0.7762548529509786, Val g-mean: 0.7538829095686721\nEpoch: 162, Train loss: 0.4973866746266711, Train g-mean: 0.7658821382322611, Val g-mean: 0.7658589829923753\nEpoch: 163, Train loss: 0.49239189108723697, Train g-mean: 0.7739625272643349, Val g-mean: 0.764060907944067\nEpoch: 164, Train loss: 0.49619440439219786, Train g-mean: 0.7690284293307769, Val g-mean: 0.76647607011735\nEpoch: 165, Train loss: 0.4918655321012209, Train g-mean: 0.770610899814446, Val g-mean: 0.7477103008734797\nEpoch: 166, Train loss: 0.4909248262111994, Train g-mean: 0.7731738768142207, Val g-mean: 0.752865328312137\nEpoch: 167, Train loss: 0.49097919325553463, Train g-mean: 0.7724048166539523, Val g-mean: 0.7515634569930035\nEpoch: 168, Train loss: 0.49603051367988293, Train g-mean: 0.771946391816614, Val g-mean: 0.7580009258634565\nEpoch: 169, Train loss: 0.48907147043645033, Train g-mean: 0.7729053691964397, Val g-mean: 0.7585939842531068\nEpoch: 170, Train loss: 0.4912786179332859, Train g-mean: 0.7735443737484624, Val g-mean: 0.7635452166069605\nEpoch: 171, Train loss: 0.489846365769523, Train g-mean: 0.775139044948979, Val g-mean: 0.7387571826010616\nEpoch: 172, Train loss: 0.4939780826767221, Train g-mean: 0.7692245433695255, Val g-mean: 0.7603172718401777\nEpoch: 173, Train loss: 0.49807755115273433, Train g-mean: 0.7747451985707544, Val g-mean: 0.7532252634884828\nEpoch: 174, Train loss: 0.4960822572887805, Train g-mean: 0.7726050216702851, Val g-mean: 0.7400477614761684\nEpoch: 175, Train loss: 0.4949338503912122, Train g-mean: 0.7725606953956162, Val g-mean: 0.7523674877459962\nEpoch: 176, Train loss: 0.5009765244248191, Train g-mean: 0.7697320008145656, Val g-mean: 0.7586175565400718\nEpoch: 177, Train loss: 0.493815382381006, Train g-mean: 0.7752048579184214, Val g-mean: 0.7620009881979616\nEpoch: 178, Train loss: 0.48946554604242687, Train g-mean: 0.7725483263282402, Val g-mean: 0.7636842249054084\nEpoch: 179, Train loss: 0.4892724990642746, Train g-mean: 0.7751729496886945, Val g-mean: 0.7572368267667873\nEpoch: 180, Train loss: 0.48934488935057846, Train g-mean: 0.7748111885666136, Val g-mean: 0.755219191007129\nEpoch: 181, Train loss: 0.4931892718739398, Train g-mean: 0.7737624282624302, Val g-mean: 0.7562801821184808\nEpoch: 182, Train loss: 0.4908189357244794, Train g-mean: 0.7748008090436748, Val g-mean: 0.7497232781136691\nEpoch: 183, Train loss: 0.4896386559553226, Train g-mean: 0.7748835112971019, Val g-mean: 0.7494344710372756\nEpoch: 184, Train loss: 0.4814137731128763, Train g-mean: 0.7763382952724684, Val g-mean: 0.7561677202969286\nEpoch: 185, Train loss: 0.4951428842478346, Train g-mean: 0.7726637999566814, Val g-mean: 0.7522062295152403\nEpoch: 186, Train loss: 0.49337370612767956, Train g-mean: 0.7753895357307784, Val g-mean: 0.7531277523121717\nEpoch: 187, Train loss: 0.4943797135702725, Train g-mean: 0.7761189772126937, Val g-mean: 0.7584963103312851\nEpoch: 188, Train loss: 0.48770064805360047, Train g-mean: 0.7777288322516606, Val g-mean: 0.7468109332910547\nEpoch: 189, Train loss: 0.4951655036971723, Train g-mean: 0.7736647158457042, Val g-mean: 0.759077226545902\nEpoch: 190, Train loss: 0.4919084642854679, Train g-mean: 0.7708493132369404, Val g-mean: 0.774369283489887\nEpoch: 191, Train loss: 0.4861686018057073, Train g-mean: 0.7702406841414634, Val g-mean: 0.7543713322393663\nEpoch: 192, Train loss: 0.49250253320786, Train g-mean: 0.7719708721185261, Val g-mean: 0.7704271723389335\nEpoch: 193, Train loss: 0.48952058357889194, Train g-mean: 0.7756218675778335, Val g-mean: 0.7638548069852167\nEpoch: 194, Train loss: 0.49409175678480766, Train g-mean: 0.7752675029257085, Val g-mean: 0.7517488434837531\nEpoch: 195, Train loss: 0.48173109149804755, Train g-mean: 0.777472858217892, Val g-mean: 0.7565629690655483\nEpoch: 196, Train loss: 0.49009470657786836, Train g-mean: 0.7773922356516524, Val g-mean: 0.762431089222705\nEpoch: 197, Train loss: 0.4841810012560093, Train g-mean: 0.7805559921157544, Val g-mean: 0.7480118036478419\nEpoch: 198, Train loss: 0.48565755303665636, Train g-mean: 0.7722237543959622, Val g-mean: 0.7584605161267206\nEpoch: 199, Train loss: 0.4936300370721464, Train g-mean: 0.7796284629022172, Val g-mean: 0.7632556180558427\n"
        }
      ],
      "source": [
        "pipeline = Pipeline(steps=[], classifier=classifier, optimizer=optimizer, criterion=criterion, max_epochs=200, fading_factor=0.9999)\n",
        "pipeline.train(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "  def evaluate(pipeline):\n",
        "    train_gmean, train_recalls = pipeline.evaluate(X_train, y_train)\n",
        "    test_gmean, test_recalls = pipeline.evaluate(X_test, y_test)\n",
        "    print('Epoch: {}'.format(pipeline.epoch))\n",
        "    print('Train g-mean: {}, recalls: {}'.format(train_gmean.item(), train_recalls))\n",
        "    print('Test g-mean: {}, recalls: {}'.format(test_gmean.item(), test_recalls))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Last classifier\nEpoch: 199\nTrain g-mean: 0.778112418933652, recalls: [0.70586217 0.85775801]\nTest g-mean: 0.7378359396166241, recalls: [0.75457581 0.72146743]\n"
        }
      ],
      "source": [
        "print('Last classifier')\n",
        "evaluate(pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Best classifier\nEpoch: 190\nTrain g-mean: 0.7710122005465565, recalls: [0.68699519 0.86530419]\nTest g-mean: 0.7315674565176691, recalls: [0.76198011 0.70236865]\n"
        }
      ],
      "source": [
        "print('Best classifier')\n",
        "pipeline.load()\n",
        "evaluate(pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "L_t4Gh43GYWl"
      },
      "outputs": [],
      "source": []
    }
  ]
}