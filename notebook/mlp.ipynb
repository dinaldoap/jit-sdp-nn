{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JIT-SDP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dinaldoap/jit-sdp-nn/blob/master/notebook/mlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXwvNEAQE6NO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import random\n",
        "from scipy.stats import mstats"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAupgTDfmJ_I",
        "colab_type": "code",
        "outputId": "8e89721c-12b8-4bd7-a6e4-11f684336fe7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/dinaldoap/jit-sdp-data/master/jenkins.csv')\n",
        "df.head()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fix</th>\n",
              "      <th>ns</th>\n",
              "      <th>nd</th>\n",
              "      <th>nf</th>\n",
              "      <th>entropy</th>\n",
              "      <th>la</th>\n",
              "      <th>ld</th>\n",
              "      <th>lt</th>\n",
              "      <th>ndev</th>\n",
              "      <th>age</th>\n",
              "      <th>nuc</th>\n",
              "      <th>exp</th>\n",
              "      <th>rexp</th>\n",
              "      <th>sexp</th>\n",
              "      <th>author_date_unix_timestamp</th>\n",
              "      <th>classification</th>\n",
              "      <th>contains_bug</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>False</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.641604</td>\n",
              "      <td>9.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>426.428571</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.000093</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5171.0</td>\n",
              "      <td>30.227271</td>\n",
              "      <td>1472.714286</td>\n",
              "      <td>1555326371</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>False</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.750000</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>426.428571</td>\n",
              "      <td>100.0</td>\n",
              "      <td>6.314775</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5170.0</td>\n",
              "      <td>29.227271</td>\n",
              "      <td>1471.714286</td>\n",
              "      <td>1555326363</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>False</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.906580</td>\n",
              "      <td>15.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>96.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.034722</td>\n",
              "      <td>2.0</td>\n",
              "      <td>629.0</td>\n",
              "      <td>14.828373</td>\n",
              "      <td>414.000000</td>\n",
              "      <td>1554971763</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>False</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.058824</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1554969774</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>False</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.662506</td>\n",
              "      <td>14.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>67.000000</td>\n",
              "      <td>6.0</td>\n",
              "      <td>21.280683</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.058824</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1554967752</td>\n",
              "      <td>Feature Addition</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     fix   ns   nd  ...  author_date_unix_timestamp    classification  contains_bug\n",
              "0  False  7.0  7.0  ...                  1555326371              None         False\n",
              "1  False  7.0  7.0  ...                  1555326363              None         False\n",
              "2  False  1.0  1.0  ...                  1554971763              None         False\n",
              "3  False  1.0  1.0  ...                  1554969774              None         False\n",
              "4  False  1.0  2.0  ...                  1554967752  Feature Addition         False\n",
              "\n",
              "[5 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alizaxdPD3El",
        "colab_type": "code",
        "outputId": "e16ebdcb-5ebc-4d31-b549-71680b7bb871",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "#df = df.sample(frac=1)\n",
        "label_col = 'contains_bug'\n",
        "features_cols = ['fix', 'ns', 'nd', 'nf', 'entropy', 'la', 'ld', 'lt', 'ndev', 'age', 'nuc', 'exp', 'rexp', 'sexp', 'classification']\n",
        "x = df[features_cols]\n",
        "x['fix'] = x['fix'].astype('int')\n",
        "df_classification = pd.get_dummies(x, columns=['classification'])\n",
        "x = pd.concat([x, df_classification], axis='columns')\n",
        "x = x.drop(['classification'], axis='columns')\n",
        "x = x.values\n",
        "y = df[label_col]\n",
        "y = y.astype('category')\n",
        "y = y.cat.codes\n",
        "y = y.values"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBD1-tp2ExzQ",
        "colab_type": "code",
        "outputId": "a50ca1f4-6542-4f34-9249-02cd73fde5dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "print(x[:3])\n",
        "print(y[:3])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.00000000e+00 7.00000000e+00 7.00000000e+00 7.00000000e+00\n",
            "  2.64160417e+00 9.00000000e+00 9.00000000e+00 4.26428571e+02\n",
            "  1.00000000e+02 9.25925926e-05 1.00000000e+00 5.17100000e+03\n",
            "  3.02272705e+01 1.47271429e+03 0.00000000e+00 7.00000000e+00\n",
            "  7.00000000e+00 7.00000000e+00 2.64160417e+00 9.00000000e+00\n",
            "  9.00000000e+00 4.26428571e+02 1.00000000e+02 9.25925926e-05\n",
            "  1.00000000e+00 5.17100000e+03 3.02272705e+01 1.47271429e+03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 7.00000000e+00 7.00000000e+00 7.00000000e+00\n",
            "  2.75000000e+00 8.00000000e+00 8.00000000e+00 4.26428571e+02\n",
            "  1.00000000e+02 6.31477513e+00 2.00000000e+00 5.17000000e+03\n",
            "  2.92272705e+01 1.47171429e+03 0.00000000e+00 7.00000000e+00\n",
            "  7.00000000e+00 7.00000000e+00 2.75000000e+00 8.00000000e+00\n",
            "  8.00000000e+00 4.26428571e+02 1.00000000e+02 6.31477513e+00\n",
            "  2.00000000e+00 5.17000000e+03 2.92272705e+01 1.47171429e+03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 1.00000000e+00 2.00000000e+00\n",
            "  9.06579548e-01 1.50000000e+01 4.40000000e+01 9.60000000e+01\n",
            "  4.00000000e+00 3.47222222e-02 2.00000000e+00 6.29000000e+02\n",
            "  1.48283729e+01 4.14000000e+02 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 2.00000000e+00 9.06579548e-01 1.50000000e+01\n",
            "  4.40000000e+01 9.60000000e+01 4.00000000e+00 3.47222222e-02\n",
            "  2.00000000e+00 6.29000000e+02 1.48283729e+01 4.14000000e+02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "[0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwCn1uNTMjfG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_index = int( len(x) * 0.8 )\n",
        "test_index = int( len(x) * 0.9 )\n",
        "x_train, y_train = x[:val_index], y[:val_index]\n",
        "x_val, y_val = x[val_index:test_index], y[val_index:test_index]\n",
        "x_test, y_test = x[test_index:], y[test_index:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSqnoapE5Poq",
        "colab_type": "code",
        "outputId": "6dc1895d-e2fc-4fb2-e3c2-f70b4f5efb4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(x_train[:3])\n",
        "print(x_val[:3])\n",
        "print(x_test[:3])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.00000000e+00 7.00000000e+00 7.00000000e+00 7.00000000e+00\n",
            "  2.64160417e+00 9.00000000e+00 9.00000000e+00 4.26428571e+02\n",
            "  1.00000000e+02 9.25925926e-05 1.00000000e+00 5.17100000e+03\n",
            "  3.02272705e+01 1.47271429e+03 0.00000000e+00 7.00000000e+00\n",
            "  7.00000000e+00 7.00000000e+00 2.64160417e+00 9.00000000e+00\n",
            "  9.00000000e+00 4.26428571e+02 1.00000000e+02 9.25925926e-05\n",
            "  1.00000000e+00 5.17100000e+03 3.02272705e+01 1.47271429e+03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 7.00000000e+00 7.00000000e+00 7.00000000e+00\n",
            "  2.75000000e+00 8.00000000e+00 8.00000000e+00 4.26428571e+02\n",
            "  1.00000000e+02 6.31477513e+00 2.00000000e+00 5.17000000e+03\n",
            "  2.92272705e+01 1.47171429e+03 0.00000000e+00 7.00000000e+00\n",
            "  7.00000000e+00 7.00000000e+00 2.75000000e+00 8.00000000e+00\n",
            "  8.00000000e+00 4.26428571e+02 1.00000000e+02 6.31477513e+00\n",
            "  2.00000000e+00 5.17000000e+03 2.92272705e+01 1.47171429e+03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 1.00000000e+00 2.00000000e+00\n",
            "  9.06579548e-01 1.50000000e+01 4.40000000e+01 9.60000000e+01\n",
            "  4.00000000e+00 3.47222222e-02 2.00000000e+00 6.29000000e+02\n",
            "  1.48283729e+01 4.14000000e+02 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 2.00000000e+00 9.06579548e-01 1.50000000e+01\n",
            "  4.40000000e+01 9.60000000e+01 4.00000000e+00 3.47222222e-02\n",
            "  2.00000000e+00 6.29000000e+02 1.48283729e+01 4.14000000e+02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "[[0.00000000e+00 8.00000000e+00 8.00000000e+00 8.00000000e+00\n",
            "  2.73215889e+00 1.10000000e+01 1.10000000e+01 1.58625000e+02\n",
            "  5.00000000e+00 1.96759259e-04 1.00000000e+00 4.17300000e+03\n",
            "  2.45570735e+02 8.19125000e+02 0.00000000e+00 8.00000000e+00\n",
            "  8.00000000e+00 8.00000000e+00 2.73215889e+00 1.10000000e+01\n",
            "  1.10000000e+01 1.58625000e+02 5.00000000e+00 1.96759259e-04\n",
            "  1.00000000e+00 4.17300000e+03 2.45570735e+02 8.19125000e+02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 8.00000000e+00 8.00000000e+00 8.00000000e+00\n",
            "  2.73215889e+00 1.10000000e+01 1.10000000e+01 1.58625000e+02\n",
            "  5.00000000e+00 4.29604022e+00 5.00000000e+00 4.17200000e+03\n",
            "  2.44570735e+02 8.18125000e+02 0.00000000e+00 8.00000000e+00\n",
            "  8.00000000e+00 8.00000000e+00 2.73215889e+00 1.10000000e+01\n",
            "  1.10000000e+01 1.58625000e+02 5.00000000e+00 4.29604022e+00\n",
            "  5.00000000e+00 4.17200000e+03 2.44570735e+02 8.18125000e+02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 1.00000000e+00 1.07000000e+02\n",
            "  1.00000000e+00 5.29513889e-01 1.00000000e+00 4.17100000e+03\n",
            "  2.43572058e+02 8.40000000e+01 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.07000000e+02 1.00000000e+00 5.29513889e-01\n",
            "  1.00000000e+00 4.17100000e+03 2.43572058e+02 8.40000000e+01\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "[[0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 1.00000000e+00 2.31000000e+02\n",
            "  2.00000000e+00 8.17129630e-03 1.00000000e+00 2.29000000e+03\n",
            "  2.35138387e+02 1.96900000e+03 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 2.31000000e+02 2.00000000e+00 8.17129630e-03\n",
            "  1.00000000e+00 2.29000000e+03 2.35138387e+02 1.96900000e+03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 3.00000000e+00 4.00000000e+00\n",
            "  1.92417435e+00 1.00000000e+01 4.00000000e+00 7.95000000e+01\n",
            "  2.00000000e+00 7.59004630e-01 4.00000000e+00 2.28900000e+03\n",
            "  2.34145689e+02 1.96800000e+03 0.00000000e+00 1.00000000e+00\n",
            "  3.00000000e+00 4.00000000e+00 1.92417435e+00 1.00000000e+01\n",
            "  4.00000000e+00 7.95000000e+01 2.00000000e+00 7.59004630e-01\n",
            "  4.00000000e+00 2.28900000e+03 2.34145689e+02 1.96800000e+03\n",
            "  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 5.00000000e+00 4.00000000e+00 2.10000000e+01\n",
            "  1.00000000e+00 5.92943287e+00 1.00000000e+00 2.28800000e+03\n",
            "  2.33161190e+02 1.96700000e+03 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 5.00000000e+00\n",
            "  4.00000000e+00 2.10000000e+01 1.00000000e+00 5.92943287e+00\n",
            "  1.00000000e+00 2.28800000e+03 2.33161190e+02 1.96700000e+03\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46STsbti3AmT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean_train = x_train.mean(axis=0)\n",
        "std_train = x_train.std(axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5vvbrrK3PES",
        "colab_type": "code",
        "outputId": "d19909eb-15f6-4046-d1a7-d0988fe02aa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "print(mean_train)\n",
        "print(std_train)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2.90871477e-01 1.61432635e+00 2.60858547e+00 4.18908715e+00\n",
            " 7.43390526e-01 5.00800621e+01 2.96331523e+01 8.11968929e+02\n",
            " 2.32963538e+01 9.76994760e+01 2.02296354e+00 1.73095790e+03\n",
            " 6.53773667e+01 8.28714959e+02 2.90871477e-01 1.61432635e+00\n",
            " 2.60858547e+00 4.18908715e+00 7.43390526e-01 5.00800621e+01\n",
            " 2.96331523e+01 8.11968929e+02 2.32963538e+01 9.76994760e+01\n",
            " 2.02296354e+00 1.73095790e+03 6.53773667e+01 8.28714959e+02\n",
            " 2.90871477e-01 1.47142488e-01 3.46521852e-02 4.71373157e-01\n",
            " 1.51538660e-02 4.08068270e-02]\n",
            "[4.54164354e-01 1.62736223e+00 6.93052365e+00 2.56616350e+01\n",
            " 1.13236779e+00 5.86250473e+02 6.19852440e+02 1.15236538e+03\n",
            " 2.84287684e+01 2.37802272e+02 7.67653474e+00 2.11471132e+03\n",
            " 6.96820390e+01 1.34456679e+03 4.54164354e-01 1.62736223e+00\n",
            " 6.93052365e+00 2.56616350e+01 1.13236779e+00 5.86250473e+02\n",
            " 6.19852440e+02 1.15236538e+03 2.84287684e+01 2.37802272e+02\n",
            " 7.67653474e+00 2.11471132e+03 6.96820390e+01 1.34456679e+03\n",
            " 4.54164354e-01 3.54247902e-01 1.82897270e-01 4.99179831e-01\n",
            " 1.22164751e-01 1.97842437e-01]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2IoFA5h425Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = (x_train - mean_train) / std_train\n",
        "x_val = (x_val - mean_train) / std_train\n",
        "x_test = (x_test - mean_train) / std_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1AoKNoU5V1w",
        "colab_type": "code",
        "outputId": "38d69b2a-d82e-4641-adbb-30c5682a6f9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(x_train[:3])\n",
        "print(x_val[:3])\n",
        "print(x_test[:3])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-6.40454219e-01  3.30944983e+00  6.33633872e-01  1.09537559e-01\n",
            "   1.67632253e+00 -7.00725440e-02 -3.32872003e-02 -3.34564335e-01\n",
            "   2.69809951e+00 -4.10842935e-01 -1.33258504e-01  1.62671948e+00\n",
            "  -5.04435529e-01  4.78964178e-01 -6.40454219e-01  3.30944983e+00\n",
            "   6.33633872e-01  1.09537559e-01  1.67632253e+00 -7.00725440e-02\n",
            "  -3.32872003e-02 -3.34564335e-01  2.69809951e+00 -4.10842935e-01\n",
            "  -1.33258504e-01  1.62671948e+00 -5.04435529e-01  4.78964178e-01\n",
            "  -6.40454219e-01 -4.15365869e-01 -1.89462561e-01  1.05899079e+00\n",
            "  -1.24044505e-01 -2.06259221e-01]\n",
            " [-6.40454219e-01  3.30944983e+00  6.33633872e-01  1.09537559e-01\n",
            "   1.77204746e+00 -7.17782996e-02 -3.49004875e-02 -3.34564335e-01\n",
            "   2.69809951e+00 -3.84288594e-01 -2.99139370e-03  1.62624660e+00\n",
            "  -5.18786429e-01  4.78220444e-01 -6.40454219e-01  3.30944983e+00\n",
            "   6.33633872e-01  1.09537559e-01  1.77204746e+00 -7.17782996e-02\n",
            "  -3.49004875e-02 -3.34564335e-01  2.69809951e+00 -3.84288594e-01\n",
            "  -2.99139370e-03  1.62624660e+00 -5.18786429e-01  4.78220444e-01\n",
            "  -6.40454219e-01 -4.15365869e-01 -1.89462561e-01  1.05899079e+00\n",
            "  -1.24044505e-01 -2.06259221e-01]\n",
            " [-6.40454219e-01 -3.77498225e-01 -2.32101577e-01 -8.53058331e-02\n",
            "   1.44113090e-01 -5.98380107e-02  2.31778513e-02 -6.21303747e-01\n",
            "  -6.78761509e-01 -4.10697311e-01 -2.99139370e-03 -5.21091408e-01\n",
            "  -7.25423575e-01 -3.08437605e-01 -6.40454219e-01 -3.77498225e-01\n",
            "  -2.32101577e-01 -8.53058331e-02  1.44113090e-01 -5.98380107e-02\n",
            "   2.31778513e-02 -6.21303747e-01 -6.78761509e-01 -4.10697311e-01\n",
            "  -2.99139370e-03 -5.21091408e-01 -7.25423575e-01 -3.08437605e-01\n",
            "  -6.40454219e-01 -4.15365869e-01 -1.89462561e-01  1.05899079e+00\n",
            "  -1.24044505e-01 -2.06259221e-01]]\n",
            "[[-0.64045422  3.92394118  0.77792311  0.14850624  1.75629188 -0.06666103\n",
            "  -0.03006063 -0.566959   -0.64358587 -0.4108425  -0.1332585   1.15478745\n",
            "   2.58593708 -0.00713238 -0.64045422  3.92394118  0.77792311  0.14850624\n",
            "   1.75629188 -0.06666103 -0.03006063 -0.566959   -0.64358587 -0.4108425\n",
            "  -0.1332585   1.15478745  2.58593708 -0.00713238 -0.64045422 -0.41536587\n",
            "  -0.18946256  1.05899079 -0.1240445  -0.20625922]\n",
            " [-0.64045422  3.92394118  0.77792311  0.14850624  1.75629188 -0.06666103\n",
            "  -0.03006063 -0.566959   -0.64358587 -0.39277773  0.38780994  1.15431457\n",
            "   2.57158618 -0.00787611 -0.64045422  3.92394118  0.77792311  0.14850624\n",
            "   1.75629188 -0.06666103 -0.03006063 -0.566959   -0.64358587 -0.39277773\n",
            "   0.38780994  1.15431457  2.57158618 -0.00787611 -0.64045422 -0.41536587\n",
            "  -0.18946256  1.05899079 -0.1240445  -0.20625922]\n",
            " [-0.64045422 -0.37749822 -0.23210158 -0.12427451 -0.65649211 -0.08371859\n",
            "  -0.0461935  -0.61175816 -0.78428842 -0.40861663 -0.1332585   1.1538417\n",
            "   2.55725426 -0.55386982 -0.64045422 -0.37749822 -0.23210158 -0.12427451\n",
            "  -0.65649211 -0.08371859 -0.0461935  -0.61175816 -0.78428842 -0.40861663\n",
            "  -0.1332585   1.1538417   2.55725426 -0.55386982 -0.64045422 -0.41536587\n",
            "  -0.18946256  1.05899079 -0.1240445  -0.20625922]]\n",
            "[[-0.64045422 -0.37749822 -0.23210158 -0.12427451 -0.65649211 -0.08371859\n",
            "  -0.0461935  -0.5041534  -0.74911278 -0.41080896 -0.1332585   0.26435859\n",
            "   2.43622349  0.84806872 -0.64045422 -0.37749822 -0.23210158 -0.12427451\n",
            "  -0.65649211 -0.08371859 -0.0461935  -0.5041534  -0.74911278 -0.41080896\n",
            "  -0.1332585   0.26435859  2.43622349  0.84806872 -0.64045422 -0.41536587\n",
            "  -0.18946256  1.05899079 -0.1240445  -0.20625922]\n",
            " [-0.64045422 -0.37749822  0.05647691 -0.00736848  1.04275646 -0.06836679\n",
            "  -0.04135364 -0.63562212 -0.74911278 -0.40765158  0.25754283  0.26388571\n",
            "   2.42197738  0.84732499 -0.64045422 -0.37749822  0.05647691 -0.00736848\n",
            "   1.04275646 -0.06836679 -0.04135364 -0.63562212 -0.74911278 -0.40765158\n",
            "   0.25754283  0.26388571  2.42197738  0.84732499 -0.64045422  2.40751606\n",
            "  -0.18946256 -0.94429528 -0.1240445  -0.20625922]\n",
            " [-0.64045422 -0.37749822 -0.23210158 -0.12427451 -0.65649211 -0.07689557\n",
            "  -0.04135364 -0.68638727 -0.78428842 -0.38590903 -0.1332585   0.26341283\n",
            "   2.40784893  0.84658126 -0.64045422 -0.37749822 -0.23210158 -0.12427451\n",
            "  -0.65649211 -0.07689557 -0.04135364 -0.68638727 -0.78428842 -0.38590903\n",
            "  -0.1332585   0.26341283  2.40784893  0.84658126 -0.64045422 -0.41536587\n",
            "   5.27808762 -0.94429528 -0.1240445  -0.20625922]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNcuSWBQp2Ua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_sampler(y):\n",
        "  _, counts = np.unique(y, return_counts=True)\n",
        "  n_samples = len(y)\n",
        "  class_weights = n_samples / counts\n",
        "  weights = class_weights[y]\n",
        "  fade = reversed(range(n_samples))\n",
        "  fading_factor = 1 #0.999999\n",
        "  fade = [fading_factor**x for x in (fade)]\n",
        "  weights = weights * fade\n",
        "  return data.WeightedRandomSampler(weights=weights, num_samples=n_samples, replacement=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fh6O6Nz6uC51",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sampler = create_sampler(y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iP6L6GN22_bR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, y_train = torch.from_numpy(x_train), torch.from_numpy(y_train)\n",
        "x_val, y_val = torch.from_numpy(x_val), torch.from_numpy(y_val)\n",
        "x_test, y_test = torch.from_numpy(x_test), torch.from_numpy(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XnnrzRjL3IP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = data.TensorDataset(x_train, y_train)\n",
        "val_dataset = data.TensorDataset(x_val, y_val)\n",
        "test_dataset = data.TensorDataset(x_test, y_test)\n",
        "\n",
        "train_dataloader = data.DataLoader(train_dataset, batch_size=512, sampler=sampler)\n",
        "val_dataloader = data.DataLoader(val_dataset, batch_size=32)\n",
        "test_dataloader = data.DataLoader(test_dataset, batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHoY9-JcORo-",
        "colab_type": "code",
        "outputId": "dbd8e37f-b349-4ac6-b22d-40243d9082ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        }
      },
      "source": [
        "inputs, targets = next(iter(train_dataloader))\n",
        "print(inputs[:3])\n",
        "print(targets[:3])\n",
        "\n",
        "inputs, targets = next(iter(val_dataloader))\n",
        "print(inputs[:3])\n",
        "print(targets[:3])\n",
        "\n",
        "inputs, targets = next(iter(test_dataloader))\n",
        "print(inputs[:3])\n",
        "print(targets[:3])"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-6.4045e-01,  2.3699e-01, -8.7812e-02, -8.5306e-02,  2.0096e-01,\n",
            "         -8.0307e-02, -4.4580e-02,  6.2179e-01,  2.7333e+00, -4.0949e-01,\n",
            "         -2.9914e-03,  1.0716e+00,  2.9129e-01,  8.7410e-01, -6.4045e-01,\n",
            "          2.3699e-01, -8.7812e-02, -8.5306e-02,  2.0096e-01, -8.0307e-02,\n",
            "         -4.4580e-02,  6.2179e-01,  2.7333e+00, -4.0949e-01, -2.9914e-03,\n",
            "          1.0716e+00,  2.9129e-01,  8.7410e-01, -6.4045e-01,  2.4075e+00,\n",
            "         -1.8946e-01, -9.4430e-01, -1.2404e-01, -2.0626e-01],\n",
            "        [-6.4045e-01, -3.7750e-01, -2.3210e-01, -1.2427e-01, -6.5649e-01,\n",
            "         -8.0307e-02, -4.6193e-02, -6.1089e-01, -2.5665e-01, -3.8140e-01,\n",
            "         -1.3326e-01, -8.1475e-01, -9.3211e-01, -6.1262e-01, -6.4045e-01,\n",
            "         -3.7750e-01, -2.3210e-01, -1.2427e-01, -6.5649e-01, -8.0307e-02,\n",
            "         -4.6193e-02, -6.1089e-01, -2.5665e-01, -3.8140e-01, -1.3326e-01,\n",
            "         -8.1475e-01, -9.3211e-01, -6.1262e-01, -6.4045e-01, -4.1537e-01,\n",
            "         -1.8946e-01,  1.0590e+00, -1.2404e-01, -2.0626e-01],\n",
            "        [ 1.5614e+00, -3.7750e-01, -2.3210e-01, -1.2427e-01, -6.5649e-01,\n",
            "         -8.2013e-02, -4.4580e-02, -6.9246e-01, -6.0841e-01,  6.0880e+00,\n",
            "         -1.3326e-01, -8.1853e-01, -9.3822e-01, -6.1634e-01,  1.5614e+00,\n",
            "         -3.7750e-01, -2.3210e-01, -1.2427e-01, -6.5649e-01, -8.2013e-02,\n",
            "         -4.4580e-02, -6.9246e-01, -6.0841e-01,  6.0880e+00, -1.3326e-01,\n",
            "         -8.1853e-01, -9.3822e-01, -6.1634e-01,  1.5614e+00, -4.1537e-01,\n",
            "         -1.8946e-01, -9.4430e-01, -1.2404e-01, -2.0626e-01]],\n",
            "       dtype=torch.float64)\n",
            "tensor([0, 0, 0], dtype=torch.int8)\n",
            "tensor([[-0.6405,  3.9239,  0.7779,  0.1485,  1.7563, -0.0667, -0.0301, -0.5670,\n",
            "         -0.6436, -0.4108, -0.1333,  1.1548,  2.5859, -0.0071, -0.6405,  3.9239,\n",
            "          0.7779,  0.1485,  1.7563, -0.0667, -0.0301, -0.5670, -0.6436, -0.4108,\n",
            "         -0.1333,  1.1548,  2.5859, -0.0071, -0.6405, -0.4154, -0.1895,  1.0590,\n",
            "         -0.1240, -0.2063],\n",
            "        [-0.6405,  3.9239,  0.7779,  0.1485,  1.7563, -0.0667, -0.0301, -0.5670,\n",
            "         -0.6436, -0.3928,  0.3878,  1.1543,  2.5716, -0.0079, -0.6405,  3.9239,\n",
            "          0.7779,  0.1485,  1.7563, -0.0667, -0.0301, -0.5670, -0.6436, -0.3928,\n",
            "          0.3878,  1.1543,  2.5716, -0.0079, -0.6405, -0.4154, -0.1895,  1.0590,\n",
            "         -0.1240, -0.2063],\n",
            "        [-0.6405, -0.3775, -0.2321, -0.1243, -0.6565, -0.0837, -0.0462, -0.6118,\n",
            "         -0.7843, -0.4086, -0.1333,  1.1538,  2.5573, -0.5539, -0.6405, -0.3775,\n",
            "         -0.2321, -0.1243, -0.6565, -0.0837, -0.0462, -0.6118, -0.7843, -0.4086,\n",
            "         -0.1333,  1.1538,  2.5573, -0.5539, -0.6405, -0.4154, -0.1895,  1.0590,\n",
            "         -0.1240, -0.2063]], dtype=torch.float64)\n",
            "tensor([0, 0, 0], dtype=torch.int8)\n",
            "tensor([[-0.6405, -0.3775, -0.2321, -0.1243, -0.6565, -0.0837, -0.0462, -0.5042,\n",
            "         -0.7491, -0.4108, -0.1333,  0.2644,  2.4362,  0.8481, -0.6405, -0.3775,\n",
            "         -0.2321, -0.1243, -0.6565, -0.0837, -0.0462, -0.5042, -0.7491, -0.4108,\n",
            "         -0.1333,  0.2644,  2.4362,  0.8481, -0.6405, -0.4154, -0.1895,  1.0590,\n",
            "         -0.1240, -0.2063],\n",
            "        [-0.6405, -0.3775,  0.0565, -0.0074,  1.0428, -0.0684, -0.0414, -0.6356,\n",
            "         -0.7491, -0.4077,  0.2575,  0.2639,  2.4220,  0.8473, -0.6405, -0.3775,\n",
            "          0.0565, -0.0074,  1.0428, -0.0684, -0.0414, -0.6356, -0.7491, -0.4077,\n",
            "          0.2575,  0.2639,  2.4220,  0.8473, -0.6405,  2.4075, -0.1895, -0.9443,\n",
            "         -0.1240, -0.2063],\n",
            "        [-0.6405, -0.3775, -0.2321, -0.1243, -0.6565, -0.0769, -0.0414, -0.6864,\n",
            "         -0.7843, -0.3859, -0.1333,  0.2634,  2.4078,  0.8466, -0.6405, -0.3775,\n",
            "         -0.2321, -0.1243, -0.6565, -0.0769, -0.0414, -0.6864, -0.7843, -0.3859,\n",
            "         -0.1333,  0.2634,  2.4078,  0.8466, -0.6405, -0.4154,  5.2781, -0.9443,\n",
            "         -0.1240, -0.2063]], dtype=torch.float64)\n",
            "tensor([1, 1, 0], dtype=torch.int8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-GctbL8E1oA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Classifier(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, drop_prob, epoch=None, val_loss=None):\n",
        "    super(Classifier, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.drop_prob = drop_prob\n",
        "    self.epoch = epoch\n",
        "    self.val_loss = val_loss\n",
        "    self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "    self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "    self.fcout = nn.Linear(hidden_size, 1)\n",
        "    self.dropout = nn.Dropout(drop_prob)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = torch.relu(self.fc1(x))\n",
        "    x = self.dropout(x)\n",
        "    x = torch.relu(self.fc2(x))\n",
        "    x = self.dropout(x)\n",
        "    x = torch.sigmoid(self.fcout(x))\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inpSePkGLTIs",
        "colab_type": "code",
        "outputId": "c7a68620-f1aa-4744-8aa5-d11869fe8196",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "classifier = Classifier(input_size=x.shape[1], hidden_size=128, drop_prob=0.2)\n",
        "classifier"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Classifier(\n",
              "  (fc1): Linear(in_features=34, out_features=128, bias=True)\n",
              "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (fcout): Linear(in_features=128, out_features=1, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsT5eIrNoXAh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = 'classifier.cpt'\n",
        "def save_classifier(classifier):\n",
        "  checkpoint = {\n",
        "      'input_size': classifier.input_size,\n",
        "      'hidden_size': classifier.hidden_size,\n",
        "      'drop_prob': classifier.drop_prob,\n",
        "      'val_loss': classifier.val_loss,\n",
        "      'epoch': classifier.epoch,\n",
        "      'state_dict': classifier.state_dict()\n",
        "  }\n",
        "  with open(filename, 'wb') as f:\n",
        "    torch.save(checkpoint, f)\n",
        "\n",
        "def load_classifier():\n",
        "  with open(filename, 'rb') as f:\n",
        "    checkpoint = torch.load(f)\n",
        "  classifier = Classifier(checkpoint['input_size'], checkpoint['hidden_size'], checkpoint['drop_prob'], checkpoint['epoch'], checkpoint['val_loss'])\n",
        "  classifier.load_state_dict(checkpoint['state_dict'])\n",
        "  return classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LX_eofKUq0ok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_classifier(classifier)\n",
        "classifier = load_classifier()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PR8s5ZA5rX-V",
        "colab_type": "code",
        "outputId": "eb00133e-21b2-4c5c-bd29-7e26e9d9cd74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  classifier = classifier.cuda()\n",
        "  inputs = inputs.cuda()\n",
        "\n",
        "classifier(inputs[:3].float())"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.5208],\n",
              "        [0.4950],\n",
              "        [0.5222]], device='cuda:0', grad_fn=<SigmoidBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arYOr5lMLYy7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(params=classifier.parameters(), lr=0.003)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFy03p96Lza1",
        "colab_type": "code",
        "outputId": "30419c27-0afd-45d4-e36d-8952b42f0c81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_loss = 0\n",
        "epochs = 200\n",
        "for epoch in range(epochs):\n",
        "  classifier.train()\n",
        "  for inputs, targets in train_dataloader:\n",
        "    if torch.cuda.is_available():\n",
        "      inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "    outputs = classifier(inputs.float())\n",
        "    loss = criterion(outputs.squeeze(), targets.float())\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    classifier.eval()\n",
        "    val_loss = 0\n",
        "    for inputs, targets in val_dataloader:\n",
        "      if torch.cuda.is_available():\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "      outputs = classifier(inputs.float())\n",
        "      loss = criterion(outputs.squeeze(), targets.float())\n",
        "      val_loss += loss.item()\n",
        "\n",
        "    train_loss = train_loss / len(train_dataloader)\n",
        "    val_loss = val_loss / len(val_dataloader)\n",
        "    print('Epoch: {}, Train loss: {}, Val loss: {}'.format(epoch, train_loss, val_loss))\n",
        "\n",
        "    if classifier.val_loss is None or val_loss < classifier.val_loss:\n",
        "      classifier.epoch = epoch\n",
        "      classifier.val_loss = val_loss\n",
        "      save_classifier(classifier)\n",
        "\n",
        "classifier.epoch = epoch\n",
        "classifier.val_loss = val_loss"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Train loss: 0.6041479926360281, Val loss: 0.6569316120524156\n",
            "Epoch: 1, Train loss: 0.570694009518029, Val loss: 0.6002790253413351\n",
            "Epoch: 2, Train loss: 0.5530547666982444, Val loss: 0.6287823727256373\n",
            "Epoch: 3, Train loss: 0.536836711639991, Val loss: 0.5841200296816073\n",
            "Epoch: 4, Train loss: 0.5251436923176268, Val loss: 0.5523381531238556\n",
            "Epoch: 5, Train loss: 0.5201705241784459, Val loss: 0.5655573088871805\n",
            "Epoch: 6, Train loss: 0.5133162199525065, Val loss: 0.5597422138640755\n",
            "Epoch: 7, Train loss: 0.5102744896136565, Val loss: 0.5325270863740068\n",
            "Epoch: 8, Train loss: 0.5069560162825617, Val loss: 0.5612616039028293\n",
            "Epoch: 9, Train loss: 0.500765390439523, Val loss: 0.5270608032220289\n",
            "Epoch: 10, Train loss: 0.492939662181591, Val loss: 0.5258188384928202\n",
            "Epoch: 11, Train loss: 0.5020244132333486, Val loss: 0.5353236657224203\n",
            "Epoch: 12, Train loss: 0.49028811018418195, Val loss: 0.5639263093471527\n",
            "Epoch: 13, Train loss: 0.48446724607253044, Val loss: 0.5588250777831203\n",
            "Epoch: 14, Train loss: 0.48673778636249115, Val loss: 0.5022751222315588\n",
            "Epoch: 15, Train loss: 0.478036593413926, Val loss: 0.5625937579101637\n",
            "Epoch: 16, Train loss: 0.4848203936432688, Val loss: 0.5276749494828676\n",
            "Epoch: 17, Train loss: 0.4827998609375925, Val loss: 0.5209214308936345\n",
            "Epoch: 18, Train loss: 0.48068782303163876, Val loss: 0.5372447724405088\n",
            "Epoch: 19, Train loss: 0.48300496361062956, Val loss: 0.498038097823921\n",
            "Epoch: 20, Train loss: 0.47753244027270714, Val loss: 0.49134852129377815\n",
            "Epoch: 21, Train loss: 0.48217318151728467, Val loss: 0.47796267408289406\n",
            "Epoch: 22, Train loss: 0.47354003334629324, Val loss: 0.5231092881999517\n",
            "Epoch: 23, Train loss: 0.4684646191159329, Val loss: 0.4915628582239151\n",
            "Epoch: 24, Train loss: 0.469742251231899, Val loss: 0.4943407017149423\n",
            "Epoch: 25, Train loss: 0.4709130128955274, Val loss: 0.5597099081466073\n",
            "Epoch: 26, Train loss: 0.46093251020431375, Val loss: 0.5754804354357091\n",
            "Epoch: 27, Train loss: 0.46637717639948195, Val loss: 0.5128096251895553\n",
            "Epoch: 28, Train loss: 0.4602118936430847, Val loss: 0.4934079550990933\n",
            "Epoch: 29, Train loss: 0.46739873579523916, Val loss: 0.527585832500144\n",
            "Epoch: 30, Train loss: 0.4651396934116572, Val loss: 0.4727450097469907\n",
            "Epoch: 31, Train loss: 0.46413266753921567, Val loss: 0.5162758389978033\n",
            "Epoch: 32, Train loss: 0.4551175691829883, Val loss: 0.5419965768723112\n",
            "Epoch: 33, Train loss: 0.46371714263147107, Val loss: 0.5620970457400146\n",
            "Epoch: 34, Train loss: 0.45958973806913117, Val loss: 0.6054371571854541\n",
            "Epoch: 35, Train loss: 0.47066397699395174, Val loss: 0.5656187936271492\n",
            "Epoch: 36, Train loss: 0.46356029213329136, Val loss: 0.46047317648404523\n",
            "Epoch: 37, Train loss: 0.4554528714012838, Val loss: 0.5930146888682717\n",
            "Epoch: 38, Train loss: 0.45391453520454794, Val loss: 0.4977421907610015\n",
            "Epoch: 39, Train loss: 0.45088982300172714, Val loss: 0.5054228511687956\n",
            "Epoch: 40, Train loss: 0.4556031477336848, Val loss: 0.4864695897619975\n",
            "Epoch: 41, Train loss: 0.45643174478852105, Val loss: 0.5067097236843485\n",
            "Epoch: 42, Train loss: 0.4486186150995899, Val loss: 0.5041220394012175\n",
            "Epoch: 43, Train loss: 0.4598435938470406, Val loss: 0.5665637237068853\n",
            "Epoch: 44, Train loss: 0.454264235026253, Val loss: 0.526929399096652\n",
            "Epoch: 45, Train loss: 0.4507157099750641, Val loss: 0.5262722671031952\n",
            "Epoch: 46, Train loss: 0.44504158327586557, Val loss: 0.5037110871390292\n",
            "Epoch: 47, Train loss: 0.4465575614596687, Val loss: 0.5086309249071699\n",
            "Epoch: 48, Train loss: 0.4456641206067521, Val loss: 0.5169399145402407\n",
            "Epoch: 49, Train loss: 0.44123910871755057, Val loss: 0.522812385504183\n",
            "Epoch: 50, Train loss: 0.4424387810020099, Val loss: 0.5093640759587288\n",
            "Epoch: 51, Train loss: 0.4425737449929078, Val loss: 0.4902868364006281\n",
            "Epoch: 52, Train loss: 0.4436678859994206, Val loss: 0.5336233120607702\n",
            "Epoch: 53, Train loss: 0.44615415829682925, Val loss: 0.5523874095003856\n",
            "Epoch: 54, Train loss: 0.4408399442042866, Val loss: 0.5365115939394424\n",
            "Epoch: 55, Train loss: 0.44314263285044503, Val loss: 0.5062926570443731\n",
            "Epoch: 56, Train loss: 0.43889574341105886, Val loss: 0.5494111001883683\n",
            "Epoch: 57, Train loss: 0.43723871941423403, Val loss: 0.5136035200404493\n",
            "Epoch: 58, Train loss: 0.4357111672652018, Val loss: 0.5187489443311566\n",
            "Epoch: 59, Train loss: 0.4363308052747194, Val loss: 0.5203330920715081\n",
            "Epoch: 60, Train loss: 0.44039757497616183, Val loss: 0.5457600343384241\n",
            "Epoch: 61, Train loss: 0.4370204308178099, Val loss: 0.5298880679826987\n",
            "Epoch: 62, Train loss: 0.4337587578739672, Val loss: 0.5133571034591449\n",
            "Epoch: 63, Train loss: 0.4328230437051987, Val loss: 0.5255887265268125\n",
            "Epoch: 64, Train loss: 0.43716539394544857, Val loss: 0.5061982466202033\n",
            "Epoch: 65, Train loss: 0.4415475847563428, Val loss: 0.5063160012819266\n",
            "Epoch: 66, Train loss: 0.44149714897139364, Val loss: 0.5033972714292375\n",
            "Epoch: 67, Train loss: 0.4335280884750223, Val loss: 0.5678409138008168\n",
            "Epoch: 68, Train loss: 0.433963910345047, Val loss: 0.5427003110709944\n",
            "Epoch: 69, Train loss: 0.430242222014326, Val loss: 0.5376061025614801\n",
            "Epoch: 70, Train loss: 0.4306714953420261, Val loss: 0.47859494760632515\n",
            "Epoch: 71, Train loss: 0.43332486984352714, Val loss: 0.5207479427519598\n",
            "Epoch: 72, Train loss: 0.4228446568933549, Val loss: 0.4834783424280192\n",
            "Epoch: 73, Train loss: 0.4270314427475328, Val loss: 0.6460130424483826\n",
            "Epoch: 74, Train loss: 0.4283663987388219, Val loss: 0.6034482230285281\n",
            "Epoch: 75, Train loss: 0.42804622979349743, Val loss: 0.5171534975892619\n",
            "Epoch: 76, Train loss: 0.430707726126017, Val loss: 0.55181739842029\n",
            "Epoch: 77, Train loss: 0.42811807214285985, Val loss: 0.4920867246232535\n",
            "Epoch: 78, Train loss: 0.4289982050184267, Val loss: 0.5136836453487998\n",
            "Epoch: 79, Train loss: 0.4288419901527957, Val loss: 0.5300191321263188\n",
            "Epoch: 80, Train loss: 0.4229522091756735, Val loss: 0.5161503304384256\n",
            "Epoch: 81, Train loss: 0.4252742835895285, Val loss: 0.5391949501476789\n",
            "Epoch: 82, Train loss: 0.4222272383775002, Val loss: 0.5565156532745612\n",
            "Epoch: 83, Train loss: 0.4269026346572808, Val loss: 0.4880524340428804\n",
            "Epoch: 84, Train loss: 0.41980419066801555, Val loss: 0.5222832599752828\n",
            "Epoch: 85, Train loss: 0.4191077760717469, Val loss: 0.5597859242636907\n",
            "Epoch: 86, Train loss: 0.4191391023937579, Val loss: 0.5822962336242199\n",
            "Epoch: 87, Train loss: 0.42342064844135574, Val loss: 0.5781080939463878\n",
            "Epoch: 88, Train loss: 0.42275286454552213, Val loss: 0.5380909995813119\n",
            "Epoch: 89, Train loss: 0.4159667595799016, Val loss: 0.5063024974967304\n",
            "Epoch: 90, Train loss: 0.42055218654916804, Val loss: 0.5462856351544982\n",
            "Epoch: 91, Train loss: 0.41525319590852083, Val loss: 0.5616277736660681\n",
            "Epoch: 92, Train loss: 0.4210239877237263, Val loss: 0.5323346991484103\n",
            "Epoch: 93, Train loss: 0.427465899811328, Val loss: 0.4855213298609382\n",
            "Epoch: 94, Train loss: 0.4199725361776493, Val loss: 0.5300258303943434\n",
            "Epoch: 95, Train loss: 0.4126152960148614, Val loss: 0.533865747679221\n",
            "Epoch: 96, Train loss: 0.42097576973649276, Val loss: 0.54488929733634\n",
            "Epoch: 97, Train loss: 0.4127534712923756, Val loss: 0.5352775734898291\n",
            "Epoch: 98, Train loss: 0.4251000091317798, Val loss: 0.5320814347972995\n",
            "Epoch: 99, Train loss: 0.4166513237118813, Val loss: 0.52206657121056\n",
            "Epoch: 100, Train loss: 0.41353114538224145, Val loss: 0.5622517719472709\n",
            "Epoch: 101, Train loss: 0.4120527840739885, Val loss: 0.5648552736169413\n",
            "Epoch: 102, Train loss: 0.41209916937007823, Val loss: 0.507307130843401\n",
            "Epoch: 103, Train loss: 0.4139625834445301, Val loss: 0.5160766675284034\n",
            "Epoch: 104, Train loss: 0.40954628457892395, Val loss: 0.5592623803960649\n",
            "Epoch: 105, Train loss: 0.4102546650773256, Val loss: 0.522004592575525\n",
            "Epoch: 106, Train loss: 0.41147453554525176, Val loss: 0.5388729231137979\n",
            "Epoch: 107, Train loss: 0.4157264501818581, Val loss: 0.49849581777265195\n",
            "Epoch: 108, Train loss: 0.41460466847861166, Val loss: 0.49940929640280574\n",
            "Epoch: 109, Train loss: 0.4059056797744005, Val loss: 0.5127328416626704\n",
            "Epoch: 110, Train loss: 0.40275254102945635, Val loss: 0.5773176160690031\n",
            "Epoch: 111, Train loss: 0.40932197378716983, Val loss: 0.5048026009217689\n",
            "Epoch: 112, Train loss: 0.4150600930072347, Val loss: 0.5405585848187145\n",
            "Epoch: 113, Train loss: 0.4067693969660095, Val loss: 0.5628754169140991\n",
            "Epoch: 114, Train loss: 0.4035514342707842, Val loss: 0.5031646009730665\n",
            "Epoch: 115, Train loss: 0.39991392678315824, Val loss: 0.5487534043036009\n",
            "Epoch: 116, Train loss: 0.40957185677956476, Val loss: 0.5822117648234493\n",
            "Epoch: 117, Train loss: 0.40841375371141153, Val loss: 0.5586918455205465\n",
            "Epoch: 118, Train loss: 0.40199638630745776, Val loss: 0.5298591995317685\n",
            "Epoch: 119, Train loss: 0.40484971859627594, Val loss: 0.5322597313084101\n",
            "Epoch: 120, Train loss: 0.4055728648468522, Val loss: 0.5203551408883772\n",
            "Epoch: 121, Train loss: 0.4032488058386264, Val loss: 0.5553405182926279\n",
            "Epoch: 122, Train loss: 0.4059812957006013, Val loss: 0.5012615732848644\n",
            "Epoch: 123, Train loss: 0.40581574391366443, Val loss: 0.5164326448973856\n",
            "Epoch: 124, Train loss: 0.40027908179609434, Val loss: 0.5218135424350437\n",
            "Epoch: 125, Train loss: 0.4013962182803762, Val loss: 0.48474722061502307\n",
            "Epoch: 126, Train loss: 0.40193513828778427, Val loss: 0.5067217002965902\n",
            "Epoch: 127, Train loss: 0.4001274236457035, Val loss: 0.5828463133228453\n",
            "Epoch: 128, Train loss: 0.40035642873931476, Val loss: 0.47863030276800456\n",
            "Epoch: 129, Train loss: 0.40856179090228223, Val loss: 0.4938677874834914\n",
            "Epoch: 130, Train loss: 0.40248717589647276, Val loss: 0.5034292334396588\n",
            "Epoch: 131, Train loss: 0.39521985428465845, Val loss: 0.5667048353505763\n",
            "Epoch: 132, Train loss: 0.40350916019054184, Val loss: 0.5181340351700783\n",
            "Epoch: 133, Train loss: 0.39995845970118565, Val loss: 0.5549609090544676\n",
            "Epoch: 134, Train loss: 0.39665331412214055, Val loss: 0.5317586072181401\n",
            "Epoch: 135, Train loss: 0.40074349912866636, Val loss: 0.48887106305674505\n",
            "Epoch: 136, Train loss: 0.40344749978573297, Val loss: 0.5215031300720415\n",
            "Epoch: 137, Train loss: 0.40509675340992085, Val loss: 0.5097319817072467\n",
            "Epoch: 138, Train loss: 0.39937336387266764, Val loss: 0.523893343578828\n",
            "Epoch: 139, Train loss: 0.3965738293220155, Val loss: 0.6103612880565619\n",
            "Epoch: 140, Train loss: 0.39557141503924226, Val loss: 0.5155720583310253\n",
            "Epoch: 141, Train loss: 0.3986857009146386, Val loss: 0.5617696948741612\n",
            "Epoch: 142, Train loss: 0.39452142561792747, Val loss: 0.5394411241929782\n",
            "Epoch: 143, Train loss: 0.39829410483055977, Val loss: 0.5096170341497973\n",
            "Epoch: 144, Train loss: 0.3964603405693157, Val loss: 0.5294569273920435\n",
            "Epoch: 145, Train loss: 0.3958715863434343, Val loss: 0.49910836764856387\n",
            "Epoch: 146, Train loss: 0.39666009751173453, Val loss: 0.48402098938822746\n",
            "Epoch: 147, Train loss: 0.39970347306373155, Val loss: 0.4981272383347938\n",
            "Epoch: 148, Train loss: 0.39776451180411077, Val loss: 0.5377817710763529\n",
            "Epoch: 149, Train loss: 0.398424612270644, Val loss: 0.5022911294118354\n",
            "Epoch: 150, Train loss: 0.40230282322322203, Val loss: 0.5286816646786112\n",
            "Epoch: 151, Train loss: 0.3973998323280965, Val loss: 0.5437127418423954\n",
            "Epoch: 152, Train loss: 0.3950930242873058, Val loss: 0.551032558671738\n",
            "Epoch: 153, Train loss: 0.3876083101231195, Val loss: 0.5647744108971796\n",
            "Epoch: 154, Train loss: 0.3940333541166321, Val loss: 0.5581965756259466\n",
            "Epoch: 155, Train loss: 0.3968463503894355, Val loss: 0.5563207588305599\n",
            "Epoch: 156, Train loss: 0.3920528064264988, Val loss: 0.4976354862906431\n",
            "Epoch: 157, Train loss: 0.3892844062366353, Val loss: 0.5581417522932354\n",
            "Epoch: 158, Train loss: 0.39596132474332074, Val loss: 0.5295703887547317\n",
            "Epoch: 159, Train loss: 0.3926741209043717, Val loss: 0.49484069861079516\n",
            "Epoch: 160, Train loss: 0.394507234102837, Val loss: 0.5408227927982807\n",
            "Epoch: 161, Train loss: 0.39349971096783226, Val loss: 0.4916227467750248\n",
            "Epoch: 162, Train loss: 0.3940767114834486, Val loss: 0.4821965057206781\n",
            "Epoch: 163, Train loss: 0.39242582589251035, Val loss: 0.5185110133729482\n",
            "Epoch: 164, Train loss: 0.39536160755310723, Val loss: 0.5703110763509023\n",
            "Epoch: 165, Train loss: 0.3848385208343608, Val loss: 0.5611308375863653\n",
            "Epoch: 166, Train loss: 0.3805577441095656, Val loss: 0.5250000626240906\n",
            "Epoch: 167, Train loss: 0.3836849511821002, Val loss: 0.5773851808748747\n",
            "Epoch: 168, Train loss: 0.39293896179753157, Val loss: 0.5351002639845798\n",
            "Epoch: 169, Train loss: 0.3886581797927507, Val loss: 0.5525202704103369\n",
            "Epoch: 170, Train loss: 0.38530265066584446, Val loss: 0.48496211653477267\n",
            "Epoch: 171, Train loss: 0.38536447097689447, Val loss: 0.5004711858928204\n",
            "Epoch: 172, Train loss: 0.38377294566804654, Val loss: 0.5000156993536573\n",
            "Epoch: 173, Train loss: 0.3797954377444623, Val loss: 0.5240813125120966\n",
            "Epoch: 174, Train loss: 0.38088755089178433, Val loss: 0.4933207205643779\n",
            "Epoch: 175, Train loss: 0.3890613046335899, Val loss: 0.532892291875262\n",
            "Epoch: 176, Train loss: 0.39005204650144754, Val loss: 0.5521602344356085\n",
            "Epoch: 177, Train loss: 0.38495868826445284, Val loss: 0.5556922376547989\n",
            "Epoch: 178, Train loss: 0.3803676501137479, Val loss: 0.5775292192242647\n",
            "Epoch: 179, Train loss: 0.3897090314648195, Val loss: 0.5527279439725374\n",
            "Epoch: 180, Train loss: 0.3838274017729823, Val loss: 0.5103181822127417\n",
            "Epoch: 181, Train loss: 0.378093297114912, Val loss: 0.5400739708229115\n",
            "Epoch: 182, Train loss: 0.3864890304268176, Val loss: 0.5352236432464499\n",
            "Epoch: 183, Train loss: 0.392306459491349, Val loss: 0.5540679440294441\n",
            "Epoch: 184, Train loss: 0.3850710928582543, Val loss: 0.5106146570883299\n",
            "Epoch: 185, Train loss: 0.3885150628658329, Val loss: 0.565895956989966\n",
            "Epoch: 186, Train loss: 0.38071293600131595, Val loss: 0.5051241097481627\n",
            "Epoch: 187, Train loss: 0.38532164561323157, Val loss: 0.5333608586929346\n",
            "Epoch: 188, Train loss: 0.3851677853463961, Val loss: 0.5316454897585668\n",
            "Epoch: 189, Train loss: 0.3803070879782472, Val loss: 0.5452671168666137\n",
            "Epoch: 190, Train loss: 0.38145849134003185, Val loss: 0.5358626746425503\n",
            "Epoch: 191, Train loss: 0.37888035727840635, Val loss: 0.5587815093366724\n",
            "Epoch: 192, Train loss: 0.38079945522267095, Val loss: 0.510820604076511\n",
            "Epoch: 193, Train loss: 0.37606809125598023, Val loss: 0.5465886890888214\n",
            "Epoch: 194, Train loss: 0.3823345022730283, Val loss: 0.5205235067558917\n",
            "Epoch: 195, Train loss: 0.3778668156248756, Val loss: 0.5250075949650062\n",
            "Epoch: 196, Train loss: 0.37916754372021194, Val loss: 0.5006554616909278\n",
            "Epoch: 197, Train loss: 0.37782303954611235, Val loss: 0.5307763340441805\n",
            "Epoch: 198, Train loss: 0.3801266774444857, Val loss: 0.5587028321859083\n",
            "Epoch: 199, Train loss: 0.3776868791562138, Val loss: 0.5544449390941545\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXw0R-Tkucgw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_recalls(targets, predictions):\n",
        "  n_classes = 2\n",
        "  confusion_matrix, _, _ = np.histogram2d(targets.detach().cpu().numpy(), predictions.squeeze().detach().cpu().numpy(), bins=n_classes)\n",
        "  return np.diag(confusion_matrix) / (np.sum(confusion_matrix, axis=1))\n",
        "\n",
        "def gmean(classifier, dataloader):\n",
        "  with torch.no_grad():\n",
        "    classifier.eval()\n",
        "    recalls = np.zeros((2))\n",
        "    for inputs, targets in dataloader:\n",
        "      if torch.cuda.is_available():\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "      \n",
        "      outputs = classifier(inputs.float())\n",
        "      predictions = torch.round(outputs).int()\n",
        "      recalls += calculate_recalls(targets, predictions)\n",
        "\n",
        "    recalls = recalls / len(dataloader)\n",
        "    return mstats.gmean(recalls)    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlrNw0bEqAeh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(classifier):\n",
        "  if torch.cuda.is_available():\n",
        "    classifier = classifier.cuda()\n",
        "  train_accuracy = gmean(classifier, train_dataloader)\n",
        "  test_accuracy = gmean(classifier, test_dataloader)\n",
        "  print('Epoch: {}'.format(classifier.epoch))\n",
        "  print('Train g-mean: {}'.format(train_accuracy.item()))\n",
        "  print('Test g-mean: {}'.format(test_accuracy.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFoBTASdvqRv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "319f083d-75c0-4351-877a-9aef533531db"
      },
      "source": [
        "print('Best classifier')\n",
        "evaluate(load_classifier())"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best classifier\n",
            "Epoch: 36\n",
            "Train g-mean: 0.7918174773270166\n",
            "Test g-mean: 0.6907747008974929\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "918KM5F07hFl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "95ccd453-dde1-4533-f753-6d797ca7dfa3"
      },
      "source": [
        "print('Last classifier')\n",
        "evaluate(classifier)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Last classifier\n",
            "Epoch: 199\n",
            "Train g-mean: 0.846907670020874\n",
            "Test g-mean: 0.6768300496081506\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_t4Gh43GYWl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}