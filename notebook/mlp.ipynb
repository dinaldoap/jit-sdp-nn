{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JIT-SDP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dinaldoap/jit-sdp-nn/blob/master/notebook/mlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXwvNEAQE6NO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAupgTDfmJ_I",
        "colab_type": "code",
        "outputId": "b80c5ed0-300e-44dc-a1f5-9bf10c1f7e3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/dinaldoap/jit-sdp-data/master/jenkins.csv')\n",
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fix</th>\n",
              "      <th>ns</th>\n",
              "      <th>nd</th>\n",
              "      <th>nf</th>\n",
              "      <th>entropy</th>\n",
              "      <th>la</th>\n",
              "      <th>ld</th>\n",
              "      <th>lt</th>\n",
              "      <th>ndev</th>\n",
              "      <th>age</th>\n",
              "      <th>nuc</th>\n",
              "      <th>exp</th>\n",
              "      <th>rexp</th>\n",
              "      <th>sexp</th>\n",
              "      <th>author_date_unix_timestamp</th>\n",
              "      <th>classification</th>\n",
              "      <th>contains_bug</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>False</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.641604</td>\n",
              "      <td>9.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>426.428571</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.000093</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5171.0</td>\n",
              "      <td>30.227271</td>\n",
              "      <td>1472.714286</td>\n",
              "      <td>1555326371</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>False</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.750000</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>426.428571</td>\n",
              "      <td>100.0</td>\n",
              "      <td>6.314775</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5170.0</td>\n",
              "      <td>29.227271</td>\n",
              "      <td>1471.714286</td>\n",
              "      <td>1555326363</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>False</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.906580</td>\n",
              "      <td>15.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>96.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.034722</td>\n",
              "      <td>2.0</td>\n",
              "      <td>629.0</td>\n",
              "      <td>14.828373</td>\n",
              "      <td>414.000000</td>\n",
              "      <td>1554971763</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>False</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.058824</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1554969774</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>False</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.662506</td>\n",
              "      <td>14.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>67.000000</td>\n",
              "      <td>6.0</td>\n",
              "      <td>21.280683</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.058824</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1554967752</td>\n",
              "      <td>Feature Addition</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     fix   ns   nd  ...  author_date_unix_timestamp    classification  contains_bug\n",
              "0  False  7.0  7.0  ...                  1555326371              None         False\n",
              "1  False  7.0  7.0  ...                  1555326363              None         False\n",
              "2  False  1.0  1.0  ...                  1554971763              None         False\n",
              "3  False  1.0  1.0  ...                  1554969774              None         False\n",
              "4  False  1.0  2.0  ...                  1554967752  Feature Addition         False\n",
              "\n",
              "[5 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alizaxdPD3El",
        "colab_type": "code",
        "outputId": "5247867f-93a7-4c65-9af7-860c5683bfef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "df = df.sample(frac=1)\n",
        "label_col = 'contains_bug'\n",
        "features_cols = ['fix', 'ns', 'nd', 'nf', 'entropy', 'la', 'ld', 'lt', 'ndev', 'age', 'nuc', 'exp', 'rexp', 'sexp', 'classification']\n",
        "x = df[features_cols]\n",
        "x['fix'] = x['fix'].astype('int')\n",
        "df_classification = pd.get_dummies(x, columns=['classification'])\n",
        "x = pd.concat([x, df_classification], axis='columns')\n",
        "x = x.drop(['classification'], axis='columns')\n",
        "x = x.values\n",
        "y = df[label_col]\n",
        "y = y.astype('category')\n",
        "y = y.cat.codes\n",
        "y = y.values"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBD1-tp2ExzQ",
        "colab_type": "code",
        "outputId": "a5fcb732-b801-4217-9422-d93e7fdb1919",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "print(x[:3])\n",
        "print(y[:3])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 3.00000000e+00 3.41800000e+03\n",
            "  3.40000000e+01 1.15740741e-05 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  3.00000000e+00 3.41800000e+03 3.40000000e+01 1.15740741e-05\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.00000000e+00 2.00000000e+00 5.00000000e+00 2.60000000e+01\n",
            "  1.37290201e+00 6.08000000e+02 2.30000000e+01 2.43230769e+02\n",
            "  9.50000000e+01 3.92893185e+01 5.00000000e+00 5.00000000e+00\n",
            "  9.04040404e-01 4.00000000e+00 1.00000000e+00 2.00000000e+00\n",
            "  5.00000000e+00 2.60000000e+01 1.37290201e+00 6.08000000e+02\n",
            "  2.30000000e+01 2.43230769e+02 9.50000000e+01 3.92893185e+01\n",
            "  5.00000000e+00 5.00000000e+00 9.04040404e-01 4.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 4.00000000e+00 4.00000000e+00 3.30000000e+01\n",
            "  1.00000000e+00 1.53231944e+01 1.00000000e+00 1.45000000e+02\n",
            "  3.19461538e+00 1.16000000e+02 1.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 4.00000000e+00\n",
            "  4.00000000e+00 3.30000000e+01 1.00000000e+00 1.53231944e+01\n",
            "  1.00000000e+00 1.45000000e+02 3.19461538e+00 1.16000000e+02\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "[0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwCn1uNTMjfG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_index = int( len(x) * 0.8 )\n",
        "test_index = int( len(x) * 0.9 )\n",
        "x_train, y_train = x[:val_index], y[:val_index]\n",
        "x_val, y_val = x[val_index:test_index], y[val_index:test_index]\n",
        "x_test, y_test = x[test_index:], y[test_index:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSqnoapE5Poq",
        "colab_type": "code",
        "outputId": "0dc35029-d69f-451c-da56-34bb7ca34e34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(x_train[:3])\n",
        "print(x_val[:3])\n",
        "print(x_test[:3])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 3.00000000e+00 3.41800000e+03\n",
            "  3.40000000e+01 1.15740741e-05 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  3.00000000e+00 3.41800000e+03 3.40000000e+01 1.15740741e-05\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.00000000e+00 2.00000000e+00 5.00000000e+00 2.60000000e+01\n",
            "  1.37290201e+00 6.08000000e+02 2.30000000e+01 2.43230769e+02\n",
            "  9.50000000e+01 3.92893185e+01 5.00000000e+00 5.00000000e+00\n",
            "  9.04040404e-01 4.00000000e+00 1.00000000e+00 2.00000000e+00\n",
            "  5.00000000e+00 2.60000000e+01 1.37290201e+00 6.08000000e+02\n",
            "  2.30000000e+01 2.43230769e+02 9.50000000e+01 3.92893185e+01\n",
            "  5.00000000e+00 5.00000000e+00 9.04040404e-01 4.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 4.00000000e+00 4.00000000e+00 3.30000000e+01\n",
            "  1.00000000e+00 1.53231944e+01 1.00000000e+00 1.45000000e+02\n",
            "  3.19461538e+00 1.16000000e+02 1.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 4.00000000e+00\n",
            "  4.00000000e+00 3.30000000e+01 1.00000000e+00 1.53231944e+01\n",
            "  1.00000000e+00 1.45000000e+02 3.19461538e+00 1.16000000e+02\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "[[0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 7.00000000e+00 4.00000000e+00 4.86000000e+02\n",
            "  7.00000000e+00 2.41022801e+01 1.00000000e+00 4.49000000e+02\n",
            "  9.62015169e+01 2.59000000e+02 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 7.00000000e+00\n",
            "  4.00000000e+00 4.86000000e+02 7.00000000e+00 2.41022801e+01\n",
            "  1.00000000e+00 4.49000000e+02 9.62015169e+01 2.59000000e+02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 6.00000000e+00 2.00000000e+00 5.02000000e+02\n",
            "  1.20000000e+01 4.50810185e-02 1.00000000e+00 2.35000000e+02\n",
            "  2.95508224e+01 1.39000000e+02 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 6.00000000e+00\n",
            "  2.00000000e+00 5.02000000e+02 1.20000000e+01 4.50810185e-02\n",
            "  1.00000000e+00 2.35000000e+02 2.95508224e+01 1.39000000e+02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+01 1.00000000e+01 1.00000000e+01\n",
            "  3.18872188e+00 1.50000000e+01 9.00000000e+00 3.01000000e+02\n",
            "  1.10000000e+01 3.47068056e+00 4.00000000e+00 6.49700000e+03\n",
            "  1.48213918e+02 9.94200000e+02 0.00000000e+00 1.00000000e+01\n",
            "  1.00000000e+01 1.00000000e+01 3.18872188e+00 1.50000000e+01\n",
            "  9.00000000e+00 3.01000000e+02 1.10000000e+01 3.47068056e+00\n",
            "  4.00000000e+00 6.49700000e+03 1.48213918e+02 9.94200000e+02\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "[[0.00000000e+00 7.00000000e+00 7.00000000e+00 7.00000000e+00\n",
            "  2.80735492e+00 7.00000000e+00 7.00000000e+00 4.54571429e+02\n",
            "  4.40000000e+01 7.65008267e+00 2.00000000e+00 2.25400000e+03\n",
            "  1.21297158e+02 5.43571429e+02 0.00000000e+00 7.00000000e+00\n",
            "  7.00000000e+00 7.00000000e+00 2.80735492e+00 7.00000000e+00\n",
            "  7.00000000e+00 4.54571429e+02 4.40000000e+01 7.65008267e+00\n",
            "  2.00000000e+00 2.25400000e+03 1.21297158e+02 5.43571429e+02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 2.00000000e+00 2.00000000e+00\n",
            "  1.89278431e-01 6.80000000e+01 1.00000000e+00 1.02000000e+02\n",
            "  8.00000000e+00 4.00954861e-01 1.00000000e+00 5.00000000e+01\n",
            "  1.73839632e+01 3.60000000e+01 0.00000000e+00 1.00000000e+00\n",
            "  2.00000000e+00 2.00000000e+00 1.89278431e-01 6.80000000e+01\n",
            "  1.00000000e+00 1.02000000e+02 8.00000000e+00 4.00954861e-01\n",
            "  1.00000000e+00 5.00000000e+01 1.73839632e+01 3.60000000e+01\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 4.84100000e+03\n",
            "  4.30000000e+01 9.18122917e+01 1.00000000e+00 4.60400000e+03\n",
            "  8.43696192e+01 2.81800000e+03 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 4.84100000e+03 4.30000000e+01 9.18122917e+01\n",
            "  1.00000000e+00 4.60400000e+03 8.43696192e+01 2.81800000e+03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46STsbti3AmT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean_train = x_train.mean(axis=0)\n",
        "std_train = x_train.std(axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5vvbrrK3PES",
        "colab_type": "code",
        "outputId": "48dd8527-8624-4062-ef1d-171b948bb5db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "print(mean_train)\n",
        "print(std_train)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2.79906905e-01 1.60062064e+00 2.50969744e+00 3.94946987e+00\n",
            " 7.32263415e-01 4.98735454e+01 2.67649858e+01 7.10852265e+02\n",
            " 1.91607965e+01 8.25198811e+01 1.91383501e+00 1.75702715e+03\n",
            " 8.56499295e+01 9.02132140e+02 2.79906905e-01 1.60062064e+00\n",
            " 2.50969744e+00 3.94946987e+00 7.32263415e-01 4.98735454e+01\n",
            " 2.67649858e+01 7.10852265e+02 1.91607965e+01 8.25198811e+01\n",
            " 1.91383501e+00 1.75702715e+03 8.56499295e+01 9.02132140e+02\n",
            " 2.79906905e-01 1.55986553e-01 3.60486165e-02 4.77217481e-01\n",
            " 1.60848203e-02 3.47556245e-02]\n",
            "[4.48953260e-01 1.63004491e+00 6.20794694e+00 2.46206593e+01\n",
            " 1.11373816e+00 6.38695803e+02 5.33382734e+02 1.06711354e+03\n",
            " 2.70511866e+01 2.13864775e+02 7.39000523e+00 1.98723452e+03\n",
            " 8.19200817e+01 1.30608455e+03 4.48953260e-01 1.63004491e+00\n",
            " 6.20794694e+00 2.46206593e+01 1.11373816e+00 6.38695803e+02\n",
            " 5.33382734e+02 1.06711354e+03 2.70511866e+01 2.13864775e+02\n",
            " 7.39000523e+00 1.98723452e+03 8.19200817e+01 1.30608455e+03\n",
            " 4.48953260e-01 3.62842594e-01 1.86411142e-01 4.99480687e-01\n",
            " 1.25801824e-01 1.83160233e-01]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2IoFA5h425Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = (x_train - mean_train) / std_train\n",
        "x_val = (x_val - mean_train) / std_train\n",
        "x_test = (x_test - mean_train) / std_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1AoKNoU5V1w",
        "colab_type": "code",
        "outputId": "4c50121e-0e57-4588-d20d-cafad1833bd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        }
      },
      "source": [
        "print(x_train[:3])\n",
        "print(x_val[:3])\n",
        "print(x_test[:3])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1.60393778 -0.36846876 -0.24318788 -0.11979654 -0.65748256 -0.07808654\n",
            "  -0.04455522  2.53688818  0.54856017 -0.38585068 -0.12365824 -0.88415692\n",
            "  -1.04553032 -0.69071496  1.60393778 -0.36846876 -0.24318788 -0.11979654\n",
            "  -0.65748256 -0.07808654 -0.04455522  2.53688818  0.54856017 -0.38585068\n",
            "  -0.12365824 -0.88415692 -1.04553032 -0.69071496  1.60393778 -0.42990144\n",
            "  -0.19338231 -0.95542729 -0.1278584  -0.1897553 ]\n",
            " [ 1.60393778  0.24501126  0.40114753  0.89561087  0.57521473  0.87385333\n",
            "  -0.00705869 -0.43821156  2.80354443 -0.20213971  0.41761337 -0.88164086\n",
            "  -1.03449469 -0.68765237  1.60393778  0.24501126  0.40114753  0.89561087\n",
            "   0.57521473  0.87385333 -0.00705869 -0.43821156  2.80354443 -0.20213971\n",
            "   0.41761337 -0.88164086 -1.03449469 -0.68765237  1.60393778 -0.42990144\n",
            "  -0.19338231 -0.95542729 -0.1278584  -0.1897553 ]\n",
            " [ 1.60393778 -0.36846876 -0.24318788 -0.11979654 -0.65748256 -0.07182378\n",
            "  -0.0426804  -0.63522037 -0.67134935 -0.31420175 -0.12365824 -0.8111912\n",
            "  -1.00653359 -0.60189988  1.60393778 -0.36846876 -0.24318788 -0.11979654\n",
            "  -0.65748256 -0.07182378 -0.0426804  -0.63522037 -0.67134935 -0.31420175\n",
            "  -0.12365824 -0.8111912  -1.00653359 -0.60189988  1.60393778 -0.42990144\n",
            "  -0.19338231 -0.95542729 -0.1278584  -0.1897553 ]]\n",
            "[[-0.62346558 -0.36846876 -0.24318788 -0.11979654 -0.65748256 -0.06712671\n",
            "  -0.0426804  -0.21071072 -0.44954762 -0.27315205 -0.12365824 -0.65821479\n",
            "   0.12880343 -0.49241233 -0.62346558 -0.36846876 -0.24318788 -0.11979654\n",
            "  -0.65748256 -0.06712671 -0.0426804  -0.21071072 -0.44954762 -0.27315205\n",
            "  -0.12365824 -0.65821479  0.12880343 -0.49241233 -0.62346558 -0.42990144\n",
            "  -0.19338231  1.04665212 -0.1278584  -0.1897553 ]\n",
            " [-0.62346558 -0.36846876 -0.24318788 -0.11979654 -0.65748256 -0.0686924\n",
            "  -0.04643005 -0.195717   -0.26471284 -0.38563994 -0.12365824 -0.76590213\n",
            "  -0.68480287 -0.58429    -0.62346558 -0.36846876 -0.24318788 -0.11979654\n",
            "  -0.65748256 -0.0686924  -0.04643005 -0.195717   -0.26471284 -0.38563994\n",
            "  -0.12365824 -0.76590213 -0.68480287 -0.58429    -0.62346558 -0.42990144\n",
            "  -0.19338231  1.04665212 -0.1278584  -0.1897553 ]\n",
            " [-0.62346558  5.15285149  1.20656678  0.24575013  2.20559783 -0.05460118\n",
            "  -0.03330626 -0.38407559 -0.3016798  -0.36962235  0.28229547  2.38521061\n",
            "   0.7637198   0.0704915  -0.62346558  5.15285149  1.20656678  0.24575013\n",
            "   2.20559783 -0.05460118 -0.03330626 -0.38407559 -0.3016798  -0.36962235\n",
            "   0.28229547  2.38521061  0.7637198   0.0704915  -0.62346558 -0.42990144\n",
            "   5.17110391 -0.95542729 -0.1278584  -0.1897553 ]]\n",
            "[[-0.62346558  3.31241141  0.72331523  0.12390124  1.86317717 -0.06712671\n",
            "  -0.03705592 -0.24016267  0.91822972 -0.35008008  0.01165967  0.25008264\n",
            "   0.43514639 -0.27453101 -0.62346558  3.31241141  0.72331523  0.12390124\n",
            "   1.86317717 -0.06712671 -0.03705592 -0.24016267  0.91822972 -0.35008008\n",
            "   0.01165967  0.25008264  0.43514639 -0.27453101 -0.62346558 -0.42990144\n",
            "  -0.19338231  1.04665212 -0.1278584  -0.1897553 ]\n",
            " [-0.62346558 -0.36846876 -0.08210403 -0.07918025 -0.48753379  0.02838042\n",
            "  -0.04830487 -0.57055996 -0.41258066 -0.38397593 -0.12365824 -0.85899633\n",
            "  -0.83332395 -0.66315166 -0.62346558 -0.36846876 -0.08210403 -0.07918025\n",
            "  -0.48753379  0.02838042 -0.04830487 -0.57055996 -0.41258066 -0.38397593\n",
            "  -0.12365824 -0.85899633 -0.83332395 -0.66315166 -0.62346558 -0.42990144\n",
            "  -0.19338231 -0.95542729 -0.1278584   5.26994511]\n",
            " [-0.62346558 -0.36846876 -0.24318788 -0.11979654 -0.65748256 -0.07808654\n",
            "  -0.04830487  3.87039201  0.88126277  0.04344994 -0.12365824  1.43263054\n",
            "  -0.01562877  1.46687889 -0.62346558 -0.36846876 -0.24318788 -0.11979654\n",
            "  -0.65748256 -0.07808654 -0.04830487  3.87039201  0.88126277  0.04344994\n",
            "  -0.12365824  1.43263054 -0.01562877  1.46687889 -0.62346558 -0.42990144\n",
            "  -0.19338231  1.04665212 -0.1278584  -0.1897553 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iP6L6GN22_bR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, y_train = torch.from_numpy(x_train), torch.from_numpy(y_train)\n",
        "x_val, y_val = torch.from_numpy(x_val), torch.from_numpy(y_val)\n",
        "x_test, y_test = torch.from_numpy(x_test), torch.from_numpy(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XnnrzRjL3IP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = data.TensorDataset(x_train, y_train)\n",
        "val_dataset = data.TensorDataset(x_val, y_val)\n",
        "test_dataset = data.TensorDataset(x_test, y_test)\n",
        "\n",
        "train_dataloader = data.DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
        "val_dataloader = data.DataLoader(val_dataset, batch_size=32)\n",
        "test_dataloader = data.DataLoader(test_dataset, batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHoY9-JcORo-",
        "colab_type": "code",
        "outputId": "77795dc1-15dc-4be3-b94e-2774e8db9dd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "source": [
        "inputs, targets = next(iter(train_dataloader))\n",
        "print(inputs[:3])\n",
        "print(targets[:3])\n",
        "\n",
        "inputs, targets = next(iter(val_dataloader))\n",
        "print(inputs[:3])\n",
        "print(targets[:3])\n",
        "\n",
        "inputs, targets = next(iter(test_dataloader))\n",
        "print(inputs[:3])\n",
        "print(targets[:3])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 1.6039, -0.3685, -0.2432, -0.1198, -0.6575, -0.0640, -0.0502, -0.5584,\n",
            "         -0.6344,  0.2449, -0.1237,  1.2731,  0.8030,  2.0978,  1.6039, -0.3685,\n",
            "         -0.2432, -0.1198, -0.6575, -0.0640, -0.0502, -0.5584, -0.6344,  0.2449,\n",
            "         -0.1237,  1.2731,  0.8030,  2.0978,  1.6039, -0.4299, -0.1934, -0.9554,\n",
            "         -0.1279, -0.1898],\n",
            "        [-0.6235, -0.3685, -0.2432, -0.1198, -0.6575, -0.0671, -0.0483,  1.3936,\n",
            "         -0.4126, -0.3799, -0.1237,  0.9717, -0.0191, -0.0767, -0.6235, -0.3685,\n",
            "         -0.2432, -0.1198, -0.6575, -0.0671, -0.0483,  1.3936, -0.4126, -0.3799,\n",
            "         -0.1237,  0.9717, -0.0191, -0.0767, -0.6235, -0.4299, -0.1934,  1.0467,\n",
            "         -0.1279, -0.1898],\n",
            "        [-0.6235, -0.3685, -0.2432, -0.1198, -0.6575, -0.0765, -0.0502, -0.4347,\n",
            "         -0.0799, -0.3258, -0.1237, -0.8837, -1.0440, -0.6907, -0.6235, -0.3685,\n",
            "         -0.2432, -0.1198, -0.6575, -0.0765, -0.0502, -0.4347, -0.0799, -0.3258,\n",
            "         -0.1237, -0.8837, -1.0440, -0.6907, -0.6235,  2.3261, -0.1934, -0.9554,\n",
            "         -0.1279, -0.1898]], dtype=torch.float64)\n",
            "tensor([0, 0, 0], dtype=torch.int8)\n",
            "tensor([[-0.6235, -0.3685, -0.2432, -0.1198, -0.6575, -0.0671, -0.0427, -0.2107,\n",
            "         -0.4495, -0.2732, -0.1237, -0.6582,  0.1288, -0.4924, -0.6235, -0.3685,\n",
            "         -0.2432, -0.1198, -0.6575, -0.0671, -0.0427, -0.2107, -0.4495, -0.2732,\n",
            "         -0.1237, -0.6582,  0.1288, -0.4924, -0.6235, -0.4299, -0.1934,  1.0467,\n",
            "         -0.1279, -0.1898],\n",
            "        [-0.6235, -0.3685, -0.2432, -0.1198, -0.6575, -0.0687, -0.0464, -0.1957,\n",
            "         -0.2647, -0.3856, -0.1237, -0.7659, -0.6848, -0.5843, -0.6235, -0.3685,\n",
            "         -0.2432, -0.1198, -0.6575, -0.0687, -0.0464, -0.1957, -0.2647, -0.3856,\n",
            "         -0.1237, -0.7659, -0.6848, -0.5843, -0.6235, -0.4299, -0.1934,  1.0467,\n",
            "         -0.1279, -0.1898],\n",
            "        [-0.6235,  5.1529,  1.2066,  0.2458,  2.2056, -0.0546, -0.0333, -0.3841,\n",
            "         -0.3017, -0.3696,  0.2823,  2.3852,  0.7637,  0.0705, -0.6235,  5.1529,\n",
            "          1.2066,  0.2458,  2.2056, -0.0546, -0.0333, -0.3841, -0.3017, -0.3696,\n",
            "          0.2823,  2.3852,  0.7637,  0.0705, -0.6235, -0.4299,  5.1711, -0.9554,\n",
            "         -0.1279, -0.1898]], dtype=torch.float64)\n",
            "tensor([0, 0, 0], dtype=torch.int8)\n",
            "tensor([[-0.6235,  3.3124,  0.7233,  0.1239,  1.8632, -0.0671, -0.0371, -0.2402,\n",
            "          0.9182, -0.3501,  0.0117,  0.2501,  0.4351, -0.2745, -0.6235,  3.3124,\n",
            "          0.7233,  0.1239,  1.8632, -0.0671, -0.0371, -0.2402,  0.9182, -0.3501,\n",
            "          0.0117,  0.2501,  0.4351, -0.2745, -0.6235, -0.4299, -0.1934,  1.0467,\n",
            "         -0.1279, -0.1898],\n",
            "        [-0.6235, -0.3685, -0.0821, -0.0792, -0.4875,  0.0284, -0.0483, -0.5706,\n",
            "         -0.4126, -0.3840, -0.1237, -0.8590, -0.8333, -0.6632, -0.6235, -0.3685,\n",
            "         -0.0821, -0.0792, -0.4875,  0.0284, -0.0483, -0.5706, -0.4126, -0.3840,\n",
            "         -0.1237, -0.8590, -0.8333, -0.6632, -0.6235, -0.4299, -0.1934, -0.9554,\n",
            "         -0.1279,  5.2699],\n",
            "        [-0.6235, -0.3685, -0.2432, -0.1198, -0.6575, -0.0781, -0.0483,  3.8704,\n",
            "          0.8813,  0.0434, -0.1237,  1.4326, -0.0156,  1.4669, -0.6235, -0.3685,\n",
            "         -0.2432, -0.1198, -0.6575, -0.0781, -0.0483,  3.8704,  0.8813,  0.0434,\n",
            "         -0.1237,  1.4326, -0.0156,  1.4669, -0.6235, -0.4299, -0.1934,  1.0467,\n",
            "         -0.1279, -0.1898]], dtype=torch.float64)\n",
            "tensor([0, 1, 0], dtype=torch.int8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-GctbL8E1oA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Classifier(nn.Module):\n",
        "  def __init__(self, epochs, input_size, hidden_size, val_loss=None):\n",
        "    super(Classifier, self).__init__()\n",
        "    self.epochs = epochs\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.val_loss = val_loss\n",
        "    self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "    self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "    self.fcout = nn.Linear(hidden_size, 1)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = torch.sigmoid(self.fc1(x))\n",
        "    x = torch.sigmoid(self.fc2(x))\n",
        "    x = torch.sigmoid(self.fcout(x))\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inpSePkGLTIs",
        "colab_type": "code",
        "outputId": "de25bdf0-52e7-45fe-fa46-51fb0bf88148",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "classifier = Classifier(epochs=100, input_size=x.shape[1], hidden_size=256)\n",
        "classifier"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Classifier(\n",
              "  (fc1): Linear(in_features=34, out_features=256, bias=True)\n",
              "  (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
              "  (fcout): Linear(in_features=256, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsT5eIrNoXAh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = 'classifier.cpt'\n",
        "def save_classifier(classifier):\n",
        "  checkpoint = {\n",
        "      'epochs': classifier.epochs,\n",
        "      'input_size': classifier.input_size,\n",
        "      'hidden_size': classifier.hidden_size,\n",
        "      'val_loss': classifier.val_loss,\n",
        "      'state_dict': classifier.state_dict()\n",
        "  }\n",
        "  with open(filename, 'wb') as f:\n",
        "    torch.save(checkpoint, f)\n",
        "\n",
        "def load_classifier():\n",
        "  with open(filename, 'rb') as f:\n",
        "    checkpoint = torch.load(f)\n",
        "  classifier = Classifier(checkpoint['epochs'], checkpoint['input_size'], checkpoint['hidden_size'], checkpoint['val_loss'])\n",
        "  classifier.load_state_dict(checkpoint['state_dict'])\n",
        "  return classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LX_eofKUq0ok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_classifier(classifier)\n",
        "classifier = load_classifier()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PR8s5ZA5rX-V",
        "colab_type": "code",
        "outputId": "29506436-2871-454c-f88b-03c29c9bf167",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  classifier = classifier.cuda()\n",
        "  inputs = inputs.cuda()\n",
        "\n",
        "classifier(inputs[:3].float())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4918],\n",
              "        [0.4897],\n",
              "        [0.4877]], device='cuda:0', grad_fn=<SigmoidBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arYOr5lMLYy7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(params=classifier.parameters(), lr=0.03)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFy03p96Lza1",
        "colab_type": "code",
        "outputId": "529b6f50-4c9c-47ff-ff09-75e0f0a8fe88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_loss = 0\n",
        "epochs = classifier.epochs\n",
        "for epoch in range(epochs):\n",
        "  classifier.train()\n",
        "  for inputs, targets in train_dataloader:\n",
        "    if torch.cuda.is_available():\n",
        "      inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "    outputs = classifier(inputs.float())\n",
        "    loss = criterion(outputs.squeeze(), targets.float())\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    classifier.eval()\n",
        "    val_loss = 0\n",
        "    for inputs, targets in val_dataloader:\n",
        "      if torch.cuda.is_available():\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "      outputs = classifier(inputs.float())\n",
        "      loss = criterion(outputs.squeeze(), targets.float())\n",
        "      val_loss += loss.item()\n",
        "\n",
        "    train_loss = train_loss / len(train_dataloader)\n",
        "    val_loss = val_loss / len(val_dataloader)\n",
        "    print('Epoch: {}, Train loss: {}, Val loss: {}'.format(epoch, train_loss, val_loss))\n",
        "\n",
        "    if classifier.val_loss is None or val_loss < classifier.val_loss:\n",
        "      classifier.val_loss = val_loss\n",
        "      classifier.epochs = epoch\n",
        "      save_classifier(classifier)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Train loss: 0.6293988745463522, Val loss: 0.5178971757230005\n",
            "Epoch: 1, Train loss: 0.5174578928138411, Val loss: 0.5150406968436743\n",
            "Epoch: 2, Train loss: 0.4922014182157307, Val loss: 0.48938576524194916\n",
            "Epoch: 3, Train loss: 0.4802163870850858, Val loss: 0.47704749240687017\n",
            "Epoch: 4, Train loss: 0.462813416676876, Val loss: 0.49967874037592036\n",
            "Epoch: 5, Train loss: 0.45928301177414776, Val loss: 0.45506799711208595\n",
            "Epoch: 6, Train loss: 0.4329116129462114, Val loss: 0.4413805445166011\n",
            "Epoch: 7, Train loss: 0.4257573683627452, Val loss: 0.4302190730446263\n",
            "Epoch: 8, Train loss: 0.41540277762258265, Val loss: 0.4196354268412841\n",
            "Epoch: 9, Train loss: 0.4058128236581997, Val loss: 0.42543389746233035\n",
            "Epoch: 10, Train loss: 0.4049193794760006, Val loss: 0.4063084784306978\n",
            "Epoch: 11, Train loss: 0.39976176132035074, Val loss: 0.41631731645841347\n",
            "Epoch: 12, Train loss: 0.40285212494544603, Val loss: 0.4144957171458947\n",
            "Epoch: 13, Train loss: 0.39521982813551043, Val loss: 0.4097483520837207\n",
            "Epoch: 14, Train loss: 0.3940770695462232, Val loss: 0.40632926849158185\n",
            "Epoch: 15, Train loss: 0.3955710626219825, Val loss: 0.4393488337334834\n",
            "Epoch: 16, Train loss: 0.39222508145156426, Val loss: 0.400454727442641\n",
            "Epoch: 17, Train loss: 0.3881209695215766, Val loss: 0.403499034870612\n",
            "Epoch: 18, Train loss: 0.38938910371346364, Val loss: 0.4063251994942364\n",
            "Epoch: 19, Train loss: 0.3849644646849243, Val loss: 0.4057502760306785\n",
            "Epoch: 20, Train loss: 0.3860539268135136, Val loss: 0.4230585839403303\n",
            "Epoch: 21, Train loss: 0.39090763835185216, Val loss: 0.4288782128377965\n",
            "Epoch: 22, Train loss: 0.3826291504418833, Val loss: 0.39695679827740316\n",
            "Epoch: 23, Train loss: 0.3791848035311385, Val loss: 0.3988499976694584\n",
            "Epoch: 24, Train loss: 0.37349939087936673, Val loss: 0.39602230606894745\n",
            "Epoch: 25, Train loss: 0.37686921963041015, Val loss: 0.40167866980558947\n",
            "Epoch: 26, Train loss: 0.37179703116650775, Val loss: 0.39215920649861036\n",
            "Epoch: 27, Train loss: 0.37121469347106667, Val loss: 0.4004918189817353\n",
            "Epoch: 28, Train loss: 0.37239133777545924, Val loss: 0.4016707827778239\n",
            "Epoch: 29, Train loss: 0.3693658268928337, Val loss: 0.40133337833379445\n",
            "Epoch: 30, Train loss: 0.37120953164728016, Val loss: 0.4089109176083615\n",
            "Epoch: 31, Train loss: 0.3719456250631578, Val loss: 0.42648692095750257\n",
            "Epoch: 32, Train loss: 0.37210961406472426, Val loss: 0.3957437765049307\n",
            "Epoch: 33, Train loss: 0.3662691075093503, Val loss: 0.39219926729014043\n",
            "Epoch: 34, Train loss: 0.36523847663216147, Val loss: 0.401362016208862\n",
            "Epoch: 35, Train loss: 0.3641727508902059, Val loss: 0.397581311040803\n",
            "Epoch: 36, Train loss: 0.364811529302282, Val loss: 0.39532721317128133\n",
            "Epoch: 37, Train loss: 0.36249935913461556, Val loss: 0.399344077235774\n",
            "Epoch: 38, Train loss: 0.3591400024891888, Val loss: 0.3964853357327612\n",
            "Epoch: 39, Train loss: 0.3573499398733466, Val loss: 0.40514150203058596\n",
            "Epoch: 40, Train loss: 0.3603224026981814, Val loss: 0.40951538164364665\n",
            "Epoch: 41, Train loss: 0.35526285011471315, Val loss: 0.39643365889787674\n",
            "Epoch: 42, Train loss: 0.349840841910008, Val loss: 0.39868743384354993\n",
            "Epoch: 43, Train loss: 0.34717587400478134, Val loss: 0.3942613046812384\n",
            "Epoch: 44, Train loss: 0.3483486193232897, Val loss: 0.39803649112582207\n",
            "Epoch: 45, Train loss: 0.3491060797002576, Val loss: 0.4286737412606415\n",
            "Epoch: 46, Train loss: 0.34786521307111484, Val loss: 0.4360159180666271\n",
            "Epoch: 47, Train loss: 0.3472418068562371, Val loss: 0.41579538250440046\n",
            "Epoch: 48, Train loss: 0.3441234643488454, Val loss: 0.41583587425319773\n",
            "Epoch: 49, Train loss: 0.34170507473702316, Val loss: 0.42777592629978534\n",
            "Epoch: 50, Train loss: 0.34094830068657145, Val loss: 0.41121852593986613\n",
            "Epoch: 51, Train loss: 0.3364569972776293, Val loss: 0.4072573949631892\n",
            "Epoch: 52, Train loss: 0.33389447114661835, Val loss: 0.4082817447028662\n",
            "Epoch: 53, Train loss: 0.3322178214217998, Val loss: 0.40631685268722084\n",
            "Epoch: 54, Train loss: 0.32739220857470125, Val loss: 0.40085773581736966\n",
            "Epoch: 55, Train loss: 0.3315041054235866, Val loss: 0.4026133812179691\n",
            "Epoch: 56, Train loss: 0.33062402116872497, Val loss: 0.43789382161278473\n",
            "Epoch: 57, Train loss: 0.3261147966791709, Val loss: 0.4208119170446145\n",
            "Epoch: 58, Train loss: 0.3230877689384826, Val loss: 0.4025827340389553\n",
            "Epoch: 59, Train loss: 0.32214556679355316, Val loss: 0.40605769953445386\n",
            "Epoch: 60, Train loss: 0.31981198177139675, Val loss: 0.41288527669875247\n",
            "Epoch: 61, Train loss: 0.31869476829322824, Val loss: 0.41144849575663867\n",
            "Epoch: 62, Train loss: 0.31609851508662173, Val loss: 0.4588309840152138\n",
            "Epoch: 63, Train loss: 0.32080503367262064, Val loss: 0.4117920145784554\n",
            "Epoch: 64, Train loss: 0.3137917853402665, Val loss: 0.4104450460719435\n",
            "Epoch: 65, Train loss: 0.3110845136982505, Val loss: 0.42517131449360596\n",
            "Epoch: 66, Train loss: 0.3083634487587692, Val loss: 0.42321113693086726\n",
            "Epoch: 67, Train loss: 0.31785743516735465, Val loss: 0.4239984419392912\n",
            "Epoch: 68, Train loss: 0.2996857085463384, Val loss: 0.42132171243429184\n",
            "Epoch: 69, Train loss: 0.30420016750598416, Val loss: 0.4215723960415313\n",
            "Epoch: 70, Train loss: 0.30046382376627556, Val loss: 0.43111804990391983\n",
            "Epoch: 71, Train loss: 0.29500025192610957, Val loss: 0.4265138201023403\n",
            "Epoch: 72, Train loss: 0.29115908270857643, Val loss: 0.4177563276730086\n",
            "Epoch: 73, Train loss: 0.2968539283008382, Val loss: 0.4438157416880131\n",
            "Epoch: 74, Train loss: 0.2983214548944669, Val loss: 0.4491341796360518\n",
            "Epoch: 75, Train loss: 0.2955441090404967, Val loss: 0.4350260588291444\n",
            "Epoch: 76, Train loss: 0.2878469371558804, Val loss: 0.43457857637028946\n",
            "Epoch: 77, Train loss: 0.2925492619050218, Val loss: 0.442338304888261\n",
            "Epoch: 78, Train loss: 0.2798683175250435, Val loss: 0.43657000676581736\n",
            "Epoch: 79, Train loss: 0.2779451682609543, Val loss: 0.455429056365239\n",
            "Epoch: 80, Train loss: 0.2770492444290462, Val loss: 0.4398540115278018\n",
            "Epoch: 81, Train loss: 0.279637391655732, Val loss: 0.44355278520991925\n",
            "Epoch: 82, Train loss: 0.27684832626500383, Val loss: 0.43966616905833544\n",
            "Epoch: 83, Train loss: 0.2854058862814565, Val loss: 0.4808773498393987\n",
            "Epoch: 84, Train loss: 0.27900632267220304, Val loss: 0.4648635905039938\n",
            "Epoch: 85, Train loss: 0.2734470972745626, Val loss: 0.45923687831351634\n",
            "Epoch: 86, Train loss: 0.2722886518672083, Val loss: 0.4642695157151473\n",
            "Epoch: 87, Train loss: 0.2776830275226972, Val loss: 0.47042123522413404\n",
            "Epoch: 88, Train loss: 0.2707165340566043, Val loss: 0.449681606143713\n",
            "Epoch: 89, Train loss: 0.26596790280254107, Val loss: 0.4524783566594124\n",
            "Epoch: 90, Train loss: 0.263352886947182, Val loss: 0.46583218049061925\n",
            "Epoch: 91, Train loss: 0.2584794854153715, Val loss: 0.46311945938750315\n",
            "Epoch: 92, Train loss: 0.2636987655702116, Val loss: 0.4651909721525092\n",
            "Epoch: 93, Train loss: 0.2628216992850442, Val loss: 0.464925719915252\n",
            "Epoch: 94, Train loss: 0.2581050908383449, Val loss: 0.4614100164096606\n",
            "Epoch: 95, Train loss: 0.2621919710537235, Val loss: 0.4792253471126682\n",
            "Epoch: 96, Train loss: 0.25952281580447223, Val loss: 0.469274013842407\n",
            "Epoch: 97, Train loss: 0.25015606392007106, Val loss: 0.48221953958272934\n",
            "Epoch: 98, Train loss: 0.25689985927484044, Val loss: 0.47649628139640154\n",
            "Epoch: 99, Train loss: 0.25666130989699554, Val loss: 0.47498164031850665\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXw0R-Tkucgw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(classifier, dataloader):\n",
        "  with torch.no_grad():\n",
        "    classifier.eval()\n",
        "    accuracy = 0\n",
        "    for inputs, targets in dataloader:\n",
        "      if torch.cuda.is_available():\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "      \n",
        "      outputs = classifier(inputs.float())\n",
        "      predictions = torch.round(outputs).int()\n",
        "      equals = (targets == predictions.squeeze())\n",
        "      accuracy += torch.mean(equals.float())\n",
        "\n",
        "    accuracy = accuracy / len(dataloader)\n",
        "    return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlrNw0bEqAeh",
        "colab_type": "code",
        "outputId": "8b910db8-2121-4fe3-bef2-59e31217b71f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "classifier = load_classifier()\n",
        "if torch.cuda.is_available():\n",
        "  classifier = classifier.cuda()\n",
        "train_accuracy = accuracy(classifier, train_dataloader)\n",
        "test_accuracy = accuracy(classifier, test_dataloader)\n",
        "print('Epochs: {}'.format(classifier.epochs))\n",
        "print('Train accuracy: {}'.format(train_accuracy.item()))\n",
        "print('Test accuracy: {}'.format(test_accuracy.item()))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epochs: 26\n",
            "Train accuracy: 0.8419219255447388\n",
            "Test accuracy: 0.8356714248657227\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "918KM5F07hFl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}