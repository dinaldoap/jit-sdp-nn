{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JIT-SDP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dinaldoap/jit-sdp-nn/blob/master/notebook/mlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import random\n",
        "from scipy.stats import mstats\n",
        "\n",
        "from jitsdp import metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fix</th>\n      <th>ns</th>\n      <th>nd</th>\n      <th>nf</th>\n      <th>entropy</th>\n      <th>la</th>\n      <th>ld</th>\n      <th>lt</th>\n      <th>ndev</th>\n      <th>age</th>\n      <th>nuc</th>\n      <th>exp</th>\n      <th>rexp</th>\n      <th>sexp</th>\n      <th>author_date_unix_timestamp</th>\n      <th>classification</th>\n      <th>contains_bug</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>False</td>\n      <td>7.0</td>\n      <td>7.0</td>\n      <td>7.0</td>\n      <td>2.641604</td>\n      <td>9.0</td>\n      <td>9.0</td>\n      <td>426.428571</td>\n      <td>100.0</td>\n      <td>0.000093</td>\n      <td>1.0</td>\n      <td>5171.0</td>\n      <td>30.227271</td>\n      <td>1472.714286</td>\n      <td>1555326371</td>\n      <td>None</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>False</td>\n      <td>7.0</td>\n      <td>7.0</td>\n      <td>7.0</td>\n      <td>2.750000</td>\n      <td>8.0</td>\n      <td>8.0</td>\n      <td>426.428571</td>\n      <td>100.0</td>\n      <td>6.314775</td>\n      <td>2.0</td>\n      <td>5170.0</td>\n      <td>29.227271</td>\n      <td>1471.714286</td>\n      <td>1555326363</td>\n      <td>None</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0.906580</td>\n      <td>15.0</td>\n      <td>44.0</td>\n      <td>96.000000</td>\n      <td>4.0</td>\n      <td>0.034722</td>\n      <td>2.0</td>\n      <td>629.0</td>\n      <td>14.828373</td>\n      <td>414.000000</td>\n      <td>1554971763</td>\n      <td>None</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>40.000000</td>\n      <td>1.0</td>\n      <td>0.000012</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>3.058824</td>\n      <td>3.000000</td>\n      <td>1554969774</td>\n      <td>None</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>4.0</td>\n      <td>1.662506</td>\n      <td>14.0</td>\n      <td>10.0</td>\n      <td>67.000000</td>\n      <td>6.0</td>\n      <td>21.280683</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>2.058824</td>\n      <td>2.000000</td>\n      <td>1554967752</td>\n      <td>Feature Addition</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "     fix   ns   nd   nf   entropy    la    ld          lt   ndev        age  \\\n0  False  7.0  7.0  7.0  2.641604   9.0   9.0  426.428571  100.0   0.000093   \n1  False  7.0  7.0  7.0  2.750000   8.0   8.0  426.428571  100.0   6.314775   \n2  False  1.0  1.0  2.0  0.906580  15.0  44.0   96.000000    4.0   0.034722   \n3  False  1.0  1.0  1.0  0.000000   0.0   0.0   40.000000    1.0   0.000012   \n4  False  1.0  2.0  4.0  1.662506  14.0  10.0   67.000000    6.0  21.280683   \n\n   nuc     exp       rexp         sexp  author_date_unix_timestamp  \\\n0  1.0  5171.0  30.227271  1472.714286                  1555326371   \n1  2.0  5170.0  29.227271  1471.714286                  1555326363   \n2  2.0   629.0  14.828373   414.000000                  1554971763   \n3  1.0     4.0   3.058824     3.000000                  1554969774   \n4  4.0     3.0   2.058824     2.000000                  1554967752   \n\n     classification  contains_bug  \n0              None         False  \n1              None         False  \n2              None         False  \n3              None         False  \n4  Feature Addition         False  "
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/dinaldoap/jit-sdp-data/master/jenkins.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "/home/pytorch/.conda/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  \"\"\"\n"
        }
      ],
      "source": [
        "#df = df.sample(frac=1)\n",
        "label_col = 'contains_bug'\n",
        "features_cols = ['fix', 'ns', 'nd', 'nf', 'entropy', 'la', 'ld', 'lt', 'ndev', 'age', 'nuc', 'exp', 'rexp', 'sexp', 'classification']\n",
        "x = df[features_cols]\n",
        "x['fix'] = x['fix'].astype('int')\n",
        "df_classification = pd.get_dummies(x, columns=['classification'])\n",
        "x = pd.concat([x, df_classification], axis='columns')\n",
        "x = x.drop(['classification'], axis='columns')\n",
        "x = x.values\n",
        "y = df[label_col]\n",
        "y = y.astype('category')\n",
        "y = y.cat.codes\n",
        "y = y.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "[[0.00000000e+00 7.00000000e+00 7.00000000e+00 7.00000000e+00\n  2.64160417e+00 9.00000000e+00 9.00000000e+00 4.26428571e+02\n  1.00000000e+02 9.25925926e-05 1.00000000e+00 5.17100000e+03\n  3.02272705e+01 1.47271429e+03 0.00000000e+00 7.00000000e+00\n  7.00000000e+00 7.00000000e+00 2.64160417e+00 9.00000000e+00\n  9.00000000e+00 4.26428571e+02 1.00000000e+02 9.25925926e-05\n  1.00000000e+00 5.17100000e+03 3.02272705e+01 1.47271429e+03\n  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n  0.00000000e+00 0.00000000e+00]]\n[0]\n"
        }
      ],
      "source": [
        "print(x[:1])\n",
        "print(y[:1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_index = int( len(x) * 0.8 )\n",
        "test_index = int( len(x) * 0.9 )\n",
        "x_train, y_train = x[:val_index], y[:val_index]\n",
        "x_val, y_val = x[val_index:test_index], y[val_index:test_index]\n",
        "x_test, y_test = x[test_index:], y[test_index:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "[[0.00000000e+00 7.00000000e+00 7.00000000e+00 7.00000000e+00\n  2.64160417e+00 9.00000000e+00 9.00000000e+00 4.26428571e+02\n  1.00000000e+02 9.25925926e-05 1.00000000e+00 5.17100000e+03\n  3.02272705e+01 1.47271429e+03 0.00000000e+00 7.00000000e+00\n  7.00000000e+00 7.00000000e+00 2.64160417e+00 9.00000000e+00\n  9.00000000e+00 4.26428571e+02 1.00000000e+02 9.25925926e-05\n  1.00000000e+00 5.17100000e+03 3.02272705e+01 1.47271429e+03\n  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n  0.00000000e+00 0.00000000e+00]]\n[[0.00000000e+00 8.00000000e+00 8.00000000e+00 8.00000000e+00\n  2.73215889e+00 1.10000000e+01 1.10000000e+01 1.58625000e+02\n  5.00000000e+00 1.96759259e-04 1.00000000e+00 4.17300000e+03\n  2.45570735e+02 8.19125000e+02 0.00000000e+00 8.00000000e+00\n  8.00000000e+00 8.00000000e+00 2.73215889e+00 1.10000000e+01\n  1.10000000e+01 1.58625000e+02 5.00000000e+00 1.96759259e-04\n  1.00000000e+00 4.17300000e+03 2.45570735e+02 8.19125000e+02\n  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n  0.00000000e+00 0.00000000e+00]]\n[[0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n  0.00000000e+00 1.00000000e+00 1.00000000e+00 2.31000000e+02\n  2.00000000e+00 8.17129630e-03 1.00000000e+00 2.29000000e+03\n  2.35138387e+02 1.96900000e+03 0.00000000e+00 1.00000000e+00\n  1.00000000e+00 1.00000000e+00 0.00000000e+00 1.00000000e+00\n  1.00000000e+00 2.31000000e+02 2.00000000e+00 8.17129630e-03\n  1.00000000e+00 2.29000000e+03 2.35138387e+02 1.96900000e+03\n  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n  0.00000000e+00 0.00000000e+00]]\n"
        }
      ],
      "source": [
        "print(x_train[:1])\n",
        "print(x_val[:1])\n",
        "print(x_test[:1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "mean_train = x_train.mean(axis=0)\n",
        "std_train = x_train.std(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "[2.90871477e-01 1.61432635e+00 2.60858547e+00 4.18908715e+00\n 7.43390526e-01 5.00800621e+01 2.96331523e+01 8.11968929e+02\n 2.32963538e+01 9.76994760e+01 2.02296354e+00 1.73095790e+03\n 6.53773667e+01 8.28714959e+02 2.90871477e-01 1.61432635e+00\n 2.60858547e+00 4.18908715e+00 7.43390526e-01 5.00800621e+01\n 2.96331523e+01 8.11968929e+02 2.32963538e+01 9.76994760e+01\n 2.02296354e+00 1.73095790e+03 6.53773667e+01 8.28714959e+02\n 2.90871477e-01 1.47142488e-01 3.46521852e-02 4.71373157e-01\n 1.51538660e-02 4.08068270e-02]\n[4.54164354e-01 1.62736223e+00 6.93052365e+00 2.56616350e+01\n 1.13236779e+00 5.86250473e+02 6.19852440e+02 1.15236538e+03\n 2.84287684e+01 2.37802272e+02 7.67653474e+00 2.11471132e+03\n 6.96820390e+01 1.34456679e+03 4.54164354e-01 1.62736223e+00\n 6.93052365e+00 2.56616350e+01 1.13236779e+00 5.86250473e+02\n 6.19852440e+02 1.15236538e+03 2.84287684e+01 2.37802272e+02\n 7.67653474e+00 2.11471132e+03 6.96820390e+01 1.34456679e+03\n 4.54164354e-01 3.54247902e-01 1.82897270e-01 4.99179831e-01\n 1.22164751e-01 1.97842437e-01]\n"
        }
      ],
      "source": [
        "print(mean_train)\n",
        "print(std_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train = (x_train - mean_train) / std_train\n",
        "x_val = (x_val - mean_train) / std_train\n",
        "x_test = (x_test - mean_train) / std_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "[[-0.64045422  3.30944983  0.63363387  0.10953756  1.67632253 -0.07007254\n  -0.0332872  -0.33456434  2.69809951 -0.41084293 -0.1332585   1.62671948\n  -0.50443553  0.47896418 -0.64045422  3.30944983  0.63363387  0.10953756\n   1.67632253 -0.07007254 -0.0332872  -0.33456434  2.69809951 -0.41084293\n  -0.1332585   1.62671948 -0.50443553  0.47896418 -0.64045422 -0.41536587\n  -0.18946256  1.05899079 -0.1240445  -0.20625922]]\n[[-0.64045422  3.92394118  0.77792311  0.14850624  1.75629188 -0.06666103\n  -0.03006063 -0.566959   -0.64358587 -0.4108425  -0.1332585   1.15478745\n   2.58593708 -0.00713238 -0.64045422  3.92394118  0.77792311  0.14850624\n   1.75629188 -0.06666103 -0.03006063 -0.566959   -0.64358587 -0.4108425\n  -0.1332585   1.15478745  2.58593708 -0.00713238 -0.64045422 -0.41536587\n  -0.18946256  1.05899079 -0.1240445  -0.20625922]]\n[[-0.64045422 -0.37749822 -0.23210158 -0.12427451 -0.65649211 -0.08371859\n  -0.0461935  -0.5041534  -0.74911278 -0.41080896 -0.1332585   0.26435859\n   2.43622349  0.84806872 -0.64045422 -0.37749822 -0.23210158 -0.12427451\n  -0.65649211 -0.08371859 -0.0461935  -0.5041534  -0.74911278 -0.41080896\n  -0.1332585   0.26435859  2.43622349  0.84806872 -0.64045422 -0.41536587\n  -0.18946256  1.05899079 -0.1240445  -0.20625922]]\n"
        }
      ],
      "source": [
        "print(x_train[:1])\n",
        "print(x_val[:1])\n",
        "print(x_test[:1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calc_fading_weights(size, fading_factor):  \n",
        "  fading_weights = reversed(range(size))\n",
        "  return [fading_factor**x for x in fading_weights]\n",
        "  \n",
        "def create_sampler(y):\n",
        "  n_samples = len(y)\n",
        "  fading_count = calc_fading_weights(n_samples, 0.9999) \n",
        "  total = np.sum(fading_count)\n",
        "  bug = np.sum(fading_count * y)\n",
        "  normal = total - bug\n",
        "  class_weights = total / [normal, bug]\n",
        "  class_weights = class_weights[y]\n",
        "\n",
        "  instance_weights = calc_fading_weights(n_samples, 0.9999) \n",
        "  weights = instance_weights * class_weights\n",
        "  return data.WeightedRandomSampler(weights=weights, num_samples=n_samples, replacement=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "sampler = create_sampler(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train, y_train = torch.from_numpy(x_train), torch.from_numpy(y_train)\n",
        "x_val, y_val = torch.from_numpy(x_val), torch.from_numpy(y_val)\n",
        "x_test, y_test = torch.from_numpy(x_test), torch.from_numpy(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset = data.TensorDataset(x_train, y_train)\n",
        "val_dataset = data.TensorDataset(x_val, y_val)\n",
        "test_dataset = data.TensorDataset(x_test, y_test)\n",
        "\n",
        "train_dataloader = data.DataLoader(train_dataset, batch_size=512, sampler=sampler)\n",
        "val_dataloader = data.DataLoader(val_dataset, batch_size=32)\n",
        "test_dataloader = data.DataLoader(test_dataset, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "tensor([[-0.6405, -0.3775, -0.2321, -0.1243, -0.6565, -0.0786, -0.0381,  0.5771,\n         -0.4677, -0.3812, -0.1333,  1.9355,  1.8576,  2.9856, -0.6405, -0.3775,\n         -0.2321, -0.1243, -0.6565, -0.0786, -0.0381,  0.5771, -0.4677, -0.3812,\n         -0.1333,  1.9355,  1.8576,  2.9856, -0.6405, -0.4154, -0.1895,  1.0590,\n         -0.1240, -0.2063]], dtype=torch.float64)\ntensor([1], dtype=torch.int8)\ntensor([[-0.6405,  3.9239,  0.7779,  0.1485,  1.7563, -0.0667, -0.0301, -0.5670,\n         -0.6436, -0.4108, -0.1333,  1.1548,  2.5859, -0.0071, -0.6405,  3.9239,\n          0.7779,  0.1485,  1.7563, -0.0667, -0.0301, -0.5670, -0.6436, -0.4108,\n         -0.1333,  1.1548,  2.5859, -0.0071, -0.6405, -0.4154, -0.1895,  1.0590,\n         -0.1240, -0.2063]], dtype=torch.float64)\ntensor([0], dtype=torch.int8)\ntensor([[-0.6405, -0.3775, -0.2321, -0.1243, -0.6565, -0.0837, -0.0462, -0.5042,\n         -0.7491, -0.4108, -0.1333,  0.2644,  2.4362,  0.8481, -0.6405, -0.3775,\n         -0.2321, -0.1243, -0.6565, -0.0837, -0.0462, -0.5042, -0.7491, -0.4108,\n         -0.1333,  0.2644,  2.4362,  0.8481, -0.6405, -0.4154, -0.1895,  1.0590,\n         -0.1240, -0.2063]], dtype=torch.float64)\ntensor([1], dtype=torch.int8)\n"
        }
      ],
      "source": [
        "inputs, targets = next(iter(train_dataloader))\n",
        "print(inputs[:1])\n",
        "print(targets[:1])\n",
        "\n",
        "inputs, targets = next(iter(val_dataloader))\n",
        "print(inputs[:1])\n",
        "print(targets[:1])\n",
        "\n",
        "inputs, targets = next(iter(test_dataloader))\n",
        "print(inputs[:1])\n",
        "print(targets[:1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Classifier(nn.Module):\n",
        "  FILENAME = 'models/classifier.cpt'\n",
        "  def __init__(self, input_size, hidden_size, drop_prob, epoch=None, val_gmean=None):\n",
        "    super(Classifier, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.drop_prob = drop_prob\n",
        "    self.epoch = epoch\n",
        "    self.val_gmean = val_gmean\n",
        "    self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "    self.fcout = nn.Linear(hidden_size, 1)\n",
        "    self.dropout = nn.Dropout(drop_prob)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = torch.relu(self.fc1(x))\n",
        "    x = self.dropout(x)\n",
        "    x = torch.sigmoid(self.fcout(x))\n",
        "    return x\n",
        "\n",
        "  def save(self):\n",
        "    checkpoint = {\n",
        "        'input_size': self.input_size,\n",
        "        'hidden_size': self.hidden_size,\n",
        "        'drop_prob': self.drop_prob,\n",
        "        'val_gmean': self.val_gmean,\n",
        "        'epoch': self.epoch,\n",
        "        'state_dict': self.state_dict()\n",
        "    }\n",
        "    with open(Classifier.FILENAME, 'wb') as f:\n",
        "      torch.save(checkpoint, f)\n",
        "\n",
        "  def load(self):\n",
        "    with open(Classifier.FILENAME, 'rb') as f:\n",
        "      checkpoint = torch.load(f)\n",
        "      self.input_size = checkpoint['input_size']\n",
        "      self.hidden_size = checkpoint['hidden_size']\n",
        "      self.drop_prob = checkpoint['drop_prob']\n",
        "      self.epoch = checkpoint['epoch']\n",
        "      self.val_gmean = checkpoint['val_gmean']\n",
        "      self.load_state_dict(checkpoint['state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "classifier = Classifier(input_size=x.shape[1], hidden_size=x.shape[1], drop_prob=0.5)\n",
        "classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "classifier.save()\n",
        "classifier.load()\n",
        "classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": "tensor([[0.5747],\n        [0.4403],\n        [0.6097]], grad_fn=<SigmoidBackward>)"
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "  classifier = classifier.cuda()\n",
        "  inputs = inputs.cuda()\n",
        "\n",
        "classifier(inputs[:3].float())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(params=classifier.parameters(), lr=0.003)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "n: 0.7146341935217179\nEpoch: 9, Train loss: 0.5626559176440501, Val loss: 0.5275704515607733, Val g-mean: 0.714822330035359\nEpoch: 10, Train loss: 0.5582143270090888, Val loss: 0.5511370650247523, Val g-mean: 0.7190957145724548\nEpoch: 11, Train loss: 0.5641191575692298, Val loss: 0.5180769962699789, Val g-mean: 0.7134546727584911\nEpoch: 12, Train loss: 0.5598072157977082, Val loss: 0.5099656593642736, Val g-mean: 0.7239005990901451\nEpoch: 13, Train loss: 0.5540287942914536, Val loss: 0.5080413998741853, Val g-mean: 0.717797671898113\nEpoch: 14, Train loss: 0.552680328883697, Val loss: 0.5151356046921328, Val g-mean: 0.7238348921244395\nEpoch: 15, Train loss: 0.5450032028032491, Val loss: 0.5131815275863597, Val g-mean: 0.7229667687913341\nEpoch: 16, Train loss: 0.5521451869357917, Val loss: 0.5028435399657801, Val g-mean: 0.7232398075301184\nEpoch: 17, Train loss: 0.5452191626783769, Val loss: 0.5105633704285872, Val g-mean: 0.7248085496947541\nEpoch: 18, Train loss: 0.5486727163346101, Val loss: 0.49078010728484706, Val g-mean: 0.7276043652105115\nEpoch: 19, Train loss: 0.5431952040058257, Val loss: 0.4838260180856052, Val g-mean: 0.7142303072961642\nEpoch: 20, Train loss: 0.5457536166714521, Val loss: 0.48203604550738083, Val g-mean: 0.7354663905196736\nEpoch: 21, Train loss: 0.5433368421066406, Val loss: 0.4890753775835037, Val g-mean: 0.7242997842041983\nEpoch: 22, Train loss: 0.5458423284492696, Val loss: 0.4842630036567387, Val g-mean: 0.7269076877153048\nEpoch: 23, Train loss: 0.5427623871852478, Val loss: 0.4906844696716258, Val g-mean: 0.7321668479527137\nEpoch: 24, Train loss: 0.5351023804302004, Val loss: 0.4866694796242212, Val g-mean: 0.7252991155125826\nEpoch: 25, Train loss: 0.5406662393849015, Val loss: 0.46663840467992584, Val g-mean: 0.7314269898177644\nEpoch: 26, Train loss: 0.5366362925925823, Val loss: 0.47422225655693756, Val g-mean: 0.7340488885731344\nEpoch: 27, Train loss: 0.5363497937890495, Val loss: 0.48132147090999705, Val g-mean: 0.7342874034990938\nEpoch: 28, Train loss: 0.5354314702779908, Val loss: 0.47182717684068176, Val g-mean: 0.7323476751268724\nEpoch: 29, Train loss: 0.5348774044608622, Val loss: 0.4811478921849477, Val g-mean: 0.7363302318584202\nEpoch: 30, Train loss: 0.5451927949694572, Val loss: 0.4998029431230144, Val g-mean: 0.7490726716971783\nEpoch: 31, Train loss: 0.5374872218352342, Val loss: 0.49511178582906723, Val g-mean: 0.7402407754100764\nEpoch: 32, Train loss: 0.533324433342359, Val loss: 0.5088433374307657, Val g-mean: 0.7475292988605672\nEpoch: 33, Train loss: 0.5382740689206473, Val loss: 0.46570060813897535, Val g-mean: 0.7430008915811722\nEpoch: 34, Train loss: 0.5309734247569671, Val loss: 0.4635403338623674, Val g-mean: 0.7326279331376436\nEpoch: 35, Train loss: 0.5339181522592233, Val loss: 0.472374522372296, Val g-mean: 0.7472923624185361\nEpoch: 36, Train loss: 0.5357141756176539, Val loss: 0.4759039208292961, Val g-mean: 0.7339318422945722\nEpoch: 37, Train loss: 0.5379401860089669, Val loss: 0.48336244158838926, Val g-mean: 0.7398236743538058\nEpoch: 38, Train loss: 0.5306769463510147, Val loss: 0.4845043536471693, Val g-mean: 0.7382261239086622\nEpoch: 39, Train loss: 0.527549167612903, Val loss: 0.48821982584501566, Val g-mean: 0.7436737149398215\nEpoch: 40, Train loss: 0.5327040040738675, Val loss: 0.48298825284368113, Val g-mean: 0.7414615644438912\nEpoch: 41, Train loss: 0.5378403179589262, Val loss: 0.4832338996623692, Val g-mean: 0.7452644833912481\nEpoch: 42, Train loss: 0.5347776980586829, Val loss: 0.46268872132426814, Val g-mean: 0.7332667110692657\nEpoch: 43, Train loss: 0.526303705341429, Val loss: 0.4768647471148717, Val g-mean: 0.7409709784854058\nEpoch: 44, Train loss: 0.5259299481324672, Val loss: 0.4740809396301445, Val g-mean: 0.7474111227577162\nEpoch: 45, Train loss: 0.5322726993822537, Val loss: 0.4530118852853775, Val g-mean: 0.74324458977499\nEpoch: 46, Train loss: 0.5326202018456069, Val loss: 0.4908554218709469, Val g-mean: 0.7468229301467413\nEpoch: 47, Train loss: 0.5277262450399012, Val loss: 0.4452772671846967, Val g-mean: 0.7348716095968714\nEpoch: 48, Train loss: 0.5301847978671967, Val loss: 0.44659179272620303, Val g-mean: 0.7440158242636978\nEpoch: 49, Train loss: 0.524578303684897, Val loss: 0.46383523686151756, Val g-mean: 0.7380818294783243\nEpoch: 50, Train loss: 0.5272458117413444, Val loss: 0.4507750210009123, Val g-mean: 0.7449621489268085\nEpoch: 51, Train loss: 0.5244635678728629, Val loss: 0.4609876818170673, Val g-mean: 0.7303289925187537\nEpoch: 52, Train loss: 0.5269350975161065, Val loss: 0.46658213181715263, Val g-mean: 0.7424918296843867\nEpoch: 53, Train loss: 0.5297649298043765, Val loss: 0.45939929920591804, Val g-mean: 0.7497230299886293\nEpoch: 54, Train loss: 0.5235999252553878, Val loss: 0.4502270637374175, Val g-mean: 0.7420119496779531\nEpoch: 55, Train loss: 0.5299171343345289, Val loss: 0.4450512585279189, Val g-mean: 0.7437384614536519\nEpoch: 56, Train loss: 0.527784605127936, Val loss: 0.465523351179926, Val g-mean: 0.7492995228655868\nEpoch: 57, Train loss: 0.5293171167714126, Val loss: 0.4560919025618779, Val g-mean: 0.7383239517923559\nEpoch: 58, Train loss: 0.5255257763369636, Val loss: 0.45591258022345993, Val g-mean: 0.7478766564886817\nEpoch: 59, Train loss: 0.5266198358460051, Val loss: 0.4378727456848872, Val g-mean: 0.7466338099602058\nEpoch: 60, Train loss: 0.5276809746275968, Val loss: 0.4741647445449704, Val g-mean: 0.7500912705816359\nEpoch: 61, Train loss: 0.5273090000065027, Val loss: 0.4470979657612349, Val g-mean: 0.7513042345653087\nEpoch: 62, Train loss: 0.5164453394975366, Val loss: 0.46768653902568313, Val g-mean: 0.7424408738403216\nEpoch: 63, Train loss: 0.530139930958722, Val loss: 0.46718381384485647, Val g-mean: 0.7435463581902643\nEpoch: 64, Train loss: 0.525801290229621, Val loss: 0.45506282719342334, Val g-mean: 0.7449250669580972\nEpoch: 65, Train loss: 0.5224008478918531, Val loss: 0.4739910942551337, Val g-mean: 0.7513156899455921\nEpoch: 66, Train loss: 0.5250309240875507, Val loss: 0.4657014569169597, Val g-mean: 0.7384395945757094\nEpoch: 67, Train loss: 0.5290402389201624, Val loss: 0.44569265136593267, Val g-mean: 0.7482811357784871\nEpoch: 68, Train loss: 0.5181812139891561, Val loss: 0.4575815673329328, Val g-mean: 0.7468973048220818\nEpoch: 69, Train loss: 0.5233391117959246, Val loss: 0.4527986132000622, Val g-mean: 0.7475519029908991\nEpoch: 70, Train loss: 0.5179567577334585, Val loss: 0.44558515987898173, Val g-mean: 0.7562645578042669\nEpoch: 71, Train loss: 0.5215975562258047, Val loss: 0.4505417335190271, Val g-mean: 0.7456773144098787\nEpoch: 72, Train loss: 0.5266572313338087, Val loss: 0.4467491695755406, Val g-mean: 0.7500446773838392\nEpoch: 73, Train loss: 0.5147051828912202, Val loss: 0.4505711768410708, Val g-mean: 0.7548285395224148\nEpoch: 74, Train loss: 0.5193807789050991, Val loss: 0.459279641705124, Val g-mean: 0.7613852797843307\nEpoch: 75, Train loss: 0.5214979374313654, Val loss: 0.44736586118999283, Val g-mean: 0.7520628281329232\nEpoch: 76, Train loss: 0.523564296395905, Val loss: 0.44820136753352063, Val g-mean: 0.7377686921898978\nEpoch: 77, Train loss: 0.5256317380078875, Val loss: 0.44539673351927805, Val g-mean: 0.7463989665895046\nEpoch: 78, Train loss: 0.519408737679867, Val loss: 0.4450207054615021, Val g-mean: 0.7571961101707329\nEpoch: 79, Train loss: 0.515460219170455, Val loss: 0.46734278747125674, Val g-mean: 0.7535179477724133\nEpoch: 80, Train loss: 0.5247615770861195, Val loss: 0.4521090588287303, Val g-mean: 0.742326915332568\nEpoch: 81, Train loss: 0.5234612459058423, Val loss: 0.4448924843025835, Val g-mean: 0.7506017645126585\nEpoch: 82, Train loss: 0.5198258544115509, Val loss: 0.48332813107653666, Val g-mean: 0.7468777773468963\nEpoch: 83, Train loss: 0.5289903881390583, Val loss: 0.4339291033776183, Val g-mean: 0.7466456143055924\nEpoch: 84, Train loss: 0.5173479836296608, Val loss: 0.46809280291199684, Val g-mean: 0.7495389119860043\nEpoch: 85, Train loss: 0.5197103829266512, Val loss: 0.4518578901494804, Val g-mean: 0.7517209155822985\nEpoch: 86, Train loss: 0.5201871251617657, Val loss: 0.4358852729201317, Val g-mean: 0.7448690773171841\nEpoch: 87, Train loss: 0.5189642055399449, Val loss: 0.47911417425463076, Val g-mean: 0.7522108655828428\nEpoch: 88, Train loss: 0.523000354159211, Val loss: 0.44549109531860603, Val g-mean: 0.7592040659848679\nEpoch: 89, Train loss: 0.5204683242813625, Val loss: 0.452565710991621, Val g-mean: 0.7422892743190519\nEpoch: 90, Train loss: 0.5213552046726039, Val loss: 0.43246125214194, Val g-mean: 0.7446351220528525\nEpoch: 91, Train loss: 0.516008104747089, Val loss: 0.44033374774612877, Val g-mean: 0.7495103427420559\nEpoch: 92, Train loss: 0.5228247351255738, Val loss: 0.44981419510747256, Val g-mean: 0.7581878413682259\nEpoch: 93, Train loss: 0.5148669952018395, Val loss: 0.45231531442780243, Val g-mean: 0.7444824281330963\nEpoch: 94, Train loss: 0.5172593480501868, Val loss: 0.4340946307699931, Val g-mean: 0.7545534190923954\nEpoch: 95, Train loss: 0.5178976063826479, Val loss: 0.4356783981385984, Val g-mean: 0.7493189104878416\nEpoch: 96, Train loss: 0.516735712961405, Val loss: 0.44571329888544586, Val g-mean: 0.749385260304788\nEpoch: 97, Train loss: 0.5245564731661702, Val loss: 0.4403702799034746, Val g-mean: 0.7520658885991558\nEpoch: 98, Train loss: 0.5188169169653514, Val loss: 0.452959766317355, Val g-mean: 0.7494857824899566\nEpoch: 99, Train loss: 0.5159489912114359, Val loss: 0.44652150316458, Val g-mean: 0.7585471760700404\nEpoch: 100, Train loss: 0.5224419889803567, Val loss: 0.4433941025482981, Val g-mean: 0.7504073056504366\nEpoch: 101, Train loss: 0.5210452787132531, Val loss: 0.43972142844607953, Val g-mean: 0.7465519939880445\nEpoch: 102, Train loss: 0.5117781227932987, Val loss: 0.4504862080671285, Val g-mean: 0.7524541440253801\nEpoch: 103, Train loss: 0.5181975029176438, Val loss: 0.4512156373576114, Val g-mean: 0.7457508290747109\nEpoch: 104, Train loss: 0.5206270812955498, Val loss: 0.45313417617427676, Val g-mean: 0.7574510845718171\nEpoch: 105, Train loss: 0.5109634422916253, Val loss: 0.43574660428260503, Val g-mean: 0.7475325867040673\nEpoch: 106, Train loss: 0.5151023590869498, Val loss: 0.4503569240240674, Val g-mean: 0.7447110899517952\nEpoch: 107, Train loss: 0.5148474496297324, Val loss: 0.4307724521740487, Val g-mean: 0.7447877897738068\nEpoch: 108, Train loss: 0.5215719395083616, Val loss: 0.4199937062436028, Val g-mean: 0.7488380720297577\nEpoch: 109, Train loss: 0.5189928186389502, Val loss: 0.4464999756922847, Val g-mean: 0.748072348331973\nEpoch: 110, Train loss: 0.5168368786898953, Val loss: 0.42760860645457316, Val g-mean: 0.7532140031542841\nEpoch: 111, Train loss: 0.5221252730811052, Val loss: 0.44615782601268666, Val g-mean: 0.7473722097381906\nEpoch: 112, Train loss: 0.5162337296660647, Val loss: 0.4210082076882061, Val g-mean: 0.7377339824311023\nEpoch: 113, Train loss: 0.5189583070797248, Val loss: 0.4433608127659873, Val g-mean: 0.7429707617635923\nEpoch: 114, Train loss: 0.5134558514746723, Val loss: 0.4468993613202321, Val g-mean: 0.7520516302276039\nEpoch: 115, Train loss: 0.5117966253924597, Val loss: 0.4510545195325425, Val g-mean: 0.7550405417021824\nEpoch: 116, Train loss: 0.5173391088565406, Val loss: 0.44072434992382403, Val g-mean: 0.750721927147241\nEpoch: 117, Train loss: 0.5165426064079274, Val loss: 0.446284896056903, Val g-mean: 0.7448211225311894\nEpoch: 118, Train loss: 0.5207458536576421, Val loss: 0.4447281619435863, Val g-mean: 0.7511979812000297\nEpoch: 119, Train loss: 0.5234648109547695, Val loss: 0.4571610446038999, Val g-mean: 0.7519665682345984\nEpoch: 120, Train loss: 0.5279762097123224, Val loss: 0.44186387171870783, Val g-mean: 0.7500082103158502\nEpoch: 121, Train loss: 0.5225832876613342, Val loss: 0.4410538753788722, Val g-mean: 0.7468587189113579\nEpoch: 122, Train loss: 0.5152431039356626, Val loss: 0.43715838401725415, Val g-mean: 0.745039494203216\nEpoch: 123, Train loss: 0.5197050529831947, Val loss: 0.45508367019264323, Val g-mean: 0.7496997043592677\nEpoch: 124, Train loss: 0.5213590106371053, Val loss: 0.4288076066264981, Val g-mean: 0.747626483935631\nEpoch: 125, Train loss: 0.5136340954350876, Val loss: 0.43029553658868136, Val g-mean: 0.7501004640136865\nEpoch: 126, Train loss: 0.5157551140598232, Val loss: 0.4217244619993787, Val g-mean: 0.7466630926511078\nEpoch: 127, Train loss: 0.5215304030426023, Val loss: 0.43206616334225, Val g-mean: 0.7559590872540987\nEpoch: 128, Train loss: 0.5193315834158546, Val loss: 0.42878795277915505, Val g-mean: 0.7540959250846471\nEpoch: 129, Train loss: 0.5167995881886699, Val loss: 0.45313240646531705, Val g-mean: 0.754558735098368\nEpoch: 130, Train loss: 0.519149828615553, Val loss: 0.47411050235754565, Val g-mean: 0.7551403521052422\nEpoch: 131, Train loss: 0.5239708875528231, Val loss: 0.4583439209351414, Val g-mean: 0.7503307641588178\nEpoch: 132, Train loss: 0.5180814976099061, Val loss: 0.42774389586166334, Val g-mean: 0.7579065003113201\nEpoch: 133, Train loss: 0.5175051731357525, Val loss: 0.4522669054567814, Val g-mean: 0.758808371941361\nEpoch: 134, Train loss: 0.5220145835230475, Val loss: 0.4272069078134863, Val g-mean: 0.7546926353101189\nEpoch: 135, Train loss: 0.5118138992832149, Val loss: 0.44641839222688423, Val g-mean: 0.7494697916607491\nEpoch: 136, Train loss: 0.5174906894408978, Val loss: 0.452794329116219, Val g-mean: 0.7509219758130393\nEpoch: 137, Train loss: 0.5195584447364099, Val loss: 0.4349365216728888, Val g-mean: 0.7593806267406149\nEpoch: 138, Train loss: 0.5269190163235395, Val loss: 0.4485144597527228, Val g-mean: 0.7545385965637682\nEpoch: 139, Train loss: 0.5153634754472148, Val loss: 0.42029116185087906, Val g-mean: 0.7543855250752036\nEpoch: 140, Train loss: 0.522802218553406, Val loss: 0.45147473325854853, Val g-mean: 0.7571599309370649\nEpoch: 141, Train loss: 0.5142120173823672, Val loss: 0.4312384473650079, Val g-mean: 0.7567672526844169\nEpoch: 142, Train loss: 0.515512853210568, Val loss: 0.43792464740966497, Val g-mean: 0.7632815378815149\nEpoch: 143, Train loss: 0.518534471312777, Val loss: 0.44697528273651477, Val g-mean: 0.7565798223468131\nEpoch: 144, Train loss: 0.5164841685305461, Val loss: 0.4361752359882781, Val g-mean: 0.7633459207627087\nEpoch: 145, Train loss: 0.5127568942245957, Val loss: 0.4521592873099603, Val g-mean: 0.7663334921880492\nEpoch: 146, Train loss: 0.5178892685706602, Val loss: 0.43777109996268626, Val g-mean: 0.7597774631331196\nEpoch: 147, Train loss: 0.5170946530964687, Val loss: 0.4237869789725856, Val g-mean: 0.7576994697658067\nEpoch: 148, Train loss: 0.5138859430229107, Val loss: 0.42291737230200516, Val g-mean: 0.7534965042340594\nEpoch: 149, Train loss: 0.528497789196998, Val loss: 0.4359227309101506, Val g-mean: 0.7585100907704546\nEpoch: 150, Train loss: 0.5145343911724349, Val loss: 0.4146133367168276, Val g-mean: 0.7474168238523904\nEpoch: 151, Train loss: 0.5147712154139475, Val loss: 0.43507151952699613, Val g-mean: 0.7558331162798356\nEpoch: 152, Train loss: 0.5149339960932454, Val loss: 0.4577720414258932, Val g-mean: 0.7556964216911081\nEpoch: 153, Train loss: 0.5138649758197752, Val loss: 0.44051978639081907, Val g-mean: 0.7606272417061047\nEpoch: 154, Train loss: 0.5226113556155455, Val loss: 0.4401895237204276, Val g-mean: 0.7561030652727574\nEpoch: 155, Train loss: 0.5206079280646456, Val loss: 0.4384193167482552, Val g-mean: 0.7560066632461794\nEpoch: 156, Train loss: 0.5227084978010357, Val loss: 0.43153533104218933, Val g-mean: 0.7580530173228067\nEpoch: 157, Train loss: 0.5129690317302056, Val loss: 0.42891078363907964, Val g-mean: 0.7603750073298059\nEpoch: 158, Train loss: 0.5114826328607743, Val loss: 0.42747327606928975, Val g-mean: 0.7534827092378877\nEpoch: 159, Train loss: 0.5137954763860356, Val loss: 0.4358851272416742, Val g-mean: 0.7564460381108498\nEpoch: 160, Train loss: 0.507320431795797, Val loss: 0.43387861941990097, Val g-mean: 0.7580059494889162\nEpoch: 161, Train loss: 0.5083229580450236, Val loss: 0.43850096942562805, Val g-mean: 0.7546825192986842\nEpoch: 162, Train loss: 0.5094088089542644, Val loss: 0.43090860427994476, Val g-mean: 0.7538844270040906\nEpoch: 163, Train loss: 0.5140678837426844, Val loss: 0.4520291831148298, Val g-mean: 0.7568945139112695\nEpoch: 164, Train loss: 0.5162677732332974, Val loss: 0.436250697233175, Val g-mean: 0.7639563342539468\nEpoch: 165, Train loss: 0.5127337942709641, Val loss: 0.4504042912862803, Val g-mean: 0.7506303404707951\nEpoch: 166, Train loss: 0.5204898144335864, Val loss: 0.4438245400394264, Val g-mean: 0.7435914116448901\nEpoch: 167, Train loss: 0.513837973604943, Val loss: 0.4367174639513618, Val g-mean: 0.7514196407474232\nEpoch: 168, Train loss: 0.5148545004383082, Val loss: 0.4400186401448752, Val g-mean: 0.7523561496168161\nEpoch: 169, Train loss: 0.5157572613929848, Val loss: 0.4398308053220573, Val g-mean: 0.7537984426335897\nEpoch: 170, Train loss: 0.5084008087891632, Val loss: 0.43706226388090536, Val g-mean: 0.7594483044135586\nEpoch: 171, Train loss: 0.5112065362046118, Val loss: 0.4493906233263643, Val g-mean: 0.7640032895924714\nEpoch: 172, Train loss: 0.5170832911267352, Val loss: 0.42708347404473707, Val g-mean: 0.7581206270390776\nEpoch: 173, Train loss: 0.5117543521134855, Val loss: 0.4381584178068136, Val g-mean: 0.7558497295423994\nEpoch: 174, Train loss: 0.512292671053579, Val loss: 0.4338820411970741, Val g-mean: 0.7665123612706368\nEpoch: 175, Train loss: 0.5161829548407273, Val loss: 0.4357669739738891, Val g-mean: 0.7615125950484213\nEpoch: 176, Train loss: 0.5151544110224139, Val loss: 0.4359507350937316, Val g-mean: 0.7671735531003301\nEpoch: 177, Train loss: 0.5159800575392431, Val loss: 0.4368788437231591, Val g-mean: 0.7495139371359846\nEpoch: 178, Train loss: 0.5136329914033858, Val loss: 0.4381309902589572, Val g-mean: 0.7537374492284462\nEpoch: 179, Train loss: 0.5126851156220635, Val loss: 0.44358887170490463, Val g-mean: 0.7639751073494896\nEpoch: 180, Train loss: 0.5095276870614233, Val loss: 0.42597201505773946, Val g-mean: 0.7557492268592171\nEpoch: 181, Train loss: 0.5178008253069455, Val loss: 0.4363543255940864, Val g-mean: 0.7616515247307609\nEpoch: 182, Train loss: 0.5188810335823625, Val loss: 0.4441049402873767, Val g-mean: 0.7474916148752737\nEpoch: 183, Train loss: 0.5159769249255948, Val loss: 0.4575719451041598, Val g-mean: 0.7548558341256872\nEpoch: 184, Train loss: 0.5187715430330172, Val loss: 0.422237272325315, Val g-mean: 0.751964773568917\nEpoch: 185, Train loss: 0.5132559000381015, Val loss: 0.4334636612942344, Val g-mean: 0.763336396749315\nEpoch: 186, Train loss: 0.5168117396113334, Val loss: 0.4473258964717388, Val g-mean: 0.7570496328640581\nEpoch: 187, Train loss: 0.515320293506188, Val loss: 0.44157524759832184, Val g-mean: 0.7581491095355124\nEpoch: 188, Train loss: 0.5103625425748711, Val loss: 0.4428995912404437, Val g-mean: 0.758172795067717\nEpoch: 189, Train loss: 0.5222946767980924, Val loss: 0.46318939387014035, Val g-mean: 0.7525867880138878\nEpoch: 190, Train loss: 0.5131420502667234, Val loss: 0.43708370939681407, Val g-mean: 0.7590384479069565\nEpoch: 191, Train loss: 0.517012420478632, Val loss: 0.4569685639519441, Val g-mean: 0.7519446352088338\nEpoch: 192, Train loss: 0.5087708957149054, Val loss: 0.4352974181896762, Val g-mean: 0.7596897948061119\nEpoch: 193, Train loss: 0.5130161309115548, Val loss: 0.4360370471289283, Val g-mean: 0.7583117122955108\nEpoch: 194, Train loss: 0.5122422622062461, Val loss: 0.4518632004527669, Val g-mean: 0.7527812537431123\nEpoch: 195, Train loss: 0.5164804930394619, Val loss: 0.4409890194472514, Val g-mean: 0.7541232655413276\nEpoch: 196, Train loss: 0.5124872527491254, Val loss: 0.439109206199646, Val g-mean: 0.7570232644694472\nEpoch: 197, Train loss: 0.5201122766052715, Val loss: 0.4316890724003315, Val g-mean: 0.7617072510914538\nEpoch: 198, Train loss: 0.5130683724956424, Val loss: 0.4343989458131163, Val g-mean: 0.7629994569353699\nEpoch: 199, Train loss: 0.5165976739370945, Val loss: 0.4526614435017109, Val g-mean: 0.7507611440742215\n"
        }
      ],
      "source": [
        "train_loss = 0\n",
        "epochs = 200\n",
        "for epoch in range(epochs):\n",
        "  classifier.train()\n",
        "  for inputs, targets in train_dataloader:\n",
        "    if torch.cuda.is_available():\n",
        "      inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "    outputs = classifier(inputs.float())\n",
        "    loss = criterion(outputs.squeeze(), targets.float())\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  train_loss = train_loss / len(train_dataloader)\n",
        "  val_loss = metrics.loss(classifier, val_dataloader, criterion)\n",
        "  val_gmean, _ = metrics.gmean(classifier, val_dataloader)\n",
        "  print('Epoch: {}, Train loss: {}, Val loss: {}, Val g-mean: {}'.format(epoch, train_loss, val_loss, val_gmean))\n",
        "\n",
        "  if classifier.val_gmean is None or val_gmean > classifier.val_gmean:\n",
        "    classifier.epoch = epoch\n",
        "    classifier.val_gmean = val_gmean\n",
        "    classifier.save()\n",
        "\n",
        "classifier.epoch = epoch\n",
        "classifier.val_gmean = val_gmean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(classifier):\n",
        "  if torch.cuda.is_available():\n",
        "    classifier = classifier.cuda()\n",
        "  train_gmean, _ = metrics.gmean(classifier, train_dataloader)\n",
        "  test_gmean, test_recalls = metrics.gmean(classifier, test_dataloader)\n",
        "  print('Epoch: {}'.format(classifier.epoch))\n",
        "  print('Train g-mean: {}'.format(train_gmean.item()))\n",
        "  print('Val g-mean: {}'.format(classifier.val_gmean))\n",
        "  print('Test g-mean: {}, recalls: {}'.format(test_gmean.item(), test_recalls))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Best classifier\nEpoch: 176\nTrain g-mean: 0.779200493299207\nVal g-mean: 0.7671735531003301\nTest g-mean: 0.7363740914430342, recalls: [0.75900849 0.71441468]\n"
        }
      ],
      "source": [
        "print('Best classifier')\n",
        "evaluate(Classifier.load())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Last classifier\nEpoch: 199\nTrain g-mean: 0.7662040990564849\nVal g-mean: 0.7507611440742215\nTest g-mean: 0.7402974366118688, recalls: [0.7769758  0.70535053]\n"
        }
      ],
      "source": [
        "print('Last classifier')\n",
        "evaluate(classifier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "L_t4Gh43GYWl"
      },
      "outputs": [],
      "source": []
    }
  ]
}