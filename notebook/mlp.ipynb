{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JIT-SDP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXwvNEAQE6NO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAupgTDfmJ_I",
        "colab_type": "code",
        "outputId": "35a3697a-462f-4a17-bcde-80a36ebd1d10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df = pd.read_csv('/content/jenkins.csv')\n",
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fix</th>\n",
              "      <th>ns</th>\n",
              "      <th>nd</th>\n",
              "      <th>nf</th>\n",
              "      <th>entropy</th>\n",
              "      <th>la</th>\n",
              "      <th>ld</th>\n",
              "      <th>lt</th>\n",
              "      <th>ndev</th>\n",
              "      <th>age</th>\n",
              "      <th>nuc</th>\n",
              "      <th>exp</th>\n",
              "      <th>rexp</th>\n",
              "      <th>sexp</th>\n",
              "      <th>author_date_unix_timestamp</th>\n",
              "      <th>classification</th>\n",
              "      <th>contains_bug</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>False</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.641604</td>\n",
              "      <td>9.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>426.428571</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.000093</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5171.0</td>\n",
              "      <td>30.227271</td>\n",
              "      <td>1472.714286</td>\n",
              "      <td>1555326371</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>False</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.750000</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>426.428571</td>\n",
              "      <td>100.0</td>\n",
              "      <td>6.314775</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5170.0</td>\n",
              "      <td>29.227271</td>\n",
              "      <td>1471.714286</td>\n",
              "      <td>1555326363</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>False</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.906580</td>\n",
              "      <td>15.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>96.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.034722</td>\n",
              "      <td>2.0</td>\n",
              "      <td>629.0</td>\n",
              "      <td>14.828373</td>\n",
              "      <td>414.000000</td>\n",
              "      <td>1554971763</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>False</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.058824</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1554969774</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>False</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.662506</td>\n",
              "      <td>14.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>67.000000</td>\n",
              "      <td>6.0</td>\n",
              "      <td>21.280683</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.058824</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1554967752</td>\n",
              "      <td>Feature Addition</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     fix   ns   nd  ...  author_date_unix_timestamp    classification  contains_bug\n",
              "0  False  7.0  7.0  ...                  1555326371              None         False\n",
              "1  False  7.0  7.0  ...                  1555326363              None         False\n",
              "2  False  1.0  1.0  ...                  1554971763              None         False\n",
              "3  False  1.0  1.0  ...                  1554969774              None         False\n",
              "4  False  1.0  2.0  ...                  1554967752  Feature Addition         False\n",
              "\n",
              "[5 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alizaxdPD3El",
        "colab_type": "code",
        "outputId": "c4e7da1c-1571-472b-c0ef-7154cd7ff7d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "df = df.sample(frac=1)\n",
        "label_col = 'contains_bug'\n",
        "features_cols = ['fix', 'ns', 'nd', 'nf', 'entropy', 'la', 'ld', 'lt', 'ndev', 'age', 'nuc', 'exp', 'rexp', 'sexp', 'classification']\n",
        "x = df[features_cols]\n",
        "x['fix'] = x['fix'].astype('int')\n",
        "df_classification = pd.get_dummies(x, columns=['classification'])\n",
        "x = pd.concat([x, df_classification], axis='columns')\n",
        "x = x.drop(['classification'], axis='columns')\n",
        "x = x.values\n",
        "y = df[label_col]\n",
        "y = y.astype('category')\n",
        "y = y.cat.codes\n",
        "y = y.values"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBD1-tp2ExzQ",
        "colab_type": "code",
        "outputId": "202f1023-493a-4483-fd98-45875a6781d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "print(x[:3])\n",
        "print(y[:3])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 1.00000000e+00 9.30000000e+02\n",
            "  5.00000000e+00 6.95868056e-01 1.00000000e+00 3.78000000e+02\n",
            "  4.38635226e+01 4.30000000e+01 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 9.30000000e+02 5.00000000e+00 6.95868056e-01\n",
            "  1.00000000e+00 3.78000000e+02 4.38635226e+01 4.30000000e+01\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 9.00000000e+00 9.32000000e+02\n",
            "  3.00000000e+01 9.60648148e-03 1.00000000e+00 3.10500000e+03\n",
            "  9.66193307e+01 1.85400000e+03 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  9.00000000e+00 9.32000000e+02 3.00000000e+01 9.60648148e-03\n",
            "  1.00000000e+00 3.10500000e+03 9.66193307e+01 1.85400000e+03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 1.60000000e+01 8.00000000e+00 5.30000000e+01\n",
            "  4.00000000e+00 1.94147500e+02 1.00000000e+00 7.31000000e+02\n",
            "  5.64969495e+01 1.61000000e+02 1.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 1.60000000e+01\n",
            "  8.00000000e+00 5.30000000e+01 4.00000000e+00 1.94147500e+02\n",
            "  1.00000000e+00 7.31000000e+02 5.64969495e+01 1.61000000e+02\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "[0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwCn1uNTMjfG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_index = int( len(x) * 0.8 )\n",
        "test_index = int( len(x) * 0.9 )\n",
        "x_train, y_train = x[:val_index], y[:val_index]\n",
        "x_val, y_val = x[val_index:test_index], y[val_index:test_index]\n",
        "x_test, y_test = x[test_index:], y[test_index:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSqnoapE5Poq",
        "colab_type": "code",
        "outputId": "7a846dbf-3137-4725-84af-f3d731207abc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(x_train[:3])\n",
        "print(x_val[:3])\n",
        "print(x_test[:3])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 1.00000000e+00 9.30000000e+02\n",
            "  5.00000000e+00 6.95868056e-01 1.00000000e+00 3.78000000e+02\n",
            "  4.38635226e+01 4.30000000e+01 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 9.30000000e+02 5.00000000e+00 6.95868056e-01\n",
            "  1.00000000e+00 3.78000000e+02 4.38635226e+01 4.30000000e+01\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 9.00000000e+00 9.32000000e+02\n",
            "  3.00000000e+01 9.60648148e-03 1.00000000e+00 3.10500000e+03\n",
            "  9.66193307e+01 1.85400000e+03 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  9.00000000e+00 9.32000000e+02 3.00000000e+01 9.60648148e-03\n",
            "  1.00000000e+00 3.10500000e+03 9.66193307e+01 1.85400000e+03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 1.60000000e+01 8.00000000e+00 5.30000000e+01\n",
            "  4.00000000e+00 1.94147500e+02 1.00000000e+00 7.31000000e+02\n",
            "  5.64969495e+01 1.61000000e+02 1.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 1.60000000e+01\n",
            "  8.00000000e+00 5.30000000e+01 4.00000000e+00 1.94147500e+02\n",
            "  1.00000000e+00 7.31000000e+02 5.64969495e+01 1.61000000e+02\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "[[0.00000000e+00 1.00000000e+00 2.00000000e+00 2.00000000e+00\n",
            "  9.96995427e-01 6.20000000e+01 0.00000000e+00 1.42150000e+03\n",
            "  5.00000000e+00 3.80167824e-01 2.00000000e+00 7.70000000e+01\n",
            "  1.93945098e+01 7.00000000e+01 0.00000000e+00 1.00000000e+00\n",
            "  2.00000000e+00 2.00000000e+00 9.96995427e-01 6.20000000e+01\n",
            "  0.00000000e+00 1.42150000e+03 5.00000000e+00 3.80167824e-01\n",
            "  2.00000000e+00 7.70000000e+01 1.93945098e+01 7.00000000e+01\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 1.40000000e+01 4.00000000e+00 2.94000000e+02\n",
            "  2.00000000e+00 2.96235995e+01 1.00000000e+00 1.23600000e+03\n",
            "  2.40307544e+02 1.04600000e+03 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 1.40000000e+01\n",
            "  4.00000000e+00 2.94000000e+02 2.00000000e+00 2.96235995e+01\n",
            "  1.00000000e+00 1.23600000e+03 2.40307544e+02 1.04600000e+03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 9.00000000e+00 1.00000000e+00 2.49500000e+03\n",
            "  9.40000000e+01 4.95599537e+00 1.00000000e+00 2.02000000e+02\n",
            "  1.25039002e+01 8.10000000e+01 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 9.00000000e+00\n",
            "  1.00000000e+00 2.49500000e+03 9.40000000e+01 4.95599537e+00\n",
            "  1.00000000e+00 2.02000000e+02 1.25039002e+01 8.10000000e+01\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "[[0.00000000e+00 1.00000000e+00 5.00000000e+00 5.00000000e+00\n",
            "  1.37544467e+00 1.60000000e+01 1.30000000e+01 3.84000000e+02\n",
            "  3.00000000e+01 5.88701250e+01 4.00000000e+00 1.62400000e+03\n",
            "  1.03779265e+02 9.46000000e+02 0.00000000e+00 1.00000000e+00\n",
            "  5.00000000e+00 5.00000000e+00 1.37544467e+00 1.60000000e+01\n",
            "  1.30000000e+01 3.84000000e+02 3.00000000e+01 5.88701250e+01\n",
            "  4.00000000e+00 1.62400000e+03 1.03779265e+02 9.46000000e+02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 1.30000000e+01\n",
            "  2.00000000e+00 3.17129630e-03 1.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 1.30000000e+01 2.00000000e+00 3.17129630e-03\n",
            "  1.00000000e+00 1.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46STsbti3AmT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean_train = x_train.mean(axis=0)\n",
        "std_train = x_train.std(axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5vvbrrK3PES",
        "colab_type": "code",
        "outputId": "40c9972e-36bb-4bfa-e1fb-80641d4ce259",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "print(mean_train)\n",
        "print(std_train)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2.77527799e-01 1.58919059e+00 2.51073183e+00 3.97667442e+00\n",
            " 7.31179694e-01 5.32847168e+01 2.48240497e+01 7.08735227e+02\n",
            " 1.90427205e+01 8.18745511e+01 1.91936902e+00 1.74538242e+03\n",
            " 8.58253478e+01 9.03776002e+02 2.77527799e-01 1.58919059e+00\n",
            " 2.51073183e+00 3.97667442e+00 7.31179694e-01 5.32847168e+01\n",
            " 2.48240497e+01 7.08735227e+02 1.90427205e+01 8.18745511e+01\n",
            " 1.91936902e+00 1.74538242e+03 8.58253478e+01 9.03776002e+02\n",
            " 2.77527799e-01 1.55521076e-01 3.68761314e-02 4.79751746e-01\n",
            " 1.53090251e-02 3.50142229e-02]\n",
            "[4.47779097e-01 1.60848444e+00 6.43543463e+00 2.48125619e+01\n",
            " 1.11612212e+00 6.63681017e+02 5.16477345e+02 1.06804000e+03\n",
            " 2.69330566e+01 2.14149149e+02 7.36228375e+00 1.98154753e+03\n",
            " 8.22352742e+01 1.31067084e+03 4.47779097e-01 1.60848444e+00\n",
            " 6.43543463e+00 2.48125619e+01 1.11612212e+00 6.63681017e+02\n",
            " 5.16477345e+02 1.06804000e+03 2.69330566e+01 2.14149149e+02\n",
            " 7.36228375e+00 1.98154753e+03 8.22352742e+01 1.31067084e+03\n",
            " 4.47779097e-01 3.62400705e-01 1.88457641e-01 4.99589840e-01\n",
            " 1.22778902e-01 1.83815742e-01]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2IoFA5h425Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = (x_train - mean_train) / std_train\n",
        "x_val = (x_val - mean_train) / std_train\n",
        "x_test = (x_test - mean_train) / std_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1AoKNoU5V1w",
        "colab_type": "code",
        "outputId": "1d9f9a3c-cfcc-4b84-c8de-daf8776f2b12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        }
      },
      "source": [
        "print(x_train[:3])\n",
        "print(x_val[:3])\n",
        "print(x_test[:3])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.6197873  -0.3663017  -0.23475211 -0.11996643 -0.65510725 -0.07877989\n",
            "  -0.04612797  0.207169   -0.52139349 -0.37907544 -0.12487552 -0.69005784\n",
            "  -0.51026552 -0.65674461 -0.6197873  -0.3663017  -0.23475211 -0.11996643\n",
            "  -0.65510725 -0.07877989 -0.04612797  0.207169   -0.52139349 -0.37907544\n",
            "  -0.12487552 -0.69005784 -0.51026552 -0.65674461 -0.6197873  -0.42914121\n",
            "  -0.19567332  1.04135075 -0.12468775 -0.19048544]\n",
            " [-0.6197873  -0.3663017  -0.23475211 -0.11996643 -0.65510725 -0.07877989\n",
            "  -0.03063842  0.20904158  0.40683387 -0.38228004 -0.12487552  0.68613927\n",
            "   0.13125734  0.72499057 -0.6197873  -0.3663017  -0.23475211 -0.11996643\n",
            "  -0.65510725 -0.07877989 -0.03063842  0.20904158  0.40683387 -0.38228004\n",
            "  -0.12487552  0.68613927  0.13125734  0.72499057 -0.6197873  -0.42914121\n",
            "  -0.19567332  1.04135075 -0.12468775 -0.19048544]\n",
            " [ 1.61345674 -0.3663017  -0.23475211 -0.11996643 -0.65510725 -0.05617867\n",
            "  -0.03257461 -0.6139613  -0.55852259  0.52427455 -0.12487552 -0.51191425\n",
            "  -0.35664012 -0.56671437  1.61345674 -0.3663017  -0.23475211 -0.11996643\n",
            "  -0.65510725 -0.05617867 -0.03257461 -0.6139613  -0.55852259  0.52427455\n",
            "  -0.12487552 -0.51191425 -0.35664012 -0.56671437  1.61345674 -0.42914121\n",
            "  -0.19567332 -0.96029124 -0.12468775 -0.19048544]]\n",
            "[[-0.6197873  -0.3663017  -0.07936245 -0.07966426  0.23816008  0.01313173\n",
            "  -0.04806416  0.66735775 -0.52139349 -0.38054965  0.0109519  -0.84195932\n",
            "  -0.80781439 -0.63614447 -0.6197873  -0.3663017  -0.07936245 -0.07966426\n",
            "   0.23816008  0.01313173 -0.04806416  0.66735775 -0.52139349 -0.38054965\n",
            "   0.0109519  -0.84195932 -0.80781439 -0.63614447 -0.6197873  -0.42914121\n",
            "  -0.19567332  1.04135075 -0.12468775 -0.19048544]\n",
            " [-0.6197873  -0.3663017  -0.23475211 -0.11996643 -0.65510725 -0.05919217\n",
            "  -0.04031939 -0.38831432 -0.63278078 -0.24399327 -0.12487552 -0.25706293\n",
            "   1.87853932  0.10851237 -0.6197873  -0.3663017  -0.23475211 -0.11996643\n",
            "  -0.65510725 -0.05919217 -0.04031939 -0.38831432 -0.63278078 -0.24399327\n",
            "  -0.12487552 -0.25706293  1.87853932  0.10851237 -0.6197873  -0.42914121\n",
            "  -0.19567332  1.04135075 -0.12468775 -0.19048544]\n",
            " [-0.6197873  -0.3663017  -0.23475211 -0.11996643 -0.65510725 -0.06672591\n",
            "  -0.04612797  1.67246993  2.78309591 -0.35918217 -0.12487552 -0.77887731\n",
            "  -0.8916058  -0.62775182 -0.6197873  -0.3663017  -0.23475211 -0.11996643\n",
            "  -0.65510725 -0.06672591 -0.04612797  1.67246993  2.78309591 -0.35918217\n",
            "  -0.12487552 -0.77887731 -0.8916058  -0.62775182 -0.6197873  -0.42914121\n",
            "  -0.19567332  1.04135075 -0.12468775 -0.19048544]]\n",
            "[[-0.6197873  -0.3663017   0.38680653  0.04124224  0.5772352  -0.05617867\n",
            "  -0.02289365 -0.30404782  0.40683387 -0.10742245  0.28260674 -0.06125637\n",
            "   0.2183238   0.03221556 -0.6197873  -0.3663017   0.38680653  0.04124224\n",
            "   0.5772352  -0.05617867 -0.02289365 -0.30404782  0.40683387 -0.10742245\n",
            "   0.28260674 -0.06125637  0.2183238   0.03221556 -0.6197873  -0.42914121\n",
            "  -0.19567332  1.04135075 -0.12468775 -0.19048544]\n",
            " [-0.6197873  -0.3663017  -0.23475211 -0.11996643 -0.65510725 -0.08028664\n",
            "  -0.04612797 -0.65141308 -0.63278078 -0.38231009 -0.12487552 -0.88031318\n",
            "  -1.03149589 -0.68955223 -0.6197873  -0.3663017  -0.23475211 -0.11996643\n",
            "  -0.65510725 -0.08028664 -0.04612797 -0.65141308 -0.63278078 -0.38231009\n",
            "  -0.12487552 -0.88031318 -1.03149589 -0.68955223 -0.6197873  -0.42914121\n",
            "  -0.19567332  1.04135075 -0.12468775 -0.19048544]\n",
            " [-0.6197873  -0.98800495 -0.39014177 -0.16026859 -0.65510725 -0.08028664\n",
            "  -0.04806416 -0.66358491 -0.70703896 -0.3823249  -0.26070294 -0.88081784\n",
            "  -1.04365613 -0.68955223 -0.6197873  -0.98800495 -0.39014177 -0.16026859\n",
            "  -0.65510725 -0.08028664 -0.04806416 -0.66358491 -0.70703896 -0.3823249\n",
            "  -0.26070294 -0.88081784 -1.04365613 -0.68955223 -0.6197873   2.33023533\n",
            "  -0.19567332 -0.96029124 -0.12468775 -0.19048544]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iP6L6GN22_bR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, y_train = torch.from_numpy(x_train), torch.from_numpy(y_train)\n",
        "x_val, y_val = torch.from_numpy(x_val), torch.from_numpy(y_val)\n",
        "x_test, y_test = torch.from_numpy(x_test), torch.from_numpy(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XnnrzRjL3IP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = data.TensorDataset(x_train, y_train)\n",
        "val_dataset = data.TensorDataset(x_val, y_val)\n",
        "test_dataset = data.TensorDataset(x_test, y_test)\n",
        "\n",
        "train_dataloader = data.DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
        "val_dataloader = data.DataLoader(val_dataset, batch_size=32)\n",
        "test_dataloader = data.DataLoader(test_dataset, batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHoY9-JcORo-",
        "colab_type": "code",
        "outputId": "4764fdc3-20df-4732-bc5f-71bf1a857996",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        }
      },
      "source": [
        "inputs, targets = next(iter(train_dataloader))\n",
        "print(inputs[:3])\n",
        "print(targets[:3])\n",
        "\n",
        "inputs, targets = next(iter(val_dataloader))\n",
        "print(inputs[:3])\n",
        "print(targets[:3])\n",
        "\n",
        "inputs, targets = next(iter(test_dataloader))\n",
        "print(inputs[:3])\n",
        "print(targets[:3])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 1.6135e+00,  8.7710e-01,  7.6027e-02, -3.9362e-02,  1.8595e-03,\n",
            "          1.3132e-02, -4.6128e-02,  3.2577e-01, -1.1297e-01,  1.1914e-01,\n",
            "          1.4678e-01, -8.7880e-01, -1.0112e+00, -6.8879e-01,  1.6135e+00,\n",
            "          8.7710e-01,  7.6027e-02, -3.9362e-02,  1.8595e-03,  1.3132e-02,\n",
            "         -4.6128e-02,  3.2577e-01, -1.1297e-01,  1.1914e-01,  1.4678e-01,\n",
            "         -8.7880e-01, -1.0112e+00, -6.8879e-01,  1.6135e+00, -4.2914e-01,\n",
            "         -1.9567e-01, -9.6029e-01, -1.2469e-01, -1.9049e-01],\n",
            "        [-6.1979e-01, -3.6630e-01, -7.9362e-02, -7.9664e-02,  2.4032e-01,\n",
            "         -3.0564e-02,  9.1342e-02, -3.7427e-01, -3.7288e-01, -3.5514e-01,\n",
            "         -1.2488e-01, -8.7779e-01, -1.0285e+00, -6.8879e-01, -6.1979e-01,\n",
            "         -3.6630e-01, -7.9362e-02, -7.9664e-02,  2.4032e-01, -3.0564e-02,\n",
            "          9.1342e-02, -3.7427e-01, -3.7288e-01, -3.5514e-01, -1.2488e-01,\n",
            "         -8.7779e-01, -1.0285e+00, -6.8879e-01, -6.1979e-01,  2.3302e+00,\n",
            "         -1.9567e-01, -9.6029e-01, -1.2469e-01, -1.9049e-01],\n",
            "        [-6.1979e-01, -3.6630e-01, -2.3475e-01, -1.1997e-01, -6.5511e-01,\n",
            "         -2.3030e-02, -1.9021e-02, -5.8962e-01, -6.6991e-01,  6.3346e-01,\n",
            "         -1.2488e-01,  4.0404e-01,  1.3883e+00, -5.4535e-01, -6.1979e-01,\n",
            "         -3.6630e-01, -2.3475e-01, -1.1997e-01, -6.5511e-01, -2.3030e-02,\n",
            "         -1.9021e-02, -5.8962e-01, -6.6991e-01,  6.3346e-01, -1.2488e-01,\n",
            "          4.0404e-01,  1.3883e+00, -5.4535e-01, -6.1979e-01, -4.2914e-01,\n",
            "         -1.9567e-01,  1.0414e+00, -1.2469e-01, -1.9049e-01]],\n",
            "       dtype=torch.float64)\n",
            "tensor([0, 0, 0], dtype=torch.int8)\n",
            "tensor([[-0.6198, -0.3663, -0.0794, -0.0797,  0.2382,  0.0131, -0.0481,  0.6674,\n",
            "         -0.5214, -0.3805,  0.0110, -0.8420, -0.8078, -0.6361, -0.6198, -0.3663,\n",
            "         -0.0794, -0.0797,  0.2382,  0.0131, -0.0481,  0.6674, -0.5214, -0.3805,\n",
            "          0.0110, -0.8420, -0.8078, -0.6361, -0.6198, -0.4291, -0.1957,  1.0414,\n",
            "         -0.1247, -0.1905],\n",
            "        [-0.6198, -0.3663, -0.2348, -0.1200, -0.6551, -0.0592, -0.0403, -0.3883,\n",
            "         -0.6328, -0.2440, -0.1249, -0.2571,  1.8785,  0.1085, -0.6198, -0.3663,\n",
            "         -0.2348, -0.1200, -0.6551, -0.0592, -0.0403, -0.3883, -0.6328, -0.2440,\n",
            "         -0.1249, -0.2571,  1.8785,  0.1085, -0.6198, -0.4291, -0.1957,  1.0414,\n",
            "         -0.1247, -0.1905],\n",
            "        [-0.6198, -0.3663, -0.2348, -0.1200, -0.6551, -0.0667, -0.0461,  1.6725,\n",
            "          2.7831, -0.3592, -0.1249, -0.7789, -0.8916, -0.6278, -0.6198, -0.3663,\n",
            "         -0.2348, -0.1200, -0.6551, -0.0667, -0.0461,  1.6725,  2.7831, -0.3592,\n",
            "         -0.1249, -0.7789, -0.8916, -0.6278, -0.6198, -0.4291, -0.1957,  1.0414,\n",
            "         -0.1247, -0.1905]], dtype=torch.float64)\n",
            "tensor([1, 1, 0], dtype=torch.int8)\n",
            "tensor([[-0.6198, -0.3663,  0.3868,  0.0412,  0.5772, -0.0562, -0.0229, -0.3040,\n",
            "          0.4068, -0.1074,  0.2826, -0.0613,  0.2183,  0.0322, -0.6198, -0.3663,\n",
            "          0.3868,  0.0412,  0.5772, -0.0562, -0.0229, -0.3040,  0.4068, -0.1074,\n",
            "          0.2826, -0.0613,  0.2183,  0.0322, -0.6198, -0.4291, -0.1957,  1.0414,\n",
            "         -0.1247, -0.1905],\n",
            "        [-0.6198, -0.3663, -0.2348, -0.1200, -0.6551, -0.0803, -0.0461, -0.6514,\n",
            "         -0.6328, -0.3823, -0.1249, -0.8803, -1.0315, -0.6896, -0.6198, -0.3663,\n",
            "         -0.2348, -0.1200, -0.6551, -0.0803, -0.0461, -0.6514, -0.6328, -0.3823,\n",
            "         -0.1249, -0.8803, -1.0315, -0.6896, -0.6198, -0.4291, -0.1957,  1.0414,\n",
            "         -0.1247, -0.1905],\n",
            "        [-0.6198, -0.9880, -0.3901, -0.1603, -0.6551, -0.0803, -0.0481, -0.6636,\n",
            "         -0.7070, -0.3823, -0.2607, -0.8808, -1.0437, -0.6896, -0.6198, -0.9880,\n",
            "         -0.3901, -0.1603, -0.6551, -0.0803, -0.0481, -0.6636, -0.7070, -0.3823,\n",
            "         -0.2607, -0.8808, -1.0437, -0.6896, -0.6198,  2.3302, -0.1957, -0.9603,\n",
            "         -0.1247, -0.1905]], dtype=torch.float64)\n",
            "tensor([0, 0, 0], dtype=torch.int8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-GctbL8E1oA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Classifier(nn.Module):\n",
        "  def __init__(self, epochs, input_size, hidden_size, val_loss=None):\n",
        "    super(Classifier, self).__init__()\n",
        "    self.epochs = epochs\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.val_loss = val_loss\n",
        "    self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "    self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "    self.fcout = nn.Linear(hidden_size, 1)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = torch.sigmoid(self.fc1(x))\n",
        "    x = torch.sigmoid(self.fc2(x))\n",
        "    x = torch.sigmoid(self.fcout(x))\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inpSePkGLTIs",
        "colab_type": "code",
        "outputId": "6ea9e0c8-820d-4f96-ad22-3f09c12dfcca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "classifier = Classifier(epochs=100, input_size=x.shape[1], hidden_size=256)\n",
        "classifier"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Classifier(\n",
              "  (fc1): Linear(in_features=34, out_features=256, bias=True)\n",
              "  (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
              "  (fcout): Linear(in_features=256, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsT5eIrNoXAh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = 'classifier.cpt'\n",
        "def save_classifier(classifier):\n",
        "  checkpoint = {\n",
        "      'epochs': classifier.epochs,\n",
        "      'input_size': classifier.input_size,\n",
        "      'hidden_size': classifier.hidden_size,\n",
        "      'val_loss': classifier.val_loss,\n",
        "      'state_dict': classifier.state_dict()\n",
        "  }\n",
        "  with open(filename, 'wb') as f:\n",
        "    torch.save(checkpoint, f)\n",
        "\n",
        "def load_classifier():\n",
        "  with open(filename, 'rb') as f:\n",
        "    checkpoint = torch.load(f)\n",
        "  classifier = Classifier(checkpoint['epochs'], checkpoint['input_size'], checkpoint['hidden_size'], checkpoint['val_loss'])\n",
        "  classifier.load_state_dict(checkpoint['state_dict'])\n",
        "  return classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LX_eofKUq0ok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_classifier(classifier)\n",
        "classifier = load_classifier()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PR8s5ZA5rX-V",
        "colab_type": "code",
        "outputId": "754b3cd2-32de-46bf-a2f5-d867e5de5172",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  classifier = classifier.cuda()\n",
        "  inputs = inputs.cuda()\n",
        "\n",
        "classifier(inputs[:3].float())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4838],\n",
              "        [0.4883],\n",
              "        [0.4868]], device='cuda:0', grad_fn=<SigmoidBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arYOr5lMLYy7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(params=classifier.parameters(), lr=0.03)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFy03p96Lza1",
        "colab_type": "code",
        "outputId": "5b1892ef-92fa-4a5a-e153-ddbdaeda8983",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_loss = 0\n",
        "epochs = classifier.epochs\n",
        "for epoch in range(epochs):\n",
        "  classifier.train()\n",
        "  for inputs, targets in train_dataloader:\n",
        "    if torch.cuda.is_available():\n",
        "      inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "    outputs = classifier(inputs.float())\n",
        "    loss = criterion(outputs.squeeze(), targets.float())\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    classifier.eval()\n",
        "    val_loss = 0\n",
        "    for inputs, targets in val_dataloader:\n",
        "      if torch.cuda.is_available():\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "      outputs = classifier(inputs.float())\n",
        "      loss = criterion(outputs.squeeze(), targets.float())\n",
        "      val_loss += loss.item()\n",
        "\n",
        "    train_loss = train_loss / len(train_dataloader)\n",
        "    val_loss = val_loss / len(val_dataloader)\n",
        "    print('Epoch: {}, Train loss: {}, Val loss: {}'.format(epoch, train_loss, val_loss))\n",
        "\n",
        "    if classifier.val_loss is None or val_loss < classifier.val_loss:\n",
        "      classifier.val_loss = val_loss\n",
        "      classifier.epochs = epoch\n",
        "      save_classifier(classifier)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Train loss: 0.6348657419807032, Val loss: 0.5236511626525929\n",
            "Epoch: 1, Train loss: 0.5192720610596797, Val loss: 0.49649959214423833\n",
            "Epoch: 2, Train loss: 0.48539842748510753, Val loss: 0.48249259355821106\n",
            "Epoch: 3, Train loss: 0.4587922500371588, Val loss: 0.4401989179222207\n",
            "Epoch: 4, Train loss: 0.428354355344646, Val loss: 0.42181926103014694\n",
            "Epoch: 5, Train loss: 0.41587551101910103, Val loss: 0.41979212686419487\n",
            "Epoch: 6, Train loss: 0.41315477737859335, Val loss: 0.411880879613914\n",
            "Epoch: 7, Train loss: 0.4045073530258514, Val loss: 0.40399081103111567\n",
            "Epoch: 8, Train loss: 0.40109423388823545, Val loss: 0.3995685163689287\n",
            "Epoch: 9, Train loss: 0.4016550961266967, Val loss: 0.4062763591738124\n",
            "Epoch: 10, Train loss: 0.3971655750042801, Val loss: 0.40408140067991455\n",
            "Epoch: 11, Train loss: 0.39365218209531905, Val loss: 0.3954143302613183\n",
            "Epoch: 12, Train loss: 0.3893046303609241, Val loss: 0.4061592317333347\n",
            "Epoch: 13, Train loss: 0.39273722136234823, Val loss: 0.39182029429234955\n",
            "Epoch: 14, Train loss: 0.3867921973436066, Val loss: 0.4056417953811194\n",
            "Epoch: 15, Train loss: 0.3870091551886551, Val loss: 0.39792683936263384\n",
            "Epoch: 16, Train loss: 0.3795383003049424, Val loss: 0.4015104178535311\n",
            "Epoch: 17, Train loss: 0.3778263425270019, Val loss: 0.4022459534830169\n",
            "Epoch: 18, Train loss: 0.3794148613889994, Val loss: 0.39346833527088165\n",
            "Epoch: 19, Train loss: 0.3826406066090391, Val loss: 0.3945299665394582\n",
            "Epoch: 20, Train loss: 0.37601492923725727, Val loss: 0.3872855005687789\n",
            "Epoch: 21, Train loss: 0.3774498970681714, Val loss: 0.41041795850584384\n",
            "Epoch: 22, Train loss: 0.3684469052871442, Val loss: 0.3911079227140075\n",
            "Epoch: 23, Train loss: 0.3673326566259076, Val loss: 0.38957513515886505\n",
            "Epoch: 24, Train loss: 0.36245332865976454, Val loss: 0.38912170772489746\n",
            "Epoch: 25, Train loss: 0.3601127192924585, Val loss: 0.3888082514075856\n",
            "Epoch: 26, Train loss: 0.36478855480632805, Val loss: 0.38810709569799273\n",
            "Epoch: 27, Train loss: 0.36472615266836567, Val loss: 0.39281223381036207\n",
            "Epoch: 28, Train loss: 0.3625341401981149, Val loss: 0.3936082128631441\n",
            "Epoch: 29, Train loss: 0.3568379950797527, Val loss: 0.3951202162394398\n",
            "Epoch: 30, Train loss: 0.35368221860255056, Val loss: 0.3955937847495079\n",
            "Epoch: 31, Train loss: 0.3534273944039944, Val loss: 0.40446702449729566\n",
            "Epoch: 32, Train loss: 0.3518417583268457, Val loss: 0.38513132635700076\n",
            "Epoch: 33, Train loss: 0.35528608008006574, Val loss: 0.3939883234469514\n",
            "Epoch: 34, Train loss: 0.34656549724669294, Val loss: 0.4058664072501032\n",
            "Epoch: 35, Train loss: 0.3454631491796832, Val loss: 0.39747139086064537\n",
            "Epoch: 36, Train loss: 0.3402475540164705, Val loss: 0.40399955801273646\n",
            "Epoch: 37, Train loss: 0.3369829383448645, Val loss: 0.41054558028516014\n",
            "Epoch: 38, Train loss: 0.34386699379263186, Val loss: 0.41903671210533694\n",
            "Epoch: 39, Train loss: 0.34139521628638575, Val loss: 0.4012746007034653\n",
            "Epoch: 40, Train loss: 0.33628060517343006, Val loss: 0.3941825573381625\n",
            "Epoch: 41, Train loss: 0.3384472552876807, Val loss: 0.3976363463229255\n",
            "Epoch: 42, Train loss: 0.3419976763138153, Val loss: 0.40084066006698105\n",
            "Epoch: 43, Train loss: 0.3403740452888999, Val loss: 0.40785183326194163\n",
            "Epoch: 44, Train loss: 0.33332238523364954, Val loss: 0.39944724601350334\n",
            "Epoch: 45, Train loss: 0.32616080760124505, Val loss: 0.4012935628232203\n",
            "Epoch: 46, Train loss: 0.3270831136638562, Val loss: 0.4201778318536909\n",
            "Epoch: 47, Train loss: 0.328653975612068, Val loss: 0.40508628205249186\n",
            "Epoch: 48, Train loss: 0.3278043509127316, Val loss: 0.39768774062395096\n",
            "Epoch: 49, Train loss: 0.32084670333643134, Val loss: 0.41020045037332337\n",
            "Epoch: 50, Train loss: 0.3226334287065632, Val loss: 0.42509852055656283\n",
            "Epoch: 51, Train loss: 0.3222649940099067, Val loss: 0.40279731409330116\n",
            "Epoch: 52, Train loss: 0.323560100893822, Val loss: 0.3961980407567401\n",
            "Epoch: 53, Train loss: 0.31512675034635546, Val loss: 0.41711104171056496\n",
            "Epoch: 54, Train loss: 0.3142669421402073, Val loss: 0.410380666585345\n",
            "Epoch: 55, Train loss: 0.31238655960865047, Val loss: 0.41174564843899325\n",
            "Epoch: 56, Train loss: 0.30823380741014483, Val loss: 0.41878373469961316\n",
            "Epoch: 57, Train loss: 0.31268073252223416, Val loss: 0.42371463089397077\n",
            "Epoch: 58, Train loss: 0.30747275633022, Val loss: 0.4171902874582692\n",
            "Epoch: 59, Train loss: 0.3048912888248774, Val loss: 0.4278751652883856\n",
            "Epoch: 60, Train loss: 0.30511477277331184, Val loss: 0.43065671465898814\n",
            "Epoch: 61, Train loss: 0.3064092969386339, Val loss: 0.428121485600346\n",
            "Epoch: 62, Train loss: 0.30450328158571155, Val loss: 0.4307710040164621\n",
            "Epoch: 63, Train loss: 0.30140013730505943, Val loss: 0.4201394550894436\n",
            "Epoch: 64, Train loss: 0.30247879962544166, Val loss: 0.42829227408296183\n",
            "Epoch: 65, Train loss: 0.3096813134562605, Val loss: 0.4417967559083512\n",
            "Epoch: 66, Train loss: 0.29747196175082635, Val loss: 0.4308056047088222\n",
            "Epoch: 67, Train loss: 0.29129673596469563, Val loss: 0.4442177352152373\n",
            "Epoch: 68, Train loss: 0.2924529698481288, Val loss: 0.4435560634653819\n",
            "Epoch: 69, Train loss: 0.2863094303090063, Val loss: 0.4409643058714114\n",
            "Epoch: 70, Train loss: 0.2845741932409863, Val loss: 0.4438087912766557\n",
            "Epoch: 71, Train loss: 0.29216557813009114, Val loss: 0.4392801203617924\n",
            "Epoch: 72, Train loss: 0.28740992869523546, Val loss: 0.4376525210314675\n",
            "Epoch: 73, Train loss: 0.2800937835390075, Val loss: 0.4605995016662698\n",
            "Epoch: 74, Train loss: 0.28483229743287936, Val loss: 0.45969078787847567\n",
            "Epoch: 75, Train loss: 0.2815374127780257, Val loss: 0.4561365936931811\n",
            "Epoch: 76, Train loss: 0.28595702622479213, Val loss: 0.4557234970362563\n",
            "Epoch: 77, Train loss: 0.27541399087669194, Val loss: 0.47749881544395495\n",
            "Epoch: 78, Train loss: 0.27006135059714365, Val loss: 0.46879860425465986\n",
            "Epoch: 79, Train loss: 0.26521153451928964, Val loss: 0.4664162048383763\n",
            "Epoch: 80, Train loss: 0.2728711365874215, Val loss: 0.47074615053440394\n",
            "Epoch: 81, Train loss: 0.2720657477232856, Val loss: 0.49312974296902357\n",
            "Epoch: 82, Train loss: 0.2733300360656962, Val loss: 0.46558849493923943\n",
            "Epoch: 83, Train loss: 0.26973357962050243, Val loss: 0.4597692199443516\n",
            "Epoch: 84, Train loss: 0.2664681524473529, Val loss: 0.49894687631412554\n",
            "Epoch: 85, Train loss: 0.26922770158355197, Val loss: 0.4996172125010114\n",
            "Epoch: 86, Train loss: 0.26376439207894364, Val loss: 0.47136316566090836\n",
            "Epoch: 87, Train loss: 0.2645031361658591, Val loss: 0.4781728724115773\n",
            "Epoch: 88, Train loss: 0.2529100579621478, Val loss: 0.49628018078051117\n",
            "Epoch: 89, Train loss: 0.251686855242446, Val loss: 0.5045752625324225\n",
            "Epoch: 90, Train loss: 0.2529958988251863, Val loss: 0.4963734071505697\n",
            "Epoch: 91, Train loss: 0.2576626623017291, Val loss: 0.5255346790348229\n",
            "Epoch: 92, Train loss: 0.25089395813341386, Val loss: 0.5164365772353975\n",
            "Epoch: 93, Train loss: 0.25072114268211826, Val loss: 0.5111939952169594\n",
            "Epoch: 94, Train loss: 0.2472905787457678, Val loss: 0.5059561280435637\n",
            "Epoch: 95, Train loss: 0.2438769390777267, Val loss: 0.515585125080849\n",
            "Epoch: 96, Train loss: 0.2424700222271459, Val loss: 0.5118321380332896\n",
            "Epoch: 97, Train loss: 0.24236245051250585, Val loss: 0.5163983577176144\n",
            "Epoch: 98, Train loss: 0.24513300772174035, Val loss: 0.5470697734701006\n",
            "Epoch: 99, Train loss: 0.24231260257218593, Val loss: 0.5209746717622405\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXw0R-Tkucgw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(classifier, dataloader):\n",
        "  with torch.no_grad():\n",
        "    classifier.eval()\n",
        "    accuracy = 0\n",
        "    for inputs, targets in dataloader:\n",
        "      if torch.cuda.is_available():\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "      \n",
        "      outputs = classifier(inputs.float())\n",
        "      predictions = torch.round(outputs).int()\n",
        "      equals = (targets == predictions.squeeze())\n",
        "      accuracy += torch.mean(equals.float())\n",
        "\n",
        "    accuracy = accuracy / len(dataloader)\n",
        "    return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlrNw0bEqAeh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "8c5d5814-d186-40bf-d810-0ff6c87fc488"
      },
      "source": [
        "classifier = load_classifier()\n",
        "if torch.cuda.is_available():\n",
        "  classifier = classifier.cuda()\n",
        "train_accuracy = accuracy(classifier, train_dataloader)\n",
        "test_accuracy = accuracy(classifier, test_dataloader)\n",
        "print('Epochs: {}'.format(classifier.epochs))\n",
        "print('Train accuracy: {}'.format(train_accuracy.item()))\n",
        "print('Test accuracy: {}'.format(test_accuracy.item()))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epochs: 32\n",
            "Train accuracy: 0.8506277203559875\n",
            "Test accuracy: 0.8401461243629456\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "918KM5F07hFl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}