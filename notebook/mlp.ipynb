{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JIT-SDP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dinaldoap/jit-sdp-nn/blob/master/notebook/mlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXwvNEAQE6NO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import random\n",
        "from scipy.stats import mstats"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAupgTDfmJ_I",
        "colab_type": "code",
        "outputId": "746bd98d-dd1c-4b00-ef6d-0d5cfaed6430",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/dinaldoap/jit-sdp-data/master/jenkins.csv')\n",
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fix</th>\n",
              "      <th>ns</th>\n",
              "      <th>nd</th>\n",
              "      <th>nf</th>\n",
              "      <th>entropy</th>\n",
              "      <th>la</th>\n",
              "      <th>ld</th>\n",
              "      <th>lt</th>\n",
              "      <th>ndev</th>\n",
              "      <th>age</th>\n",
              "      <th>nuc</th>\n",
              "      <th>exp</th>\n",
              "      <th>rexp</th>\n",
              "      <th>sexp</th>\n",
              "      <th>author_date_unix_timestamp</th>\n",
              "      <th>classification</th>\n",
              "      <th>contains_bug</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>False</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.641604</td>\n",
              "      <td>9.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>426.428571</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.000093</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5171.0</td>\n",
              "      <td>30.227271</td>\n",
              "      <td>1472.714286</td>\n",
              "      <td>1555326371</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>False</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.750000</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>426.428571</td>\n",
              "      <td>100.0</td>\n",
              "      <td>6.314775</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5170.0</td>\n",
              "      <td>29.227271</td>\n",
              "      <td>1471.714286</td>\n",
              "      <td>1555326363</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>False</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.906580</td>\n",
              "      <td>15.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>96.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.034722</td>\n",
              "      <td>2.0</td>\n",
              "      <td>629.0</td>\n",
              "      <td>14.828373</td>\n",
              "      <td>414.000000</td>\n",
              "      <td>1554971763</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>False</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.058824</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1554969774</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>False</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.662506</td>\n",
              "      <td>14.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>67.000000</td>\n",
              "      <td>6.0</td>\n",
              "      <td>21.280683</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.058824</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1554967752</td>\n",
              "      <td>Feature Addition</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     fix   ns   nd  ...  author_date_unix_timestamp    classification  contains_bug\n",
              "0  False  7.0  7.0  ...                  1555326371              None         False\n",
              "1  False  7.0  7.0  ...                  1555326363              None         False\n",
              "2  False  1.0  1.0  ...                  1554971763              None         False\n",
              "3  False  1.0  1.0  ...                  1554969774              None         False\n",
              "4  False  1.0  2.0  ...                  1554967752  Feature Addition         False\n",
              "\n",
              "[5 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alizaxdPD3El",
        "colab_type": "code",
        "outputId": "38dc72fd-a649-40db-e331-c75e206ec679",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "df = df.sample(frac=1)\n",
        "label_col = 'contains_bug'\n",
        "features_cols = ['fix', 'ns', 'nd', 'nf', 'entropy', 'la', 'ld', 'lt', 'ndev', 'age', 'nuc', 'exp', 'rexp', 'sexp', 'classification']\n",
        "x = df[features_cols]\n",
        "x['fix'] = x['fix'].astype('int')\n",
        "df_classification = pd.get_dummies(x, columns=['classification'])\n",
        "x = pd.concat([x, df_classification], axis='columns')\n",
        "x = x.drop(['classification'], axis='columns')\n",
        "x = x.values\n",
        "y = df[label_col]\n",
        "y = y.astype('category')\n",
        "y = y.cat.codes\n",
        "y = y.values"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBD1-tp2ExzQ",
        "colab_type": "code",
        "outputId": "bb7be564-3fe7-4cce-828d-b3e8aa993669",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "print(x[:3])\n",
        "print(y[:3])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.00000000e+00 3.00000000e+00 3.00000000e+00 5.00000000e+00\n",
            "  2.26289017e+00 3.30000000e+01 1.20000000e+01 1.09200000e+02\n",
            "  1.00000000e+00 2.00793590e+02 5.00000000e+00 2.52000000e+03\n",
            "  1.85456250e+02 8.45000000e+02 0.00000000e+00 3.00000000e+00\n",
            "  3.00000000e+00 5.00000000e+00 2.26289017e+00 3.30000000e+01\n",
            "  1.20000000e+01 1.09200000e+02 1.00000000e+00 2.00793590e+02\n",
            "  5.00000000e+00 2.52000000e+03 1.85456250e+02 8.45000000e+02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 1.02000000e+02 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.44700000e+03\n",
            "  1.48454606e+02 2.36000000e+02 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 1.02000000e+02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 1.44700000e+03 1.48454606e+02 2.36000000e+02\n",
            "  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 6.00000000e+00 0.00000000e+00 4.62000000e+02\n",
            "  3.00000000e+00 5.96621528e+00 1.00000000e+00 2.38800000e+03\n",
            "  1.41955449e+02 1.02000000e+02 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 6.00000000e+00\n",
            "  0.00000000e+00 4.62000000e+02 3.00000000e+00 5.96621528e+00\n",
            "  1.00000000e+00 2.38800000e+03 1.41955449e+02 1.02000000e+02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "[0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwCn1uNTMjfG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_index = int( len(x) * 0.8 )\n",
        "test_index = int( len(x) * 0.9 )\n",
        "x_train, y_train = x[:val_index], y[:val_index]\n",
        "x_val, y_val = x[val_index:test_index], y[val_index:test_index]\n",
        "x_test, y_test = x[test_index:], y[test_index:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSqnoapE5Poq",
        "colab_type": "code",
        "outputId": "d0253a7f-3366-487b-e592-bd153a0911bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(x_train[:3])\n",
        "print(x_val[:3])\n",
        "print(x_test[:3])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.00000000e+00 3.00000000e+00 3.00000000e+00 5.00000000e+00\n",
            "  2.26289017e+00 3.30000000e+01 1.20000000e+01 1.09200000e+02\n",
            "  1.00000000e+00 2.00793590e+02 5.00000000e+00 2.52000000e+03\n",
            "  1.85456250e+02 8.45000000e+02 0.00000000e+00 3.00000000e+00\n",
            "  3.00000000e+00 5.00000000e+00 2.26289017e+00 3.30000000e+01\n",
            "  1.20000000e+01 1.09200000e+02 1.00000000e+00 2.00793590e+02\n",
            "  5.00000000e+00 2.52000000e+03 1.85456250e+02 8.45000000e+02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 1.02000000e+02 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.44700000e+03\n",
            "  1.48454606e+02 2.36000000e+02 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 1.02000000e+02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 1.44700000e+03 1.48454606e+02 2.36000000e+02\n",
            "  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 6.00000000e+00 0.00000000e+00 4.62000000e+02\n",
            "  3.00000000e+00 5.96621528e+00 1.00000000e+00 2.38800000e+03\n",
            "  1.41955449e+02 1.02000000e+02 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 6.00000000e+00\n",
            "  0.00000000e+00 4.62000000e+02 3.00000000e+00 5.96621528e+00\n",
            "  1.00000000e+00 2.38800000e+03 1.41955449e+02 1.02000000e+02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "[[0.00000000e+00 1.00000000e+00 1.00000000e+00 2.00000000e+00\n",
            "  4.89779014e-01 7.30000000e+01 2.00000000e+00 6.75000000e+01\n",
            "  1.00000000e+00 1.20837731e+01 1.00000000e+00 2.48400000e+03\n",
            "  1.57296549e+02 2.14200000e+03 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 2.00000000e+00 4.89779014e-01 7.30000000e+01\n",
            "  2.00000000e+00 6.75000000e+01 1.00000000e+00 1.20837731e+01\n",
            "  1.00000000e+00 2.48400000e+03 1.57296549e+02 2.14200000e+03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 8.00000000e+00 6.00000000e+00 3.96600000e+03\n",
            "  5.30000000e+01 7.40972222e-02 1.00000000e+00 1.93100000e+03\n",
            "  1.08420845e+02 6.65000000e+02 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 8.00000000e+00\n",
            "  6.00000000e+00 3.96600000e+03 5.30000000e+01 7.40972222e-02\n",
            "  1.00000000e+00 1.93100000e+03 1.08420845e+02 6.65000000e+02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 1.00000000e+00 1.82000000e+02\n",
            "  1.00000000e+01 1.68018634e+01 1.00000000e+00 6.37800000e+03\n",
            "  1.43124754e+02 5.20200000e+03 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.82000000e+02 1.00000000e+01 1.68018634e+01\n",
            "  1.00000000e+00 6.37800000e+03 1.43124754e+02 5.20200000e+03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "[[1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 3.00000000e+00 2.00000000e+00 6.80000000e+01\n",
            "  1.00000000e+00 5.26620370e-03 1.00000000e+00 4.96700000e+03\n",
            "  2.32727477e+02 4.17900000e+03 1.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 3.00000000e+00\n",
            "  2.00000000e+00 6.80000000e+01 1.00000000e+00 5.26620370e-03\n",
            "  1.00000000e+00 4.96700000e+03 2.32727477e+02 4.17900000e+03\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.00000000e+00 1.00000000e+00 3.00000000e+00 4.00000000e+00\n",
            "  1.99923898e+00 2.80000000e+01 2.50000000e+01 1.92750000e+02\n",
            "  5.00000000e+00 1.68004745e+01 4.00000000e+00 4.60000000e+01\n",
            "  2.40333333e+01 4.50000000e+01 1.00000000e+00 1.00000000e+00\n",
            "  3.00000000e+00 4.00000000e+00 1.99923898e+00 2.80000000e+01\n",
            "  2.50000000e+01 1.92750000e+02 5.00000000e+00 1.68004745e+01\n",
            "  4.00000000e+00 4.60000000e+01 2.40333333e+01 4.50000000e+01\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.00000000e+00 2.00000000e+00 2.00000000e+00 2.00000000e+00\n",
            "  9.99473020e-01 1.80000000e+01 1.90000000e+01 2.35000000e+02\n",
            "  2.00000000e+00 1.09899306e+00 1.00000000e+00 2.13100000e+03\n",
            "  1.79166900e+02 1.02900000e+03 1.00000000e+00 2.00000000e+00\n",
            "  2.00000000e+00 2.00000000e+00 9.99473020e-01 1.80000000e+01\n",
            "  1.90000000e+01 2.35000000e+02 2.00000000e+00 1.09899306e+00\n",
            "  1.00000000e+00 2.13100000e+03 1.79166900e+02 1.02900000e+03\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46STsbti3AmT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean_train = x_train.mean(axis=0)\n",
        "std_train = x_train.std(axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5vvbrrK3PES",
        "colab_type": "code",
        "outputId": "897c0241-9094-404d-8a7f-b653027e7f72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "print(mean_train)\n",
        "print(std_train)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2.80165503e-01 1.58965606e+00 2.51005948e+00 3.86516680e+00\n",
            " 7.27548432e-01 4.69955004e+01 2.44943884e+01 7.12848151e+02\n",
            " 1.89522627e+01 8.41431085e+01 1.87535557e+00 1.75053794e+03\n",
            " 8.54844961e+01 9.02305949e+02 2.80165503e-01 1.58965606e+00\n",
            " 2.51005948e+00 3.86516680e+00 7.27548432e-01 4.69955004e+01\n",
            " 2.44943884e+01 7.12848151e+02 1.89522627e+01 8.41431085e+01\n",
            " 1.87535557e+00 1.75053794e+03 8.54844961e+01 9.02305949e+02\n",
            " 2.80165503e-01 1.56038273e-01 3.54797000e-02 4.77527799e-01\n",
            " 1.54124644e-02 3.53762607e-02]\n",
            "[4.49079942e-01 1.61874734e+00 6.27510749e+00 1.68820786e+01\n",
            " 1.11725382e+00 5.20006369e+02 5.15577377e+02 1.07342769e+03\n",
            " 2.66921932e+01 2.21071813e+02 4.18577891e+00 1.98331660e+03\n",
            " 8.18967217e+01 1.30622988e+03 4.49079942e-01 1.61874734e+00\n",
            " 6.27510749e+00 1.68820786e+01 1.11725382e+00 5.20006369e+02\n",
            " 5.15577377e+02 1.07342769e+03 2.66921932e+01 2.21071813e+02\n",
            " 4.18577891e+00 1.98331660e+03 8.18967217e+01 1.30622988e+03\n",
            " 4.49079942e-01 3.62891623e-01 1.84988894e-01 4.99494745e-01\n",
            " 1.23186527e-01 1.84728939e-01]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2IoFA5h425Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = (x_train - mean_train) / std_train\n",
        "x_val = (x_val - mean_train) / std_train\n",
        "x_test = (x_test - mean_train) / std_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1AoKNoU5V1w",
        "colab_type": "code",
        "outputId": "eaa11e46-36d1-4304-eabc-b566ebdb780c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(x_train[:3])\n",
        "print(x_val[:3])\n",
        "print(x_test[:3])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.62386555  0.87125637  0.07807683  0.06722118  1.37421033 -0.02691409\n",
            "  -0.02423378 -0.56235568 -0.67256604  0.52765877  0.74649056  0.38796734\n",
            "   1.2207052  -0.04387126 -0.62386555  0.87125637  0.07807683  0.06722118\n",
            "   1.37421033 -0.02691409 -0.02423378 -0.56235568 -0.67256604  0.52765877\n",
            "   0.74649056  0.38796734  1.2207052  -0.04387126 -0.62386555 -0.42998588\n",
            "  -0.19179368  1.0460014  -0.12511486 -0.19150362]\n",
            " [-0.62386555 -0.36426689 -0.24064281 -0.16971647 -0.65119351  0.10577659\n",
            "  -0.04750866 -0.66408586 -0.71003018 -0.38061437 -0.44803025 -0.15304563\n",
            "   0.76889659 -0.51009854 -0.62386555 -0.36426689 -0.24064281 -0.16971647\n",
            "  -0.65119351  0.10577659 -0.04750866 -0.66408586 -0.71003018 -0.38061437\n",
            "  -0.44803025 -0.15304563  0.76889659 -0.51009854 -0.62386555  2.32565778\n",
            "  -0.19179368 -0.95602167 -0.12511486 -0.19150362]\n",
            " [-0.62386555 -0.36426689 -0.24064281 -0.16971647 -0.65119351 -0.07883654\n",
            "  -0.04750866 -0.23368891 -0.59763777 -0.35362669 -0.20912609  0.32141216\n",
            "   0.68953863 -0.61268385 -0.62386555 -0.36426689 -0.24064281 -0.16971647\n",
            "  -0.65119351 -0.07883654 -0.04750866 -0.23368891 -0.59763777 -0.35362669\n",
            "  -0.20912609  0.32141216  0.68953863 -0.61268385 -0.62386555 -0.42998588\n",
            "  -0.19179368  1.0460014  -0.12511486 -0.19150362]]\n",
            "[[-0.62386555 -0.36426689 -0.24064281 -0.11048206 -0.21281594  0.05000804\n",
            "  -0.04362951 -0.60120319 -0.67256604 -0.32595442 -0.20912609  0.36981593\n",
            "   0.87686114  0.9490627  -0.62386555 -0.36426689 -0.24064281 -0.11048206\n",
            "  -0.21281594  0.05000804 -0.04362951 -0.60120319 -0.67256604 -0.32595442\n",
            "  -0.20912609  0.36981593  0.87686114  0.9490627  -0.62386555 -0.42998588\n",
            "  -0.19179368  1.0460014  -0.12511486 -0.19150362]\n",
            " [-0.62386555 -0.36426689 -0.24064281 -0.16971647 -0.65119351 -0.07499043\n",
            "  -0.03587122  3.03062041  1.27556911 -0.3802792  -0.20912609  0.09099004\n",
            "   0.28006432 -0.18167242 -0.62386555 -0.36426689 -0.24064281 -0.16971647\n",
            "  -0.65119351 -0.07499043 -0.03587122  3.03062041  1.27556911 -0.3802792\n",
            "  -0.20912609  0.09099004  0.28006432 -0.18167242 -0.62386555 -0.42998588\n",
            "  -0.19179368  1.0460014  -0.12511486 -0.19150362]\n",
            " [-0.62386555 -0.36426689 -0.24064281 -0.16971647 -0.65119351 -0.0884518\n",
            "  -0.04556908 -0.49453555 -0.3353888  -0.30461253 -0.20912609  2.33319384\n",
            "   0.70381642  3.29168252 -0.62386555 -0.36426689 -0.24064281 -0.16971647\n",
            "  -0.65119351 -0.0884518  -0.04556908 -0.49453555 -0.3353888  -0.30461253\n",
            "  -0.20912609  2.33319384  0.70381642  3.29168252 -0.62386555 -0.42998588\n",
            "  -0.19179368  1.0460014  -0.12511486 -0.19150362]]\n",
            "[[ 1.60290948e+00 -3.64266892e-01 -2.40642806e-01 -1.69716471e-01\n",
            "  -6.51193507e-01 -8.46056953e-02 -4.36295102e-02 -6.00737394e-01\n",
            "  -6.72566041e-01 -3.80590548e-01 -2.09126089e-01  1.62175926e+00\n",
            "   1.79791056e+00  2.50851256e+00  1.60290948e+00 -3.64266892e-01\n",
            "  -2.40642806e-01 -1.69716471e-01 -6.51193507e-01 -8.46056953e-02\n",
            "  -4.36295102e-02 -6.00737394e-01 -6.72566041e-01 -3.80590548e-01\n",
            "  -2.09126089e-01  1.62175926e+00  1.79791056e+00  2.50851256e+00\n",
            "   1.60290948e+00 -4.29985876e-01 -1.91793676e-01 -9.56021668e-01\n",
            "  -1.25114855e-01 -1.91503621e-01]\n",
            " [ 1.60290948e+00 -3.64266892e-01  7.80768334e-02  7.98676558e-03\n",
            "   1.13822887e+00 -3.65293610e-02  9.80670619e-04 -4.84520902e-01\n",
            "  -5.22709491e-01 -3.04618816e-01  5.07586394e-01 -8.59438145e-01\n",
            "  -7.50349483e-01 -6.56320884e-01  1.60290948e+00 -3.64266892e-01\n",
            "   7.80768334e-02  7.98676558e-03  1.13822887e+00 -3.65293610e-02\n",
            "   9.80670619e-04 -4.84520902e-01 -5.22709491e-01 -3.04618816e-01\n",
            "   5.07586394e-01 -8.59438145e-01 -7.50349483e-01 -6.56320884e-01\n",
            "   1.60290948e+00 -4.29985876e-01 -1.91793676e-01 -9.56021668e-01\n",
            "  -1.25114855e-01 -1.91503621e-01]\n",
            " [ 1.60290948e+00  2.53494739e-01 -8.12829865e-02 -1.10482059e-01\n",
            "   2.43386582e-01 -5.57598947e-02 -1.06567679e-02 -4.45161008e-01\n",
            "  -6.35101903e-01 -3.75643165e-01 -2.09126089e-01  1.91831231e-01\n",
            "   1.14390909e+00  9.69921554e-02  1.60290948e+00  2.53494739e-01\n",
            "  -8.12829865e-02 -1.10482059e-01  2.43386582e-01 -5.57598947e-02\n",
            "  -1.06567679e-02 -4.45161008e-01 -6.35101903e-01 -3.75643165e-01\n",
            "  -2.09126089e-01  1.91831231e-01  1.14390909e+00  9.69921554e-02\n",
            "   1.60290948e+00 -4.29985876e-01 -1.91793676e-01 -9.56021668e-01\n",
            "  -1.25114855e-01 -1.91503621e-01]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNcuSWBQp2Ua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_sampler(y):\n",
        "  _, counts = np.unique(y, return_counts=True)\n",
        "  n_samples = len(y)\n",
        "  class_weights = n_samples / counts\n",
        "  weights = class_weights[y]\n",
        "  return data.WeightedRandomSampler(weights=weights, num_samples=n_samples, replacement=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fh6O6Nz6uC51",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sampler = create_sampler(y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iP6L6GN22_bR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, y_train = torch.from_numpy(x_train), torch.from_numpy(y_train)\n",
        "x_val, y_val = torch.from_numpy(x_val), torch.from_numpy(y_val)\n",
        "x_test, y_test = torch.from_numpy(x_test), torch.from_numpy(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XnnrzRjL3IP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = data.TensorDataset(x_train, y_train)\n",
        "val_dataset = data.TensorDataset(x_val, y_val)\n",
        "test_dataset = data.TensorDataset(x_test, y_test)\n",
        "\n",
        "train_dataloader = data.DataLoader(train_dataset, batch_size=512, sampler=sampler)\n",
        "val_dataloader = data.DataLoader(val_dataset, batch_size=32)\n",
        "test_dataloader = data.DataLoader(test_dataset, batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHoY9-JcORo-",
        "colab_type": "code",
        "outputId": "532f88d9-9dd3-4cff-9f21-cbd98eb938b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        }
      },
      "source": [
        "inputs, targets = next(iter(train_dataloader))\n",
        "print(inputs[:3])\n",
        "print(targets[:3])\n",
        "\n",
        "inputs, targets = next(iter(val_dataloader))\n",
        "print(inputs[:3])\n",
        "print(targets[:3])\n",
        "\n",
        "inputs, targets = next(iter(test_dataloader))\n",
        "print(inputs[:3])\n",
        "print(targets[:3])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.6239,  0.2535, -0.0813, -0.1105,  0.2439, -0.0865, -0.0436, -0.6492,\n",
            "         -0.6726, -0.3802,  0.0298,  1.3651, -0.3635,  0.0151, -0.6239,  0.2535,\n",
            "         -0.0813, -0.1105,  0.2439, -0.0865, -0.0436, -0.6492, -0.6726, -0.3802,\n",
            "          0.0298,  1.3651, -0.3635,  0.0151, -0.6239, -0.4300, -0.1918,  1.0460,\n",
            "         -0.1251, -0.1915],\n",
            "        [-0.6239, -0.3643, -0.2406, -0.1697, -0.6512, -0.0750, -0.0359,  4.4029,\n",
            "          2.1747, -0.3791, -0.2091,  0.7560,  0.3707,  0.2340, -0.6239, -0.3643,\n",
            "         -0.2406, -0.1697, -0.6512, -0.0750, -0.0359,  4.4029,  2.1747, -0.3791,\n",
            "         -0.2091,  0.7560,  0.3707,  0.2340, -0.6239, -0.4300, -0.1918,  1.0460,\n",
            "         -0.1251, -0.1915],\n",
            "        [-0.6239, -0.3643, -0.2406, -0.1697, -0.6512, -0.0865, -0.0436, -0.2039,\n",
            "         -0.6726, -0.3806, -0.2091,  0.4152,  0.5419,  0.4530, -0.6239, -0.3643,\n",
            "         -0.2406, -0.1697, -0.6512, -0.0865, -0.0436, -0.2039, -0.6726, -0.3806,\n",
            "         -0.2091,  0.4152,  0.5419,  0.4530, -0.6239,  2.3257, -0.1918, -0.9560,\n",
            "         -0.1251, -0.1915]], dtype=torch.float64)\n",
            "tensor([0, 0, 1], dtype=torch.int8)\n",
            "tensor([[-0.6239, -0.3643, -0.2406, -0.1105, -0.2128,  0.0500, -0.0436, -0.6012,\n",
            "         -0.6726, -0.3260, -0.2091,  0.3698,  0.8769,  0.9491, -0.6239, -0.3643,\n",
            "         -0.2406, -0.1105, -0.2128,  0.0500, -0.0436, -0.6012, -0.6726, -0.3260,\n",
            "         -0.2091,  0.3698,  0.8769,  0.9491, -0.6239, -0.4300, -0.1918,  1.0460,\n",
            "         -0.1251, -0.1915],\n",
            "        [-0.6239, -0.3643, -0.2406, -0.1697, -0.6512, -0.0750, -0.0359,  3.0306,\n",
            "          1.2756, -0.3803, -0.2091,  0.0910,  0.2801, -0.1817, -0.6239, -0.3643,\n",
            "         -0.2406, -0.1697, -0.6512, -0.0750, -0.0359,  3.0306,  1.2756, -0.3803,\n",
            "         -0.2091,  0.0910,  0.2801, -0.1817, -0.6239, -0.4300, -0.1918,  1.0460,\n",
            "         -0.1251, -0.1915],\n",
            "        [-0.6239, -0.3643, -0.2406, -0.1697, -0.6512, -0.0885, -0.0456, -0.4945,\n",
            "         -0.3354, -0.3046, -0.2091,  2.3332,  0.7038,  3.2917, -0.6239, -0.3643,\n",
            "         -0.2406, -0.1697, -0.6512, -0.0885, -0.0456, -0.4945, -0.3354, -0.3046,\n",
            "         -0.2091,  2.3332,  0.7038,  3.2917, -0.6239, -0.4300, -0.1918,  1.0460,\n",
            "         -0.1251, -0.1915]], dtype=torch.float64)\n",
            "tensor([1, 0, 0], dtype=torch.int8)\n",
            "tensor([[ 1.6029e+00, -3.6427e-01, -2.4064e-01, -1.6972e-01, -6.5119e-01,\n",
            "         -8.4606e-02, -4.3630e-02, -6.0074e-01, -6.7257e-01, -3.8059e-01,\n",
            "         -2.0913e-01,  1.6218e+00,  1.7979e+00,  2.5085e+00,  1.6029e+00,\n",
            "         -3.6427e-01, -2.4064e-01, -1.6972e-01, -6.5119e-01, -8.4606e-02,\n",
            "         -4.3630e-02, -6.0074e-01, -6.7257e-01, -3.8059e-01, -2.0913e-01,\n",
            "          1.6218e+00,  1.7979e+00,  2.5085e+00,  1.6029e+00, -4.2999e-01,\n",
            "         -1.9179e-01, -9.5602e-01, -1.2511e-01, -1.9150e-01],\n",
            "        [ 1.6029e+00, -3.6427e-01,  7.8077e-02,  7.9868e-03,  1.1382e+00,\n",
            "         -3.6529e-02,  9.8067e-04, -4.8452e-01, -5.2271e-01, -3.0462e-01,\n",
            "          5.0759e-01, -8.5944e-01, -7.5035e-01, -6.5632e-01,  1.6029e+00,\n",
            "         -3.6427e-01,  7.8077e-02,  7.9868e-03,  1.1382e+00, -3.6529e-02,\n",
            "          9.8067e-04, -4.8452e-01, -5.2271e-01, -3.0462e-01,  5.0759e-01,\n",
            "         -8.5944e-01, -7.5035e-01, -6.5632e-01,  1.6029e+00, -4.2999e-01,\n",
            "         -1.9179e-01, -9.5602e-01, -1.2511e-01, -1.9150e-01],\n",
            "        [ 1.6029e+00,  2.5349e-01, -8.1283e-02, -1.1048e-01,  2.4339e-01,\n",
            "         -5.5760e-02, -1.0657e-02, -4.4516e-01, -6.3510e-01, -3.7564e-01,\n",
            "         -2.0913e-01,  1.9183e-01,  1.1439e+00,  9.6992e-02,  1.6029e+00,\n",
            "          2.5349e-01, -8.1283e-02, -1.1048e-01,  2.4339e-01, -5.5760e-02,\n",
            "         -1.0657e-02, -4.4516e-01, -6.3510e-01, -3.7564e-01, -2.0913e-01,\n",
            "          1.9183e-01,  1.1439e+00,  9.6992e-02,  1.6029e+00, -4.2999e-01,\n",
            "         -1.9179e-01, -9.5602e-01, -1.2511e-01, -1.9150e-01]],\n",
            "       dtype=torch.float64)\n",
            "tensor([1, 1, 0], dtype=torch.int8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-GctbL8E1oA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Classifier(nn.Module):\n",
        "  def __init__(self, epochs, input_size, hidden_size, val_loss=None):\n",
        "    super(Classifier, self).__init__()\n",
        "    self.epochs = epochs\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.val_loss = val_loss\n",
        "    self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "    self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "    self.fcout = nn.Linear(hidden_size, 1)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = torch.relu(self.fc1(x))\n",
        "    x = torch.relu(self.fc2(x))\n",
        "    x = torch.sigmoid(self.fcout(x))\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inpSePkGLTIs",
        "colab_type": "code",
        "outputId": "7c00a58d-c7a7-409d-f970-26a58da67a5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "classifier = Classifier(epochs=100, input_size=x.shape[1], hidden_size=512)\n",
        "classifier"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Classifier(\n",
              "  (fc1): Linear(in_features=34, out_features=512, bias=True)\n",
              "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
              "  (fcout): Linear(in_features=512, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsT5eIrNoXAh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = 'classifier.cpt'\n",
        "def save_classifier(classifier):\n",
        "  checkpoint = {\n",
        "      'epochs': classifier.epochs,\n",
        "      'input_size': classifier.input_size,\n",
        "      'hidden_size': classifier.hidden_size,\n",
        "      'val_loss': classifier.val_loss,\n",
        "      'state_dict': classifier.state_dict()\n",
        "  }\n",
        "  with open(filename, 'wb') as f:\n",
        "    torch.save(checkpoint, f)\n",
        "\n",
        "def load_classifier():\n",
        "  with open(filename, 'rb') as f:\n",
        "    checkpoint = torch.load(f)\n",
        "  classifier = Classifier(checkpoint['epochs'], checkpoint['input_size'], checkpoint['hidden_size'], checkpoint['val_loss'])\n",
        "  classifier.load_state_dict(checkpoint['state_dict'])\n",
        "  return classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LX_eofKUq0ok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_classifier(classifier)\n",
        "classifier = load_classifier()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PR8s5ZA5rX-V",
        "colab_type": "code",
        "outputId": "56e2d146-dccb-4bc8-9fdd-3a190a0e5f62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  classifier = classifier.cuda()\n",
        "  inputs = inputs.cuda()\n",
        "\n",
        "classifier(inputs[:3].float())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.5125],\n",
              "        [0.4879],\n",
              "        [0.4966]], device='cuda:0', grad_fn=<SigmoidBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arYOr5lMLYy7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(params=classifier.parameters(), lr=0.003)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFy03p96Lza1",
        "colab_type": "code",
        "outputId": "883cfddf-813a-4e32-b1e3-345fc6af536c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_loss = 0\n",
        "epochs = classifier.epochs\n",
        "for epoch in range(epochs):\n",
        "  classifier.train()\n",
        "  for inputs, targets in train_dataloader:\n",
        "    if torch.cuda.is_available():\n",
        "      inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "    outputs = classifier(inputs.float())\n",
        "    loss = criterion(outputs.squeeze(), targets.float())\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    classifier.eval()\n",
        "    val_loss = 0\n",
        "    for inputs, targets in val_dataloader:\n",
        "      if torch.cuda.is_available():\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "      outputs = classifier(inputs.float())\n",
        "      loss = criterion(outputs.squeeze(), targets.float())\n",
        "      val_loss += loss.item()\n",
        "\n",
        "    train_loss = train_loss / len(train_dataloader)\n",
        "    val_loss = val_loss / len(val_dataloader)\n",
        "    print('Epoch: {}, Train loss: {}, Val loss: {}'.format(epoch, train_loss, val_loss))\n",
        "\n",
        "    if classifier.val_loss is None or val_loss < classifier.val_loss:\n",
        "      classifier.val_loss = val_loss\n",
        "      classifier.epochs = epoch\n",
        "      save_classifier(classifier)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Train loss: 0.5700377304303018, Val loss: 0.5830285835422968\n",
            "Epoch: 1, Train loss: 0.5449244233049514, Val loss: 0.5026258332164664\n",
            "Epoch: 2, Train loss: 0.5208037640757665, Val loss: 0.5248800112228644\n",
            "Epoch: 3, Train loss: 0.5101704248370499, Val loss: 0.5450558878089252\n",
            "Epoch: 4, Train loss: 0.4981757103052625, Val loss: 0.5222430684064564\n",
            "Epoch: 5, Train loss: 0.4846446341554504, Val loss: 0.4865446031877869\n",
            "Epoch: 6, Train loss: 0.4818894137057301, Val loss: 0.5440967796664489\n",
            "Epoch: 7, Train loss: 0.4803206242728459, Val loss: 0.5336525589227676\n",
            "Epoch: 8, Train loss: 0.481710764707202, Val loss: 0.4338073487344541\n",
            "Epoch: 9, Train loss: 0.47132879111173576, Val loss: 0.5052180478447362\n",
            "Epoch: 10, Train loss: 0.4789154403076368, Val loss: 0.45737868978788976\n",
            "Epoch: 11, Train loss: 0.4661008955424967, Val loss: 0.5512586832046509\n",
            "Epoch: 12, Train loss: 0.46405875196944707, Val loss: 0.46904259822086286\n",
            "Epoch: 13, Train loss: 0.4575485185862871, Val loss: 0.4802190661430359\n",
            "Epoch: 14, Train loss: 0.4488737603971866, Val loss: 0.5317774913028667\n",
            "Epoch: 15, Train loss: 0.44923608873222354, Val loss: 0.49883718121992915\n",
            "Epoch: 16, Train loss: 0.44068293727645075, Val loss: 0.525223048501893\n",
            "Epoch: 17, Train loss: 0.4470779210740037, Val loss: 0.53013744715013\n",
            "Epoch: 18, Train loss: 0.44590578244028833, Val loss: 0.5791428022478756\n",
            "Epoch: 19, Train loss: 0.44103789741687044, Val loss: 0.463518839525549\n",
            "Epoch: 20, Train loss: 0.43090083892274633, Val loss: 0.5258233566817484\n",
            "Epoch: 21, Train loss: 0.42735329340819955, Val loss: 0.5533521751824179\n",
            "Epoch: 22, Train loss: 0.4204477683679379, Val loss: 0.48159887406386825\n",
            "Epoch: 23, Train loss: 0.42773981553414436, Val loss: 0.5240342715068868\n",
            "Epoch: 24, Train loss: 0.4258102159390583, Val loss: 0.4904273332733857\n",
            "Epoch: 25, Train loss: 0.4277267055571334, Val loss: 0.544816135183761\n",
            "Epoch: 26, Train loss: 0.4186731182087416, Val loss: 0.46935801051164927\n",
            "Epoch: 27, Train loss: 0.4151157412679283, Val loss: 0.512480912239928\n",
            "Epoch: 28, Train loss: 0.4069801082050364, Val loss: 0.5274800760181326\n",
            "Epoch: 29, Train loss: 0.41277713474954314, Val loss: 0.5524409405494991\n",
            "Epoch: 30, Train loss: 0.4089306360005614, Val loss: 0.5030362402137957\n",
            "Epoch: 31, Train loss: 0.3985904041233797, Val loss: 0.48878083260435806\n",
            "Epoch: 32, Train loss: 0.4044679172815543, Val loss: 0.5107329476036524\n",
            "Epoch: 33, Train loss: 0.40131027700603444, Val loss: 0.5068483062480625\n",
            "Epoch: 34, Train loss: 0.38175327872030057, Val loss: 0.5314775352415285\n",
            "Epoch: 35, Train loss: 0.39387784226285577, Val loss: 0.5289985455180469\n",
            "Epoch: 36, Train loss: 0.39033681018926264, Val loss: 0.487891078583504\n",
            "Epoch: 37, Train loss: 0.38216833296150954, Val loss: 0.5467058075101752\n",
            "Epoch: 38, Train loss: 0.3846791632402181, Val loss: 0.5884525795516214\n",
            "Epoch: 39, Train loss: 0.37433302231898496, Val loss: 0.5344663078063413\n",
            "Epoch: 40, Train loss: 0.384491465548745, Val loss: 0.5505772746707264\n",
            "Epoch: 41, Train loss: 0.3706707568037947, Val loss: 0.5712913047326239\n",
            "Epoch: 42, Train loss: 0.3778213856279548, Val loss: 0.5193287855700442\n",
            "Epoch: 43, Train loss: 0.36141361373992265, Val loss: 0.5289579774988326\n",
            "Epoch: 44, Train loss: 0.36582062238620344, Val loss: 0.565987805002614\n",
            "Epoch: 45, Train loss: 0.3567531722141375, Val loss: 0.5822977953051266\n",
            "Epoch: 46, Train loss: 0.3706472610626123, Val loss: 0.549048191230548\n",
            "Epoch: 47, Train loss: 0.36479780259674444, Val loss: 0.606754527672341\n",
            "Epoch: 48, Train loss: 0.3593312894521433, Val loss: 0.5180803915779841\n",
            "Epoch: 49, Train loss: 0.3644817458308012, Val loss: 0.5480362408255276\n",
            "Epoch: 50, Train loss: 0.3596812737268216, Val loss: 0.5582334595291238\n",
            "Epoch: 51, Train loss: 0.36607637204695687, Val loss: 0.5262024259488833\n",
            "Epoch: 52, Train loss: 0.3397683742271099, Val loss: 0.6412388792163447\n",
            "Epoch: 53, Train loss: 0.3682921888081291, Val loss: 0.5887751928285548\n",
            "Epoch: 54, Train loss: 0.3441973487056349, Val loss: 0.5676766151660367\n",
            "Epoch: 55, Train loss: 0.35241034903943813, Val loss: 0.6196918546369201\n",
            "Epoch: 56, Train loss: 0.3445995184096381, Val loss: 0.6163076389777032\n",
            "Epoch: 57, Train loss: 0.3402602861646829, Val loss: 0.5915941509761309\n",
            "Epoch: 58, Train loss: 0.3568662127651771, Val loss: 0.549424055179483\n",
            "Epoch: 59, Train loss: 0.34201928978635027, Val loss: 0.6172673125800333\n",
            "Epoch: 60, Train loss: 0.3293074181219656, Val loss: 0.5642070609487986\n",
            "Epoch: 61, Train loss: 0.3435769695737402, Val loss: 0.5862727714212317\n",
            "Epoch: 62, Train loss: 0.32520237726783136, Val loss: 0.6182860837955224\n",
            "Epoch: 63, Train loss: 0.3243486434646906, Val loss: 0.6069731049631771\n",
            "Epoch: 64, Train loss: 0.3170763454166119, Val loss: 0.6156938283851272\n",
            "Epoch: 65, Train loss: 0.3171986605549253, Val loss: 0.6091572665854504\n",
            "Epoch: 66, Train loss: 0.31901832229689653, Val loss: 0.6173804965066282\n",
            "Epoch: 67, Train loss: 0.3052956117628948, Val loss: 0.6273960467231902\n",
            "Epoch: 68, Train loss: 0.30197638574775615, Val loss: 0.651108113166533\n",
            "Epoch: 69, Train loss: 0.3082636242643617, Val loss: 0.6436765264523657\n",
            "Epoch: 70, Train loss: 0.297251875320902, Val loss: 0.6995033410034681\n",
            "Epoch: 71, Train loss: 0.2968331503533436, Val loss: 0.6733687723937788\n",
            "Epoch: 72, Train loss: 0.29367042431617807, Val loss: 0.6790372121490931\n",
            "Epoch: 73, Train loss: 0.2930268541630384, Val loss: 0.6048755924168386\n",
            "Epoch: 74, Train loss: 0.27918479681970854, Val loss: 0.6846419070896349\n",
            "Epoch: 75, Train loss: 0.2863149245164275, Val loss: 0.6854954957962036\n",
            "Epoch: 76, Train loss: 0.28947082606214564, Val loss: 0.6936231196710938\n",
            "Epoch: 77, Train loss: 0.2939606592444104, Val loss: 0.6498530020839289\n",
            "Epoch: 78, Train loss: 0.29222157201585935, Val loss: 0.6240500760705847\n",
            "Epoch: 79, Train loss: 0.27794375260185195, Val loss: 0.7202226139212909\n",
            "Epoch: 80, Train loss: 0.27801987587347127, Val loss: 0.7653533159510085\n",
            "Epoch: 81, Train loss: 0.2797036645597885, Val loss: 0.779081093245431\n",
            "Epoch: 82, Train loss: 0.2834841562671271, Val loss: 0.7152650473933471\n",
            "Epoch: 83, Train loss: 0.2875502280848141, Val loss: 0.6698160785201349\n",
            "Epoch: 84, Train loss: 0.2693322372082475, Val loss: 0.6962757169416076\n",
            "Epoch: 85, Train loss: 0.2661358955179485, Val loss: 0.7028023310397801\n",
            "Epoch: 86, Train loss: 0.26514906695713814, Val loss: 0.7356358719499487\n",
            "Epoch: 87, Train loss: 0.2629716485245265, Val loss: 0.7367877615125555\n",
            "Epoch: 88, Train loss: 0.2571860769848636, Val loss: 0.709152599306483\n",
            "Epoch: 89, Train loss: 0.26294651684912834, Val loss: 0.7325978292838523\n",
            "Epoch: 90, Train loss: 0.2583997830115266, Val loss: 0.7275865277961681\n",
            "Epoch: 91, Train loss: 0.2553630960990528, Val loss: 0.709377431947934\n",
            "Epoch: 92, Train loss: 0.2541734906743742, Val loss: 0.7601057185154212\n",
            "Epoch: 93, Train loss: 0.25629511484114426, Val loss: 0.7462375156189266\n",
            "Epoch: 94, Train loss: 0.2543628317767612, Val loss: 0.7676472232529992\n",
            "Epoch: 95, Train loss: 0.24950723376884335, Val loss: 0.7929089077209172\n",
            "Epoch: 96, Train loss: 0.2522239947860216, Val loss: 0.844724081848797\n",
            "Epoch: 97, Train loss: 0.24445223995087642, Val loss: 0.7985997525484938\n",
            "Epoch: 98, Train loss: 0.24884987196065828, Val loss: 0.7720612565937796\n",
            "Epoch: 99, Train loss: 0.23471876320628163, Val loss: 0.7914591021835804\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXw0R-Tkucgw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_recalls(targets, predictions):\n",
        "  n_classes = 2\n",
        "  confusion_matrix, _, _ = np.histogram2d(targets.detach().cpu().numpy(), predictions.squeeze().detach().cpu().numpy(), bins=n_classes)\n",
        "  return np.diag(confusion_matrix) / (np.sum(confusion_matrix, axis=1))\n",
        "\n",
        "def gmean(classifier, dataloader):\n",
        "  with torch.no_grad():\n",
        "    classifier.eval()\n",
        "    recalls = np.zeros((2))\n",
        "    for inputs, targets in dataloader:\n",
        "      if torch.cuda.is_available():\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "      \n",
        "      outputs = classifier(inputs.float())\n",
        "      predictions = torch.round(outputs).int()\n",
        "      recalls += calculate_recalls(targets, predictions)\n",
        "\n",
        "    recalls = recalls / len(dataloader)\n",
        "    return mstats.gmean(recalls)    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlrNw0bEqAeh",
        "colab_type": "code",
        "outputId": "9b2aea8c-9e51-4738-9a7a-2a9df1366954",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "classifier = load_classifier()\n",
        "if torch.cuda.is_available():\n",
        "  classifier = classifier.cuda()\n",
        "train_accuracy = gmean(classifier, train_dataloader)\n",
        "test_accuracy = gmean(classifier, test_dataloader)\n",
        "print('Epochs: {}'.format(classifier.epochs))\n",
        "print('Train g-mean: {}'.format(train_accuracy.item()))\n",
        "print('Test g-mean: {}'.format(test_accuracy.item()))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epochs: 8\n",
            "Train g-mean: 0.7614146313143798\n",
            "Test g-mean: 0.7613957045073717\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "918KM5F07hFl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}