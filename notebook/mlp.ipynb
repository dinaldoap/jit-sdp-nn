{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JIT-SDP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dinaldoap/jit-sdp-nn/blob/master/notebook/mlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXwvNEAQE6NO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import random\n",
        "from scipy.stats import mstats"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAupgTDfmJ_I",
        "colab_type": "code",
        "outputId": "f1f0d74d-7b4d-4a88-fc6e-8dd38a7f92ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/dinaldoap/jit-sdp-data/master/jenkins.csv')\n",
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fix</th>\n",
              "      <th>ns</th>\n",
              "      <th>nd</th>\n",
              "      <th>nf</th>\n",
              "      <th>entropy</th>\n",
              "      <th>la</th>\n",
              "      <th>ld</th>\n",
              "      <th>lt</th>\n",
              "      <th>ndev</th>\n",
              "      <th>age</th>\n",
              "      <th>nuc</th>\n",
              "      <th>exp</th>\n",
              "      <th>rexp</th>\n",
              "      <th>sexp</th>\n",
              "      <th>author_date_unix_timestamp</th>\n",
              "      <th>classification</th>\n",
              "      <th>contains_bug</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>False</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.641604</td>\n",
              "      <td>9.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>426.428571</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.000093</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5171.0</td>\n",
              "      <td>30.227271</td>\n",
              "      <td>1472.714286</td>\n",
              "      <td>1555326371</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>False</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.750000</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>426.428571</td>\n",
              "      <td>100.0</td>\n",
              "      <td>6.314775</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5170.0</td>\n",
              "      <td>29.227271</td>\n",
              "      <td>1471.714286</td>\n",
              "      <td>1555326363</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>False</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.906580</td>\n",
              "      <td>15.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>96.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.034722</td>\n",
              "      <td>2.0</td>\n",
              "      <td>629.0</td>\n",
              "      <td>14.828373</td>\n",
              "      <td>414.000000</td>\n",
              "      <td>1554971763</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>False</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.058824</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1554969774</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>False</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.662506</td>\n",
              "      <td>14.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>67.000000</td>\n",
              "      <td>6.0</td>\n",
              "      <td>21.280683</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.058824</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1554967752</td>\n",
              "      <td>Feature Addition</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     fix   ns   nd  ...  author_date_unix_timestamp    classification  contains_bug\n",
              "0  False  7.0  7.0  ...                  1555326371              None         False\n",
              "1  False  7.0  7.0  ...                  1555326363              None         False\n",
              "2  False  1.0  1.0  ...                  1554971763              None         False\n",
              "3  False  1.0  1.0  ...                  1554969774              None         False\n",
              "4  False  1.0  2.0  ...                  1554967752  Feature Addition         False\n",
              "\n",
              "[5 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alizaxdPD3El",
        "colab_type": "code",
        "outputId": "2adbd7df-c702-40d9-99f0-20f26b553c85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "df = df.sample(frac=1)\n",
        "label_col = 'contains_bug'\n",
        "features_cols = ['fix', 'ns', 'nd', 'nf', 'entropy', 'la', 'ld', 'lt', 'ndev', 'age', 'nuc', 'exp', 'rexp', 'sexp', 'classification']\n",
        "x = df[features_cols]\n",
        "x['fix'] = x['fix'].astype('int')\n",
        "df_classification = pd.get_dummies(x, columns=['classification'])\n",
        "x = pd.concat([x, df_classification], axis='columns')\n",
        "x = x.drop(['classification'], axis='columns')\n",
        "x = x.values\n",
        "y = df[label_col]\n",
        "y = y.astype('category')\n",
        "y = y.cat.codes\n",
        "y = y.values"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBD1-tp2ExzQ",
        "colab_type": "code",
        "outputId": "bfdf7c65-d4d4-407c-c20d-1abdb2980b32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "print(x[:3])\n",
        "print(y[:3])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 3.00000000e+00 0.00000000e+00 2.79000000e+02\n",
            "  1.00000000e+01 1.97685185e-01 1.00000000e+00 1.39000000e+02\n",
            "  6.40666667e+01 8.20000000e+01 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 3.00000000e+00\n",
            "  0.00000000e+00 2.79000000e+02 1.00000000e+01 1.97685185e-01\n",
            "  1.00000000e+00 1.39000000e+02 6.40666667e+01 8.20000000e+01\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 4.00000000e+00 0.00000000e+00 1.98000000e+02\n",
            "  7.00000000e+00 3.75470394e+02 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 4.00000000e+00\n",
            "  0.00000000e+00 1.98000000e+02 7.00000000e+00 3.75470394e+02\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 9.00000000e+00 0.00000000e+00 8.24000000e+02\n",
            "  2.30000000e+01 9.84432870e-01 1.00000000e+00 1.41800000e+03\n",
            "  1.20647501e+02 8.29000000e+02 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 9.00000000e+00\n",
            "  0.00000000e+00 8.24000000e+02 2.30000000e+01 9.84432870e-01\n",
            "  1.00000000e+00 1.41800000e+03 1.20647501e+02 8.29000000e+02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "[0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwCn1uNTMjfG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_index = int( len(x) * 0.8 )\n",
        "test_index = int( len(x) * 0.9 )\n",
        "x_train, y_train = x[:val_index], y[:val_index]\n",
        "x_val, y_val = x[val_index:test_index], y[val_index:test_index]\n",
        "x_test, y_test = x[test_index:], y[test_index:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSqnoapE5Poq",
        "colab_type": "code",
        "outputId": "dae87e87-7851-437b-b6b2-91dfa35c2fbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(x_train[:3])\n",
        "print(x_val[:3])\n",
        "print(x_test[:3])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 3.00000000e+00 0.00000000e+00 2.79000000e+02\n",
            "  1.00000000e+01 1.97685185e-01 1.00000000e+00 1.39000000e+02\n",
            "  6.40666667e+01 8.20000000e+01 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 3.00000000e+00\n",
            "  0.00000000e+00 2.79000000e+02 1.00000000e+01 1.97685185e-01\n",
            "  1.00000000e+00 1.39000000e+02 6.40666667e+01 8.20000000e+01\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 4.00000000e+00 0.00000000e+00 1.98000000e+02\n",
            "  7.00000000e+00 3.75470394e+02 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 4.00000000e+00\n",
            "  0.00000000e+00 1.98000000e+02 7.00000000e+00 3.75470394e+02\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 9.00000000e+00 0.00000000e+00 8.24000000e+02\n",
            "  2.30000000e+01 9.84432870e-01 1.00000000e+00 1.41800000e+03\n",
            "  1.20647501e+02 8.29000000e+02 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 9.00000000e+00\n",
            "  0.00000000e+00 8.24000000e+02 2.30000000e+01 9.84432870e-01\n",
            "  1.00000000e+00 1.41800000e+03 1.20647501e+02 8.29000000e+02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "[[1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 0.00000000e+00 8.90000000e+01\n",
            "  3.00000000e+00 1.11295104e+02 1.00000000e+00 3.00000000e+00\n",
            "  5.00000000e-01 2.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 8.90000000e+01 3.00000000e+00 1.11295104e+02\n",
            "  1.00000000e+00 3.00000000e+00 5.00000000e-01 2.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 1.00000000e+00 8.66000000e+02\n",
            "  3.40000000e+01 2.01157407e-01 1.00000000e+00 3.74400000e+03\n",
            "  9.76944634e+01 1.44600000e+03 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 8.66000000e+02 3.40000000e+01 2.01157407e-01\n",
            "  1.00000000e+00 3.74400000e+03 9.76944634e+01 1.44600000e+03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 0.00000000e+00 6.84000000e+02\n",
            "  1.40000000e+01 1.42523148e-01 1.00000000e+00 1.03500000e+03\n",
            "  6.04930126e+01 7.21000000e+02 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 6.84000000e+02 1.40000000e+01 1.42523148e-01\n",
            "  1.00000000e+00 1.03500000e+03 6.04930126e+01 7.21000000e+02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "[[1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 3.00000000e+00 1.00000000e+00 1.51000000e+02\n",
            "  1.00000000e+00 2.91034954e+01 1.00000000e+00 3.89400000e+03\n",
            "  2.08356090e+02 3.39700000e+03 1.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 3.00000000e+00\n",
            "  1.00000000e+00 1.51000000e+02 1.00000000e+00 2.91034954e+01\n",
            "  1.00000000e+00 3.89400000e+03 2.08356090e+02 3.39700000e+03\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 2.00000000e+00 9.00000000e+00 1.70000000e+01\n",
            "  3.54359649e+00 2.32000000e+02 9.70000000e+01 5.11529412e+02\n",
            "  3.00000000e+01 1.24618457e+02 1.40000000e+01 6.85200000e+03\n",
            "  1.13910717e+02 3.10100000e+03 0.00000000e+00 2.00000000e+00\n",
            "  9.00000000e+00 1.70000000e+01 3.54359649e+00 2.32000000e+02\n",
            "  9.70000000e+01 5.11529412e+02 3.00000000e+01 1.24618457e+02\n",
            "  1.40000000e+01 6.85200000e+03 1.13910717e+02 3.10100000e+03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.00000000e+00 1.00000000e+00 1.00000000e+00 5.00000000e+00\n",
            "  2.32192809e+00 1.00000000e+01 1.00000000e+01 2.92000000e+01\n",
            "  6.00000000e+00 1.75892421e+02 4.00000000e+00 5.40000000e+01\n",
            "  4.44469695e+00 3.60000000e+01 1.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 5.00000000e+00 2.32192809e+00 1.00000000e+01\n",
            "  1.00000000e+01 2.92000000e+01 6.00000000e+00 1.75892421e+02\n",
            "  4.00000000e+00 5.40000000e+01 4.44469695e+00 3.60000000e+01\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46STsbti3AmT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean_train = x_train.mean(axis=0)\n",
        "std_train = x_train.std(axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5vvbrrK3PES",
        "colab_type": "code",
        "outputId": "5322d8e6-2859-4f74-da69-e6b044a8891d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "print(mean_train)\n",
        "print(std_train)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2.77010603e-01 1.59001810e+00 2.51574864e+00 3.99772433e+00\n",
            " 7.30469272e-01 5.45791052e+01 2.81344712e+01 7.08145326e+02\n",
            " 1.90090509e+01 8.36493486e+01 1.93943626e+00 1.74181190e+03\n",
            " 8.56089896e+01 9.00974561e+02 2.77010603e-01 1.59001810e+00\n",
            " 2.51574864e+00 3.99772433e+00 7.30469272e-01 5.45791052e+01\n",
            " 2.81344712e+01 7.08145326e+02 1.90090509e+01 8.36493486e+01\n",
            " 1.93943626e+00 1.74181190e+03 8.56089896e+01 9.00974561e+02\n",
            " 2.77010603e-01 1.59296612e-01 3.52728213e-02 4.76027929e-01\n",
            " 1.58262219e-02 3.65658133e-02]\n",
            "[4.47521763e-01 1.61578510e+00 6.37581314e+00 2.48750879e+01\n",
            " 1.11787668e+00 7.46209584e+02 6.18922690e+02 1.06163544e+03\n",
            " 2.68574602e+01 2.18006004e+02 7.51380104e+00 1.97913371e+03\n",
            " 8.21701552e+01 1.30899047e+03 4.47521763e-01 1.61578510e+00\n",
            " 6.37581314e+00 2.48750879e+01 1.11787668e+00 7.46209584e+02\n",
            " 6.18922690e+02 1.06163544e+03 2.68574602e+01 2.18006004e+02\n",
            " 7.51380104e+00 1.97913371e+03 8.21701552e+01 1.30899047e+03\n",
            " 4.47521763e-01 3.65952458e-01 1.84468559e-01 4.99425009e-01\n",
            " 1.24802855e-01 1.87693246e-01]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2IoFA5h425Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = (x_train - mean_train) / std_train\n",
        "x_val = (x_val - mean_train) / std_train\n",
        "x_test = (x_test - mean_train) / std_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1AoKNoU5V1w",
        "colab_type": "code",
        "outputId": "65db33c1-d9ab-4631-c6d5-4ed01ea0fa27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        }
      },
      "source": [
        "print(x_train[:3])\n",
        "print(x_val[:3])\n",
        "print(x_test[:3])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.618988   -0.36515877 -0.23773417 -0.12051111 -0.65344352 -0.06912147\n",
            "  -0.04545717 -0.40423041 -0.33543942 -0.38279525 -0.1250281  -0.80985528\n",
            "  -0.26216724 -0.62565357 -0.618988   -0.36515877 -0.23773417 -0.12051111\n",
            "  -0.65344352 -0.06912147 -0.04545717 -0.40423041 -0.33543942 -0.38279525\n",
            "  -0.1250281  -0.80985528 -0.26216724 -0.62565357 -0.618988   -0.43529319\n",
            "  -0.19121319  1.04915065 -0.12680977 -0.19481688]\n",
            " [-0.618988   -0.36515877 -0.23773417 -0.12051111 -0.65344352 -0.06778137\n",
            "  -0.04545717 -0.48052778 -0.44714023  1.33859178 -0.1250281  -0.88008803\n",
            "  -1.04185016 -0.68829727 -0.618988   -0.36515877 -0.23773417 -0.12051111\n",
            "  -0.65344352 -0.06778137 -0.04545717 -0.48052778 -0.44714023  1.33859178\n",
            "  -0.1250281  -0.88008803 -1.04185016 -0.68829727 -0.618988    2.2973022\n",
            "  -0.19121319 -0.95315196 -0.12680977 -0.19481688]\n",
            " [-0.618988   -0.36515877 -0.23773417 -0.12051111 -0.65344352 -0.06108084\n",
            "  -0.04545717  0.10912849  0.14859741 -0.37918642 -0.1250281  -0.16361295\n",
            "   0.42641408 -0.05498479 -0.618988   -0.36515877 -0.23773417 -0.12051111\n",
            "  -0.65344352 -0.06108084 -0.04545717  0.10912849  0.14859741 -0.37918642\n",
            "  -0.1250281  -0.16361295  0.42641408 -0.05498479 -0.618988   -0.43529319\n",
            "  -0.19121319  1.04915065 -0.12680977 -0.19481688]]\n",
            "[[ 1.6155402  -0.36515877 -0.23773417 -0.12051111 -0.65344352 -0.07180168\n",
            "  -0.04545717 -0.58319956 -0.59607464  0.1268119  -0.1250281  -0.87857222\n",
            "  -1.03576523 -0.68676937  1.6155402  -0.36515877 -0.23773417 -0.12051111\n",
            "  -0.65344352 -0.07180168 -0.04545717 -0.58319956 -0.59607464  0.1268119\n",
            "  -0.1250281  -0.87857222 -1.03576523 -0.68676937  1.6155402  -0.43529319\n",
            "  -0.19121319 -0.95315196 -0.12680977 -0.19481688]\n",
            " [-0.618988   -0.36515877 -0.23773417 -0.12051111 -0.65344352 -0.07180168\n",
            "  -0.04384145  0.14869009  0.55816704 -0.38277933 -0.1250281   1.01164873\n",
            "   0.14707863  0.41637082 -0.618988   -0.36515877 -0.23773417 -0.12051111\n",
            "  -0.65344352 -0.07180168 -0.04384145  0.14869009  0.55816704 -0.38277933\n",
            "  -0.1250281   1.01164873  0.14707863  0.41637082 -0.618988   -0.43529319\n",
            "  -0.19121319  1.04915065 -0.12680977 -0.19481688]\n",
            " [-0.618988   -0.36515877 -0.23773417 -0.12051111 -0.65344352 -0.07180168\n",
            "  -0.04545717 -0.02274352 -0.18650501 -0.38304828 -0.1250281  -0.35713196\n",
            "  -0.30565814 -0.13749112 -0.618988   -0.36515877 -0.23773417 -0.12051111\n",
            "  -0.65344352 -0.07180168 -0.04545717 -0.02274352 -0.18650501 -0.38304828\n",
            "  -0.1250281  -0.35713196 -0.30565814 -0.13749112 -0.618988   -0.43529319\n",
            "  -0.19121319  1.04915065 -0.12680977 -0.19481688]]\n",
            "[[ 1.6155402  -0.36515877 -0.23773417 -0.12051111 -0.65344352 -0.06912147\n",
            "  -0.04384145 -0.5247991  -0.67054185 -0.25020345 -0.1250281   1.08743946\n",
            "   1.49381609  1.9068324   1.6155402  -0.36515877 -0.23773417 -0.12051111\n",
            "  -0.65344352 -0.06912147 -0.04384145 -0.5247991  -0.67054185 -0.25020345\n",
            "  -0.1250281   1.08743946  1.49381609  1.9068324   1.6155402  -0.43529319\n",
            "  -0.19121319 -0.95315196 -0.12680977 -0.19481688]\n",
            " [-0.618988    0.25373541  1.01700775  0.5227027   2.51649155  0.23776282\n",
            "   0.11126677 -0.18520097  0.40923263  0.18792651  1.60512152  2.58203277\n",
            "   0.34442831  1.68070394 -0.618988    0.25373541  1.01700775  0.5227027\n",
            "   2.51649155  0.23776282  0.11126677 -0.18520097  0.40923263  0.18792651\n",
            "   1.60512152  2.58203277  0.34442831  1.68070394 -0.618988   -0.43529319\n",
            "  -0.19121319  1.04915065 -0.12680977 -0.19481688]\n",
            " [ 1.6155402  -0.36515877 -0.23773417  0.04029235  1.42364435 -0.05974073\n",
            "  -0.02930006 -0.63952775 -0.48437383  0.42312171  0.2742372  -0.85280337\n",
            "  -0.98775878 -0.66079516  1.6155402  -0.36515877 -0.23773417  0.04029235\n",
            "   1.42364435 -0.05974073 -0.02930006 -0.63952775 -0.48437383  0.42312171\n",
            "   0.2742372  -0.85280337 -0.98775878 -0.66079516  1.6155402  -0.43529319\n",
            "  -0.19121319 -0.95315196 -0.12680977 -0.19481688]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNcuSWBQp2Ua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_sampler(y):\n",
        "  _, counts = np.unique(y, return_counts=True)\n",
        "  n_samples = len(y)\n",
        "  class_weights = n_samples / counts\n",
        "  weights = class_weights[y]\n",
        "  return data.WeightedRandomSampler(weights=weights, num_samples=n_samples, replacement=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fh6O6Nz6uC51",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sampler = create_sampler(y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iP6L6GN22_bR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, y_train = torch.from_numpy(x_train), torch.from_numpy(y_train)\n",
        "x_val, y_val = torch.from_numpy(x_val), torch.from_numpy(y_val)\n",
        "x_test, y_test = torch.from_numpy(x_test), torch.from_numpy(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XnnrzRjL3IP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = data.TensorDataset(x_train, y_train)\n",
        "val_dataset = data.TensorDataset(x_val, y_val)\n",
        "test_dataset = data.TensorDataset(x_test, y_test)\n",
        "\n",
        "train_dataloader = data.DataLoader(train_dataset, batch_size=512, sampler=sampler)\n",
        "val_dataloader = data.DataLoader(val_dataset, batch_size=32)\n",
        "test_dataloader = data.DataLoader(test_dataset, batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHoY9-JcORo-",
        "colab_type": "code",
        "outputId": "c0d33594-e954-4160-e0ea-4628d5cbb192",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        }
      },
      "source": [
        "inputs, targets = next(iter(train_dataloader))\n",
        "print(inputs[:3])\n",
        "print(targets[:3])\n",
        "\n",
        "inputs, targets = next(iter(val_dataloader))\n",
        "print(inputs[:3])\n",
        "print(targets[:3])\n",
        "\n",
        "inputs, targets = next(iter(test_dataloader))\n",
        "print(inputs[:3])\n",
        "print(targets[:3])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-6.1899e-01, -3.6516e-01,  7.5951e-02,  9.1484e-05,  9.6358e-01,\n",
            "         -3.1599e-02, -3.8994e-02, -2.4645e-01, -5.9607e-01, -2.5859e-01,\n",
            "          2.7424e-01,  5.7156e-01,  1.5072e+00,  1.2185e+00, -6.1899e-01,\n",
            "         -3.6516e-01,  7.5951e-02,  9.1484e-05,  9.6358e-01, -3.1599e-02,\n",
            "         -3.8994e-02, -2.4645e-01, -5.9607e-01, -2.5859e-01,  2.7424e-01,\n",
            "          5.7156e-01,  1.5072e+00,  1.2185e+00, -6.1899e-01, -4.3529e-01,\n",
            "         -1.9121e-01,  1.0492e+00, -1.2681e-01, -1.9482e-01],\n",
            "        [-6.1899e-01, -3.6516e-01, -2.3773e-01, -1.2051e-01, -6.5344e-01,\n",
            "         -2.4898e-02, -4.5457e-02, -2.7707e-01, -6.3331e-01, -3.6440e-01,\n",
            "         -1.2503e-01,  6.5088e-01,  1.7223e+00,  1.3308e+00, -6.1899e-01,\n",
            "         -3.6516e-01, -2.3773e-01, -1.2051e-01, -6.5344e-01, -2.4898e-02,\n",
            "         -4.5457e-02, -2.7707e-01, -6.3331e-01, -3.6440e-01, -1.2503e-01,\n",
            "          6.5088e-01,  1.7223e+00,  1.3308e+00, -6.1899e-01,  2.2973e+00,\n",
            "         -1.9121e-01, -9.5315e-01, -1.2681e-01, -1.9482e-01],\n",
            "        [ 1.6155e+00,  2.5374e-01,  7.5951e-02,  9.1484e-05,  7.2485e-01,\n",
            "         -7.3142e-02,  4.9870e-02, -6.5385e-01, -6.3331e-01, -3.8367e-01,\n",
            "         -1.2503e-01,  1.9707e+00,  1.5318e+00,  1.4893e+00,  1.6155e+00,\n",
            "          2.5374e-01,  7.5951e-02,  9.1484e-05,  7.2485e-01, -7.3142e-02,\n",
            "          4.9870e-02, -6.5385e-01, -6.3331e-01, -3.8367e-01, -1.2503e-01,\n",
            "          1.9707e+00,  1.5318e+00,  1.4893e+00,  1.6155e+00, -4.3529e-01,\n",
            "         -1.9121e-01, -9.5315e-01, -1.2681e-01, -1.9482e-01]],\n",
            "       dtype=torch.float64)\n",
            "tensor([0, 1, 0], dtype=torch.int8)\n",
            "tensor([[ 1.6155, -0.3652, -0.2377, -0.1205, -0.6534, -0.0718, -0.0455, -0.5832,\n",
            "         -0.5961,  0.1268, -0.1250, -0.8786, -1.0358, -0.6868,  1.6155, -0.3652,\n",
            "         -0.2377, -0.1205, -0.6534, -0.0718, -0.0455, -0.5832, -0.5961,  0.1268,\n",
            "         -0.1250, -0.8786, -1.0358, -0.6868,  1.6155, -0.4353, -0.1912, -0.9532,\n",
            "         -0.1268, -0.1948],\n",
            "        [-0.6190, -0.3652, -0.2377, -0.1205, -0.6534, -0.0718, -0.0438,  0.1487,\n",
            "          0.5582, -0.3828, -0.1250,  1.0116,  0.1471,  0.4164, -0.6190, -0.3652,\n",
            "         -0.2377, -0.1205, -0.6534, -0.0718, -0.0438,  0.1487,  0.5582, -0.3828,\n",
            "         -0.1250,  1.0116,  0.1471,  0.4164, -0.6190, -0.4353, -0.1912,  1.0492,\n",
            "         -0.1268, -0.1948],\n",
            "        [-0.6190, -0.3652, -0.2377, -0.1205, -0.6534, -0.0718, -0.0455, -0.0227,\n",
            "         -0.1865, -0.3830, -0.1250, -0.3571, -0.3057, -0.1375, -0.6190, -0.3652,\n",
            "         -0.2377, -0.1205, -0.6534, -0.0718, -0.0455, -0.0227, -0.1865, -0.3830,\n",
            "         -0.1250, -0.3571, -0.3057, -0.1375, -0.6190, -0.4353, -0.1912,  1.0492,\n",
            "         -0.1268, -0.1948]], dtype=torch.float64)\n",
            "tensor([0, 0, 0], dtype=torch.int8)\n",
            "tensor([[ 1.6155, -0.3652, -0.2377, -0.1205, -0.6534, -0.0691, -0.0438, -0.5248,\n",
            "         -0.6705, -0.2502, -0.1250,  1.0874,  1.4938,  1.9068,  1.6155, -0.3652,\n",
            "         -0.2377, -0.1205, -0.6534, -0.0691, -0.0438, -0.5248, -0.6705, -0.2502,\n",
            "         -0.1250,  1.0874,  1.4938,  1.9068,  1.6155, -0.4353, -0.1912, -0.9532,\n",
            "         -0.1268, -0.1948],\n",
            "        [-0.6190,  0.2537,  1.0170,  0.5227,  2.5165,  0.2378,  0.1113, -0.1852,\n",
            "          0.4092,  0.1879,  1.6051,  2.5820,  0.3444,  1.6807, -0.6190,  0.2537,\n",
            "          1.0170,  0.5227,  2.5165,  0.2378,  0.1113, -0.1852,  0.4092,  0.1879,\n",
            "          1.6051,  2.5820,  0.3444,  1.6807, -0.6190, -0.4353, -0.1912,  1.0492,\n",
            "         -0.1268, -0.1948],\n",
            "        [ 1.6155, -0.3652, -0.2377,  0.0403,  1.4236, -0.0597, -0.0293, -0.6395,\n",
            "         -0.4844,  0.4231,  0.2742, -0.8528, -0.9878, -0.6608,  1.6155, -0.3652,\n",
            "         -0.2377,  0.0403,  1.4236, -0.0597, -0.0293, -0.6395, -0.4844,  0.4231,\n",
            "          0.2742, -0.8528, -0.9878, -0.6608,  1.6155, -0.4353, -0.1912, -0.9532,\n",
            "         -0.1268, -0.1948]], dtype=torch.float64)\n",
            "tensor([0, 1, 0], dtype=torch.int8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-GctbL8E1oA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Classifier(nn.Module):\n",
        "  def __init__(self, epochs, input_size, hidden_size, drop_prob, val_loss=None):\n",
        "    super(Classifier, self).__init__()\n",
        "    self.epochs = epochs\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.drop_prob = drop_prob\n",
        "    self.val_loss = val_loss\n",
        "    self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "    self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "    self.fcout = nn.Linear(hidden_size, 1)\n",
        "    self.dropout = nn.Dropout(drop_prob)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = torch.relu(self.fc1(x))\n",
        "    x = self.dropout(x)\n",
        "    x = torch.relu(self.fc2(x))\n",
        "    x = self.dropout(x)\n",
        "    x = torch.sigmoid(self.fcout(x))\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inpSePkGLTIs",
        "colab_type": "code",
        "outputId": "913e9e00-533c-4dce-8961-228d824e2ce8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "classifier = Classifier(epochs=200, input_size=x.shape[1], hidden_size=128, drop_prob=0.2)\n",
        "classifier"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Classifier(\n",
              "  (fc1): Linear(in_features=34, out_features=128, bias=True)\n",
              "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (fcout): Linear(in_features=128, out_features=1, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsT5eIrNoXAh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = 'classifier.cpt'\n",
        "def save_classifier(classifier):\n",
        "  checkpoint = {\n",
        "      'epochs': classifier.epochs,\n",
        "      'input_size': classifier.input_size,\n",
        "      'hidden_size': classifier.hidden_size,\n",
        "      'drop_prob': classifier.drop_prob,\n",
        "      'val_loss': classifier.val_loss,\n",
        "      'state_dict': classifier.state_dict()\n",
        "  }\n",
        "  with open(filename, 'wb') as f:\n",
        "    torch.save(checkpoint, f)\n",
        "\n",
        "def load_classifier():\n",
        "  with open(filename, 'rb') as f:\n",
        "    checkpoint = torch.load(f)\n",
        "  classifier = Classifier(checkpoint['epochs'], checkpoint['input_size'], checkpoint['hidden_size'], checkpoint['drop_prob'], checkpoint['val_loss'])\n",
        "  classifier.load_state_dict(checkpoint['state_dict'])\n",
        "  return classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LX_eofKUq0ok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_classifier(classifier)\n",
        "classifier = load_classifier()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PR8s5ZA5rX-V",
        "colab_type": "code",
        "outputId": "52bf97b1-9e81-4089-c664-841cbf77db4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  classifier = classifier.cuda()\n",
        "  inputs = inputs.cuda()\n",
        "\n",
        "classifier(inputs[:3].float())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4887],\n",
              "        [0.5433],\n",
              "        [0.5303]], device='cuda:0', grad_fn=<SigmoidBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arYOr5lMLYy7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(params=classifier.parameters(), lr=0.003)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFy03p96Lza1",
        "colab_type": "code",
        "outputId": "b0bffa48-dded-438c-d8c1-bc0f7437a66e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_loss = 0\n",
        "epochs = classifier.epochs\n",
        "for epoch in range(epochs):\n",
        "  classifier.train()\n",
        "  for inputs, targets in train_dataloader:\n",
        "    if torch.cuda.is_available():\n",
        "      inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "    outputs = classifier(inputs.float())\n",
        "    loss = criterion(outputs.squeeze(), targets.float())\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    classifier.eval()\n",
        "    val_loss = 0\n",
        "    for inputs, targets in val_dataloader:\n",
        "      if torch.cuda.is_available():\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "      outputs = classifier(inputs.float())\n",
        "      loss = criterion(outputs.squeeze(), targets.float())\n",
        "      val_loss += loss.item()\n",
        "\n",
        "    train_loss = train_loss / len(train_dataloader)\n",
        "    val_loss = val_loss / len(val_dataloader)\n",
        "    print('Epoch: {}, Train loss: {}, Val loss: {}'.format(epoch, train_loss, val_loss))\n",
        "\n",
        "    if classifier.val_loss is None or val_loss < classifier.val_loss:\n",
        "      classifier.val_loss = val_loss\n",
        "      classifier.epochs = epoch\n",
        "      save_classifier(classifier)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Train loss: 0.6078970902844479, Val loss: 0.5394719101880726\n",
            "Epoch: 1, Train loss: 0.5697627487770408, Val loss: 0.5291634845106226\n",
            "Epoch: 2, Train loss: 0.555106465291014, Val loss: 0.4810828637135656\n",
            "Epoch: 3, Train loss: 0.5395144068429092, Val loss: 0.5141895338892937\n",
            "Epoch: 4, Train loss: 0.5315664680987837, Val loss: 0.46009924929392965\n",
            "Epoch: 5, Train loss: 0.5329518560573465, Val loss: 0.525476542350493\n",
            "Epoch: 6, Train loss: 0.5226541716906192, Val loss: 0.5118679482685892\n",
            "Epoch: 7, Train loss: 0.5124962038613535, Val loss: 0.4584381843083783\n",
            "Epoch: 8, Train loss: 0.5030729410073662, Val loss: 0.5071689517874467\n",
            "Epoch: 9, Train loss: 0.5050930465444701, Val loss: 0.47921101239166763\n",
            "Epoch: 10, Train loss: 0.508087327828493, Val loss: 0.4886385880802807\n",
            "Epoch: 11, Train loss: 0.5009951243994133, Val loss: 0.4895663724133843\n",
            "Epoch: 12, Train loss: 0.4939860308123197, Val loss: 0.5244554865517115\n",
            "Epoch: 13, Train loss: 0.49797044355605924, Val loss: 0.489129833092815\n",
            "Epoch: 14, Train loss: 0.4924458448716776, Val loss: 0.46485139468782827\n",
            "Epoch: 15, Train loss: 0.49210840982467663, Val loss: 0.49565813533569636\n",
            "Epoch: 16, Train loss: 0.4915767650652012, Val loss: 0.4841486897907759\n",
            "Epoch: 17, Train loss: 0.48613565778744866, Val loss: 0.4931751619044103\n",
            "Epoch: 18, Train loss: 0.48258596334959664, Val loss: 0.4963426166459134\n",
            "Epoch: 19, Train loss: 0.48815633504905864, Val loss: 0.48826277608934204\n",
            "Epoch: 20, Train loss: 0.48822231241729136, Val loss: 0.513563183969573\n",
            "Epoch: 21, Train loss: 0.48722865580170466, Val loss: 0.46346724660773025\n",
            "Epoch: 22, Train loss: 0.47644657586714917, Val loss: 0.5014285535404557\n",
            "Epoch: 23, Train loss: 0.4804961675339366, Val loss: 0.5090893757970709\n",
            "Epoch: 24, Train loss: 0.4816085507836609, Val loss: 0.46543713384553004\n",
            "Epoch: 25, Train loss: 0.4749904403309202, Val loss: 0.5444354293377776\n",
            "Epoch: 26, Train loss: 0.47350436874967966, Val loss: 0.45359158868852417\n",
            "Epoch: 27, Train loss: 0.470069729295933, Val loss: 0.48556678938238246\n",
            "Epoch: 28, Train loss: 0.4763462392647947, Val loss: 0.4592285101350985\n",
            "Epoch: 29, Train loss: 0.46891926581441273, Val loss: 0.47462960685554306\n",
            "Epoch: 30, Train loss: 0.47228712519396265, Val loss: 0.48374230767551224\n",
            "Epoch: 31, Train loss: 0.46283216007979, Val loss: 0.48308410299451726\n",
            "Epoch: 32, Train loss: 0.4666883672874307, Val loss: 0.47204828928959996\n",
            "Epoch: 33, Train loss: 0.47446884692133817, Val loss: 0.4437037149542256\n",
            "Epoch: 34, Train loss: 0.4708854490676526, Val loss: 0.47969420960074977\n",
            "Epoch: 35, Train loss: 0.4711197644319424, Val loss: 0.48087684143530696\n",
            "Epoch: 36, Train loss: 0.465226916184149, Val loss: 0.4854397028684616\n",
            "Epoch: 37, Train loss: 0.46102441057066446, Val loss: 0.4489619202519718\n",
            "Epoch: 38, Train loss: 0.46126827616035426, Val loss: 0.5254410842531606\n",
            "Epoch: 39, Train loss: 0.45902071501564023, Val loss: 0.47275488470730026\n",
            "Epoch: 40, Train loss: 0.4527431886200714, Val loss: 0.47308022921022613\n",
            "Epoch: 41, Train loss: 0.4595679401849566, Val loss: 0.5005290978833249\n",
            "Epoch: 42, Train loss: 0.4602012126056114, Val loss: 0.4716537249715705\n",
            "Epoch: 43, Train loss: 0.4606092778434036, Val loss: 0.4826963543891907\n",
            "Epoch: 44, Train loss: 0.4612823455678519, Val loss: 0.4779277949741012\n",
            "Epoch: 45, Train loss: 0.45396797601890954, Val loss: 0.45936735366520126\n",
            "Epoch: 46, Train loss: 0.4524531698395579, Val loss: 0.44206464133764567\n",
            "Epoch: 47, Train loss: 0.4537749785503007, Val loss: 0.4578787833452225\n",
            "Epoch: 48, Train loss: 0.4488836657538755, Val loss: 0.44645231452427414\n",
            "Epoch: 49, Train loss: 0.44729405048524795, Val loss: 0.46981606946179744\n",
            "Epoch: 50, Train loss: 0.45186582113646506, Val loss: 0.49132196918914195\n",
            "Epoch: 51, Train loss: 0.4549627123447215, Val loss: 0.44869522162173925\n",
            "Epoch: 52, Train loss: 0.44554440159068814, Val loss: 0.43949394084905324\n",
            "Epoch: 53, Train loss: 0.4452885587953858, Val loss: 0.45683296142440094\n",
            "Epoch: 54, Train loss: 0.4514067860547475, Val loss: 0.48133192250603124\n",
            "Epoch: 55, Train loss: 0.45071385119072116, Val loss: 0.49146065037501485\n",
            "Epoch: 56, Train loss: 0.44633130358334994, Val loss: 0.4605991663901429\n",
            "Epoch: 57, Train loss: 0.4496745650513853, Val loss: 0.45879757247473063\n",
            "Epoch: 58, Train loss: 0.44664898484564636, Val loss: 0.47633368169006546\n",
            "Epoch: 59, Train loss: 0.44397286834675403, Val loss: 0.4507817214256839\n",
            "Epoch: 60, Train loss: 0.44240982236108195, Val loss: 0.4644990724168326\n",
            "Epoch: 61, Train loss: 0.44623537149911413, Val loss: 0.4773750716918393\n",
            "Epoch: 62, Train loss: 0.4424992834494002, Val loss: 0.4406566812019599\n",
            "Epoch: 63, Train loss: 0.4346506282816997, Val loss: 0.48918029156170395\n",
            "Epoch: 64, Train loss: 0.43776662957545615, Val loss: 0.47114221830117076\n",
            "Epoch: 65, Train loss: 0.4400531905293142, Val loss: 0.4703813697162427\n",
            "Epoch: 66, Train loss: 0.44203420678094724, Val loss: 0.4688544967456868\n",
            "Epoch: 67, Train loss: 0.43885328560526843, Val loss: 0.43644712002653824\n",
            "Epoch: 68, Train loss: 0.4410973502850992, Val loss: 0.438577654330354\n",
            "Epoch: 69, Train loss: 0.4409325930990055, Val loss: 0.4706415322266127\n",
            "Epoch: 70, Train loss: 0.4349382564517419, Val loss: 0.45382827775258766\n",
            "Epoch: 71, Train loss: 0.43725008254796943, Val loss: 0.4777527604448168\n",
            "Epoch: 72, Train loss: 0.4366966597044548, Val loss: 0.45412499810519974\n",
            "Epoch: 73, Train loss: 0.4309931133708517, Val loss: 0.4479304010930814\n",
            "Epoch: 74, Train loss: 0.4397018701740555, Val loss: 0.4718453092010398\n",
            "Epoch: 75, Train loss: 0.4406437399379738, Val loss: 0.4359253319470506\n",
            "Epoch: 76, Train loss: 0.4365906420569471, Val loss: 0.48721116506739665\n",
            "Epoch: 77, Train loss: 0.42988308402326253, Val loss: 0.46995675367744344\n",
            "Epoch: 78, Train loss: 0.43086426564800806, Val loss: 0.4686846811520426\n",
            "Epoch: 79, Train loss: 0.4318340559561912, Val loss: 0.45304021749057266\n",
            "Epoch: 80, Train loss: 0.4335372867285156, Val loss: 0.4451695813944465\n",
            "Epoch: 81, Train loss: 0.4272520106144614, Val loss: 0.4225269726624614\n",
            "Epoch: 82, Train loss: 0.4387004442411239, Val loss: 0.46172153165465907\n",
            "Epoch: 83, Train loss: 0.4290507649313922, Val loss: 0.4424476976457395\n",
            "Epoch: 84, Train loss: 0.4283229777032124, Val loss: 0.4440489485859871\n",
            "Epoch: 85, Train loss: 0.42741334221032096, Val loss: 0.4572614101987136\n",
            "Epoch: 86, Train loss: 0.42874833622032793, Val loss: 0.4652276744968013\n",
            "Epoch: 87, Train loss: 0.43277806427354726, Val loss: 0.49229220575407934\n",
            "Epoch: 88, Train loss: 0.4233936354743481, Val loss: 0.4762901804949108\n",
            "Epoch: 89, Train loss: 0.4252100868842765, Val loss: 0.4653981794652186\n",
            "Epoch: 90, Train loss: 0.4237815572604322, Val loss: 0.45722188565291855\n",
            "Epoch: 91, Train loss: 0.42628976737664664, Val loss: 0.4775223990804271\n",
            "Epoch: 92, Train loss: 0.421271826885041, Val loss: 0.4847614851437117\n",
            "Epoch: 93, Train loss: 0.4230086190674884, Val loss: 0.4732750246399327\n",
            "Epoch: 94, Train loss: 0.42938180028130657, Val loss: 0.4727298452665931\n",
            "Epoch: 95, Train loss: 0.42713346326004253, Val loss: 0.454965068321479\n",
            "Epoch: 96, Train loss: 0.4227100051922889, Val loss: 0.45637522637844086\n",
            "Epoch: 97, Train loss: 0.41998798841312085, Val loss: 0.4655914024302834\n",
            "Epoch: 98, Train loss: 0.4271885826383961, Val loss: 0.46462372141449076\n",
            "Epoch: 99, Train loss: 0.41397580053250765, Val loss: 0.4758596090893996\n",
            "Epoch: 100, Train loss: 0.42281398243701274, Val loss: 0.44876317601454885\n",
            "Epoch: 101, Train loss: 0.4228057516423746, Val loss: 0.4749991254586923\n",
            "Epoch: 102, Train loss: 0.42264014705442515, Val loss: 0.4761329587352903\n",
            "Epoch: 103, Train loss: 0.4170270554915915, Val loss: 0.4541028030216694\n",
            "Epoch: 104, Train loss: 0.4148236677643914, Val loss: 0.4738079501610053\n",
            "Epoch: 105, Train loss: 0.4181597009708935, Val loss: 0.46934950626210165\n",
            "Epoch: 106, Train loss: 0.41759232647432165, Val loss: 0.4601402604266217\n",
            "Epoch: 107, Train loss: 0.41292678550858597, Val loss: 0.4728225242150457\n",
            "Epoch: 108, Train loss: 0.4162693664787054, Val loss: 0.4746422077480115\n",
            "Epoch: 109, Train loss: 0.42035578046283656, Val loss: 0.45494608463425384\n",
            "Epoch: 110, Train loss: 0.4164053918902397, Val loss: 0.44870867697816147\n",
            "Epoch: 111, Train loss: 0.41686463936897833, Val loss: 0.46575392704260976\n",
            "Epoch: 112, Train loss: 0.414003085301825, Val loss: 0.46002997732476186\n",
            "Epoch: 113, Train loss: 0.41022223812463277, Val loss: 0.4808377403961985\n",
            "Epoch: 114, Train loss: 0.42832795506562116, Val loss: 0.45030640497019414\n",
            "Epoch: 115, Train loss: 0.41811116372436946, Val loss: 0.46485548937007\n",
            "Epoch: 116, Train loss: 0.4115894441633085, Val loss: 0.45254334003517505\n",
            "Epoch: 117, Train loss: 0.4159791391273043, Val loss: 0.4616323752622855\n",
            "Epoch: 118, Train loss: 0.40823920677936837, Val loss: 0.45445305383519125\n",
            "Epoch: 119, Train loss: 0.40541343759832593, Val loss: 0.4698338539976823\n",
            "Epoch: 120, Train loss: 0.4081623080716159, Val loss: 0.4768845356608692\n",
            "Epoch: 121, Train loss: 0.41017236083052044, Val loss: 0.4590577616503364\n",
            "Epoch: 122, Train loss: 0.4145678004071643, Val loss: 0.4687199482792302\n",
            "Epoch: 123, Train loss: 0.4080086241177444, Val loss: 0.4537142838694547\n",
            "Epoch: 124, Train loss: 0.40349997316016367, Val loss: 0.4717082381248474\n",
            "Epoch: 125, Train loss: 0.40275690776062995, Val loss: 0.4551119710269727\n",
            "Epoch: 126, Train loss: 0.40586150655149184, Val loss: 0.4829983581837855\n",
            "Epoch: 127, Train loss: 0.40349662172402173, Val loss: 0.4647261912101193\n",
            "Epoch: 128, Train loss: 0.4085020250049613, Val loss: 0.470068679436257\n",
            "Epoch: 129, Train loss: 0.4089575240642548, Val loss: 0.4564794717650664\n",
            "Epoch: 130, Train loss: 0.4079441133550142, Val loss: 0.4686887079947873\n",
            "Epoch: 131, Train loss: 0.4063050950917532, Val loss: 0.5096154318828332\n",
            "Epoch: 132, Train loss: 0.40695297379596435, Val loss: 0.49306145153547587\n",
            "Epoch: 133, Train loss: 0.40536155503551774, Val loss: 0.4648178757021302\n",
            "Epoch: 134, Train loss: 0.3978763040224608, Val loss: 0.47154741616625534\n",
            "Epoch: 135, Train loss: 0.40867795080552993, Val loss: 0.4914696381280297\n",
            "Epoch: 136, Train loss: 0.39823155537263616, Val loss: 0.4601147476779787\n",
            "Epoch: 137, Train loss: 0.3920935907214922, Val loss: 0.49744444967884766\n",
            "Epoch: 138, Train loss: 0.39783656262057404, Val loss: 0.482148729656872\n",
            "Epoch: 139, Train loss: 0.4015594994453739, Val loss: 0.4616104583991201\n",
            "Epoch: 140, Train loss: 0.41103244605667794, Val loss: 0.47173983133152914\n",
            "Epoch: 141, Train loss: 0.39672946558265504, Val loss: 0.4983182800443549\n",
            "Epoch: 142, Train loss: 0.3943253393826434, Val loss: 0.4727468957242213\n",
            "Epoch: 143, Train loss: 0.4034171565474584, Val loss: 0.47670293952289383\n",
            "Epoch: 144, Train loss: 0.4021712020408206, Val loss: 0.4652461119388279\n",
            "Epoch: 145, Train loss: 0.3974546548969157, Val loss: 0.47197109735325765\n",
            "Epoch: 146, Train loss: 0.39738443038245724, Val loss: 0.45332033912602226\n",
            "Epoch: 147, Train loss: 0.39941824960640143, Val loss: 0.46461371842183563\n",
            "Epoch: 148, Train loss: 0.3944861696707395, Val loss: 0.4614143179435479\n",
            "Epoch: 149, Train loss: 0.3981376708770718, Val loss: 0.4729587718060142\n",
            "Epoch: 150, Train loss: 0.3979808856029911, Val loss: 0.4635281060871325\n",
            "Epoch: 151, Train loss: 0.4003119410364764, Val loss: 0.45580838483415154\n",
            "Epoch: 152, Train loss: 0.40305796171826475, Val loss: 0.4750305449492053\n",
            "Epoch: 153, Train loss: 0.40274955329811424, Val loss: 0.45864055148865046\n",
            "Epoch: 154, Train loss: 0.39737988028022175, Val loss: 0.4760889975648177\n",
            "Epoch: 155, Train loss: 0.40429164874791385, Val loss: 0.4603151428072076\n",
            "Epoch: 156, Train loss: 0.40394298154749353, Val loss: 0.4628630943204227\n",
            "Epoch: 157, Train loss: 0.4001205299294408, Val loss: 0.4626795206415026\n",
            "Epoch: 158, Train loss: 0.3968877605021952, Val loss: 0.47397519648075104\n",
            "Epoch: 159, Train loss: 0.3994319049711315, Val loss: 0.461726718435162\n",
            "Epoch: 160, Train loss: 0.399902097248705, Val loss: 0.47359234565182734\n",
            "Epoch: 161, Train loss: 0.39572393716070925, Val loss: 0.4912060824664016\n",
            "Epoch: 162, Train loss: 0.3994004067152984, Val loss: 0.4650794457840292\n",
            "Epoch: 163, Train loss: 0.40012396463273453, Val loss: 0.49298683122584697\n",
            "Epoch: 164, Train loss: 0.3994441952853136, Val loss: 0.46985203263006714\n",
            "Epoch: 165, Train loss: 0.3971479504426026, Val loss: 0.47529789530917216\n",
            "Epoch: 166, Train loss: 0.3889452061347786, Val loss: 0.46768102226288694\n",
            "Epoch: 167, Train loss: 0.38883559599595274, Val loss: 0.47040583781505885\n",
            "Epoch: 168, Train loss: 0.39691867762884475, Val loss: 0.45664696395397186\n",
            "Epoch: 169, Train loss: 0.3884104033661728, Val loss: 0.4749162214759149\n",
            "Epoch: 170, Train loss: 0.3907861823061915, Val loss: 0.4720816427939816\n",
            "Epoch: 171, Train loss: 0.3861735395543926, Val loss: 0.4489029596902822\n",
            "Epoch: 172, Train loss: 0.38728577978766054, Val loss: 0.4527526921347568\n",
            "Epoch: 173, Train loss: 0.3902892645742172, Val loss: 0.47785293820657226\n",
            "Epoch: 174, Train loss: 0.3908828658402637, Val loss: 0.47754583743057755\n",
            "Epoch: 175, Train loss: 0.3905997108015593, Val loss: 0.4674461585910697\n",
            "Epoch: 176, Train loss: 0.38951949495650484, Val loss: 0.47609339928940725\n",
            "Epoch: 177, Train loss: 0.38834612260224977, Val loss: 0.4698203333506459\n",
            "Epoch: 178, Train loss: 0.3914863581432487, Val loss: 0.455544851916401\n",
            "Epoch: 179, Train loss: 0.39359529660190823, Val loss: 0.4621541205989687\n",
            "Epoch: 180, Train loss: 0.3964464590858069, Val loss: 0.4749097094724053\n",
            "Epoch: 181, Train loss: 0.39312391277444636, Val loss: 0.48850755981708827\n",
            "Epoch: 182, Train loss: 0.3889436679441396, Val loss: 0.4616103137010022\n",
            "Epoch: 183, Train loss: 0.3813917173036323, Val loss: 0.48056323355750036\n",
            "Epoch: 184, Train loss: 0.38496888895068354, Val loss: 0.4857276940816327\n",
            "Epoch: 185, Train loss: 0.38743654395907434, Val loss: 0.45739595631235525\n",
            "Epoch: 186, Train loss: 0.3893930881969158, Val loss: 0.4665820826041071\n",
            "Epoch: 187, Train loss: 0.38459410079150874, Val loss: 0.4727622266662748\n",
            "Epoch: 188, Train loss: 0.3929378473465457, Val loss: 0.47244784432022197\n",
            "Epoch: 189, Train loss: 0.3873733699941447, Val loss: 0.47552535133926493\n",
            "Epoch: 190, Train loss: 0.3839134749629618, Val loss: 0.475848002457305\n",
            "Epoch: 191, Train loss: 0.38902147395793424, Val loss: 0.475281245614353\n",
            "Epoch: 192, Train loss: 0.3869139854801809, Val loss: 0.46121362302648394\n",
            "Epoch: 193, Train loss: 0.3898900567039406, Val loss: 0.46579594125873164\n",
            "Epoch: 194, Train loss: 0.3940874866711062, Val loss: 0.4691836826111141\n",
            "Epoch: 195, Train loss: 0.384303096700308, Val loss: 0.4696978583539787\n",
            "Epoch: 196, Train loss: 0.39099767029726185, Val loss: 0.46966977731177684\n",
            "Epoch: 197, Train loss: 0.38638521083103033, Val loss: 0.4890862079827409\n",
            "Epoch: 198, Train loss: 0.38724873825330125, Val loss: 0.47047236365707296\n",
            "Epoch: 199, Train loss: 0.38073790265918267, Val loss: 0.48284911462350893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXw0R-Tkucgw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_recalls(targets, predictions):\n",
        "  n_classes = 2\n",
        "  confusion_matrix, _, _ = np.histogram2d(targets.detach().cpu().numpy(), predictions.squeeze().detach().cpu().numpy(), bins=n_classes)\n",
        "  return np.diag(confusion_matrix) / (np.sum(confusion_matrix, axis=1))\n",
        "\n",
        "def gmean(classifier, dataloader):\n",
        "  with torch.no_grad():\n",
        "    classifier.eval()\n",
        "    recalls = np.zeros((2))\n",
        "    for inputs, targets in dataloader:\n",
        "      if torch.cuda.is_available():\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "      \n",
        "      outputs = classifier(inputs.float())\n",
        "      predictions = torch.round(outputs).int()\n",
        "      recalls += calculate_recalls(targets, predictions)\n",
        "\n",
        "    recalls = recalls / len(dataloader)\n",
        "    return mstats.gmean(recalls)    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlrNw0bEqAeh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(classifier):\n",
        "  if torch.cuda.is_available():\n",
        "    classifier = classifier.cuda()\n",
        "  train_accuracy = gmean(classifier, train_dataloader)\n",
        "  test_accuracy = gmean(classifier, test_dataloader)\n",
        "  print('Epochs: {}'.format(classifier.epochs))\n",
        "  print('Train g-mean: {}'.format(train_accuracy.item()))\n",
        "  print('Test g-mean: {}'.format(test_accuracy.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFoBTASdvqRv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "62637fc3-6987-4fae-9755-a4b6b065b929"
      },
      "source": [
        "print('Best classifier')\n",
        "evaluate(load_classifier())"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best classifier\n",
            "Epochs: 81\n",
            "Train g-mean: 0.7964218100449051\n",
            "Test g-mean: 0.731752076399906\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "918KM5F07hFl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}