{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JIT-SDP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dinaldoap/jit-sdp-nn/blob/master/notebook/mlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXwvNEAQE6NO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import random\n",
        "from scipy.stats import mstats"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAupgTDfmJ_I",
        "colab_type": "code",
        "outputId": "26416f78-59db-4b65-e97f-47ccf2e5ff61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/dinaldoap/jit-sdp-data/master/jenkins.csv')\n",
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fix</th>\n",
              "      <th>ns</th>\n",
              "      <th>nd</th>\n",
              "      <th>nf</th>\n",
              "      <th>entropy</th>\n",
              "      <th>la</th>\n",
              "      <th>ld</th>\n",
              "      <th>lt</th>\n",
              "      <th>ndev</th>\n",
              "      <th>age</th>\n",
              "      <th>nuc</th>\n",
              "      <th>exp</th>\n",
              "      <th>rexp</th>\n",
              "      <th>sexp</th>\n",
              "      <th>author_date_unix_timestamp</th>\n",
              "      <th>classification</th>\n",
              "      <th>contains_bug</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>False</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.641604</td>\n",
              "      <td>9.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>426.428571</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.000093</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5171.0</td>\n",
              "      <td>30.227271</td>\n",
              "      <td>1472.714286</td>\n",
              "      <td>1555326371</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>False</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.750000</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>426.428571</td>\n",
              "      <td>100.0</td>\n",
              "      <td>6.314775</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5170.0</td>\n",
              "      <td>29.227271</td>\n",
              "      <td>1471.714286</td>\n",
              "      <td>1555326363</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>False</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.906580</td>\n",
              "      <td>15.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>96.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.034722</td>\n",
              "      <td>2.0</td>\n",
              "      <td>629.0</td>\n",
              "      <td>14.828373</td>\n",
              "      <td>414.000000</td>\n",
              "      <td>1554971763</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>False</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.058824</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1554969774</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>False</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.662506</td>\n",
              "      <td>14.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>67.000000</td>\n",
              "      <td>6.0</td>\n",
              "      <td>21.280683</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.058824</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1554967752</td>\n",
              "      <td>Feature Addition</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     fix   ns   nd  ...  author_date_unix_timestamp    classification  contains_bug\n",
              "0  False  7.0  7.0  ...                  1555326371              None         False\n",
              "1  False  7.0  7.0  ...                  1555326363              None         False\n",
              "2  False  1.0  1.0  ...                  1554971763              None         False\n",
              "3  False  1.0  1.0  ...                  1554969774              None         False\n",
              "4  False  1.0  2.0  ...                  1554967752  Feature Addition         False\n",
              "\n",
              "[5 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alizaxdPD3El",
        "colab_type": "code",
        "outputId": "11b37b14-b5ae-4759-a44a-a2d8fc767f24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "df = df.sample(frac=1)\n",
        "label_col = 'contains_bug'\n",
        "features_cols = ['fix', 'ns', 'nd', 'nf', 'entropy', 'la', 'ld', 'lt', 'ndev', 'age', 'nuc', 'exp', 'rexp', 'sexp', 'classification']\n",
        "x = df[features_cols]\n",
        "x['fix'] = x['fix'].astype('int')\n",
        "df_classification = pd.get_dummies(x, columns=['classification'])\n",
        "x = pd.concat([x, df_classification], axis='columns')\n",
        "x = x.drop(['classification'], axis='columns')\n",
        "x = x.values\n",
        "y = df[label_col]\n",
        "y = y.astype('category')\n",
        "y = y.cat.codes\n",
        "y = y.values"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBD1-tp2ExzQ",
        "colab_type": "code",
        "outputId": "66f0fff1-8462-4bb0-92cd-8ef7eabc98d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "print(x[:3])\n",
        "print(y[:3])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 1.00000000e+00 4.00000000e+01\n",
            "  5.00000000e+00 9.03027141e+02 1.00000000e+00 3.54000000e+02\n",
            "  1.07083505e+01 1.96000000e+02 1.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 4.00000000e+01 5.00000000e+00 9.03027141e+02\n",
            "  1.00000000e+00 3.54000000e+02 1.07083505e+01 1.96000000e+02\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 1.80000000e+01 1.00000000e+00 5.30000000e+01\n",
            "  1.00000000e+00 1.02440972e+00 1.00000000e+00 4.07100000e+03\n",
            "  2.79254815e+02 4.40000000e+01 1.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 1.80000000e+01\n",
            "  1.00000000e+00 5.30000000e+01 1.00000000e+00 1.02440972e+00\n",
            "  1.00000000e+00 4.07100000e+03 2.79254815e+02 4.40000000e+01\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 3.90000000e+01 5.20000000e+01\n",
            "  5.25956963e+00 8.60000000e+01 7.60000000e+01 4.66346154e+01\n",
            "  1.30000000e+01 1.79232125e+02 4.50000000e+01 4.48700000e+03\n",
            "  1.73040291e+02 3.79100000e+03 0.00000000e+00 1.00000000e+00\n",
            "  3.90000000e+01 5.20000000e+01 5.25956963e+00 8.60000000e+01\n",
            "  7.60000000e+01 4.66346154e+01 1.30000000e+01 1.79232125e+02\n",
            "  4.50000000e+01 4.48700000e+03 1.73040291e+02 3.79100000e+03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "[0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwCn1uNTMjfG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_index = int( len(x) * 0.8 )\n",
        "test_index = int( len(x) * 0.9 )\n",
        "x_train, y_train = x[:val_index], y[:val_index]\n",
        "x_val, y_val = x[val_index:test_index], y[val_index:test_index]\n",
        "x_test, y_test = x[test_index:], y[test_index:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSqnoapE5Poq",
        "colab_type": "code",
        "outputId": "47e5b70e-97e2-4793-f35c-eb15602b5b98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(x_train[:3])\n",
        "print(x_val[:3])\n",
        "print(x_test[:3])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 1.00000000e+00 4.00000000e+01\n",
            "  5.00000000e+00 9.03027141e+02 1.00000000e+00 3.54000000e+02\n",
            "  1.07083505e+01 1.96000000e+02 1.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 4.00000000e+01 5.00000000e+00 9.03027141e+02\n",
            "  1.00000000e+00 3.54000000e+02 1.07083505e+01 1.96000000e+02\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 1.80000000e+01 1.00000000e+00 5.30000000e+01\n",
            "  1.00000000e+00 1.02440972e+00 1.00000000e+00 4.07100000e+03\n",
            "  2.79254815e+02 4.40000000e+01 1.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 1.80000000e+01\n",
            "  1.00000000e+00 5.30000000e+01 1.00000000e+00 1.02440972e+00\n",
            "  1.00000000e+00 4.07100000e+03 2.79254815e+02 4.40000000e+01\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 3.90000000e+01 5.20000000e+01\n",
            "  5.25956963e+00 8.60000000e+01 7.60000000e+01 4.66346154e+01\n",
            "  1.30000000e+01 1.79232125e+02 4.50000000e+01 4.48700000e+03\n",
            "  1.73040291e+02 3.79100000e+03 0.00000000e+00 1.00000000e+00\n",
            "  3.90000000e+01 5.20000000e+01 5.25956963e+00 8.60000000e+01\n",
            "  7.60000000e+01 4.66346154e+01 1.30000000e+01 1.79232125e+02\n",
            "  4.50000000e+01 4.48700000e+03 1.73040291e+02 3.79100000e+03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "[[0.00000000e+00 2.00000000e+00 2.00000000e+00 2.00000000e+00\n",
            "  9.18295834e-01 3.00000000e+00 3.00000000e+00 4.85500000e+02\n",
            "  2.80000000e+01 5.08709491e+00 2.00000000e+00 1.54000000e+02\n",
            "  3.81627269e+01 7.60000000e+01 0.00000000e+00 2.00000000e+00\n",
            "  2.00000000e+00 2.00000000e+00 9.18295834e-01 3.00000000e+00\n",
            "  3.00000000e+00 4.85500000e+02 2.80000000e+01 5.08709491e+00\n",
            "  2.00000000e+00 1.54000000e+02 3.81627269e+01 7.60000000e+01\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 1.00000000e+00 3.50000000e+01\n",
            "  1.00000000e+01 4.29997350e+02 1.00000000e+00 1.00000000e+00\n",
            "  2.43902439e-02 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 3.50000000e+01 1.00000000e+01 4.29997350e+02\n",
            "  1.00000000e+00 1.00000000e+00 2.43902439e-02 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 0.00000000e+00 6.60000000e+01\n",
            "  1.00000000e+00 1.16898148e-03 1.00000000e+00 4.09000000e+02\n",
            "  1.35648846e+02 3.29000000e+02 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 6.60000000e+01 1.00000000e+00 1.16898148e-03\n",
            "  1.00000000e+00 4.09000000e+02 1.35648846e+02 3.29000000e+02\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "[[0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 6.00000000e+00 1.00000000e+00 5.68000000e+02\n",
            "  1.30000000e+01 1.23641968e+02 1.00000000e+00 5.60000000e+01\n",
            "  9.23606982e+00 3.50000000e+01 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 6.00000000e+00\n",
            "  1.00000000e+00 5.68000000e+02 1.30000000e+01 1.23641968e+02\n",
            "  1.00000000e+00 5.60000000e+01 9.23606982e+00 3.50000000e+01\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 7.00000000e+00 0.00000000e+00 8.90000000e+01\n",
            "  1.00000000e+00 6.13943287e+00 1.00000000e+00 2.17000000e+02\n",
            "  2.49891872e+01 6.40000000e+01 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 7.00000000e+00\n",
            "  0.00000000e+00 8.90000000e+01 1.00000000e+00 6.13943287e+00\n",
            "  1.00000000e+00 2.17000000e+02 2.49891872e+01 6.40000000e+01\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 7.00000000e+00 7.00000000e+00 7.00000000e+00\n",
            "  2.78989810e+00 8.00000000e+00 7.00000000e+00 1.55428571e+02\n",
            "  1.00000000e+00 1.08762731e+00 2.00000000e+00 1.00800000e+03\n",
            "  2.61042050e+02 1.90714286e+02 0.00000000e+00 7.00000000e+00\n",
            "  7.00000000e+00 7.00000000e+00 2.78989810e+00 8.00000000e+00\n",
            "  7.00000000e+00 1.55428571e+02 1.00000000e+00 1.08762731e+00\n",
            "  2.00000000e+00 1.00800000e+03 2.61042050e+02 1.90714286e+02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46STsbti3AmT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean_train = x_train.mean(axis=0)\n",
        "std_train = x_train.std(axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5vvbrrK3PES",
        "colab_type": "code",
        "outputId": "fd4d8f6a-fd29-4558-c800-6915f8e491f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "print(mean_train)\n",
        "print(std_train)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2.83527282e-01 1.59327644e+00 2.48125162e+00 3.91574864e+00\n",
            " 7.26402248e-01 5.14172744e+01 2.60072925e+01 7.06101901e+02\n",
            " 1.88825446e+01 8.25845774e+01 1.92955780e+00 1.74923243e+03\n",
            " 8.61675769e+01 9.07292368e+02 2.83527282e-01 1.59327644e+00\n",
            " 2.48125162e+00 3.91574864e+00 7.26402248e-01 5.14172744e+01\n",
            " 2.60072925e+01 7.06101901e+02 1.88825446e+01 8.25845774e+01\n",
            " 1.92955780e+00 1.74923243e+03 8.61675769e+01 9.07292368e+02\n",
            " 2.83527282e-01 1.56865787e-01 3.51693819e-02 4.73028187e-01\n",
            " 1.57745022e-02 3.56348591e-02]\n",
            "[4.50710065e-01 1.62355187e+00 6.11580745e+00 2.45395811e+01\n",
            " 1.11022257e+00 7.14663943e+02 5.31087907e+02 1.05905710e+03\n",
            " 2.67314279e+01 2.15463261e+02 7.53001427e+00 1.98126550e+03\n",
            " 8.23179580e+01 1.31428998e+03 4.50710065e-01 1.62355187e+00\n",
            " 6.11580745e+00 2.45395811e+01 1.11022257e+00 7.14663943e+02\n",
            " 5.31087907e+02 1.05905710e+03 2.67314279e+01 2.15463261e+02\n",
            " 7.53001427e+00 1.98126550e+03 8.23179580e+01 1.31428998e+03\n",
            " 4.50710065e-01 3.63674184e-01 1.84207754e-01 4.99271991e-01\n",
            " 1.24602036e-01 1.85378035e-01]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2IoFA5h425Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = (x_train - mean_train) / std_train\n",
        "x_val = (x_val - mean_train) / std_train\n",
        "x_test = (x_test - mean_train) / std_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1AoKNoU5V1w",
        "colab_type": "code",
        "outputId": "8b5ca49d-200b-47f7-8f40-246c262e2cb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        }
      },
      "source": [
        "print(x_train[:3])\n",
        "print(x_val[:3])\n",
        "print(x_test[:3])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1.58965325 -0.36541884 -0.2422005  -0.11881819 -0.65428525 -0.07054683\n",
            "  -0.04708692 -0.62895749 -0.51933419  3.80780723 -0.12344702 -0.70421275\n",
            "  -0.91668001 -0.54119896  1.58965325 -0.36541884 -0.2422005  -0.11881819\n",
            "  -0.65428525 -0.07054683 -0.04708692 -0.62895749 -0.51933419  3.80780723\n",
            "  -0.12344702 -0.70421275 -0.91668001 -0.54119896  1.58965325 -0.43133605\n",
            "  -0.19092238 -0.94743586 -0.12659907 -0.19222805]\n",
            " [ 1.58965325 -0.36541884 -0.2422005  -0.11881819 -0.65428525 -0.04675942\n",
            "  -0.04708692 -0.61668242 -0.6689708  -0.37853399 -0.12344702  1.1718609\n",
            "   2.34562716 -0.65685076  1.58965325 -0.36541884 -0.2422005  -0.11881819\n",
            "  -0.65428525 -0.04675942 -0.04708692 -0.61668242 -0.6689708  -0.37853399\n",
            "  -0.12344702  1.1718609   2.34562716 -0.65685076  1.58965325 -0.43133605\n",
            "  -0.19092238 -0.94743586 -0.12659907 -0.19222805]\n",
            " [-0.62906801 -0.36541884  5.97120637  1.9594569   4.08311586  0.04839019\n",
            "   0.09413264 -0.62269285 -0.22006099  0.44855697  5.7198354   1.38182771\n",
            "   1.05533126  2.19411825 -0.62906801 -0.36541884  5.97120637  1.9594569\n",
            "   4.08311586  0.04839019  0.09413264 -0.62269285 -0.22006099  0.44855697\n",
            "   5.7198354   1.38182771  1.05533126  2.19411825 -0.62906801 -0.43133605\n",
            "  -0.19092238  1.05548042 -0.12659907 -0.19222805]]\n",
            "[[-0.62906801  0.25051467 -0.07868979 -0.0780677   0.17284245 -0.06774831\n",
            "  -0.04332106 -0.20830029  0.34107626 -0.35967841  0.00935486 -0.80515833\n",
            "  -0.58316376 -0.63250301 -0.62906801  0.25051467 -0.07868979 -0.0780677\n",
            "   0.17284245 -0.06774831 -0.04332106 -0.20830029  0.34107626 -0.35967841\n",
            "   0.00935486 -0.80515833 -0.58316376 -0.63250301 -0.62906801 -0.43133605\n",
            "  -0.19092238  1.05548042 -0.12659907 -0.19222805]\n",
            " [-0.62906801 -0.36541884 -0.2422005  -0.11881819 -0.65428525 -0.07054683\n",
            "  -0.04708692 -0.63367868 -0.33228844  1.61239912 -0.12344702 -0.8823817\n",
            "  -1.04646895 -0.69032891 -0.62906801 -0.36541884 -0.2422005  -0.11881819\n",
            "  -0.65428525 -0.07054683 -0.04708692 -0.63367868 -0.33228844  1.61239912\n",
            "  -0.12344702 -0.8823817  -1.04646895 -0.69032891 -0.62906801 -0.43133605\n",
            "  -0.19092238  1.05548042 -0.12659907 -0.19222805]\n",
            " [-0.62906801 -0.36541884 -0.2422005  -0.11881819 -0.65428525 -0.07054683\n",
            "  -0.04896984 -0.60440735 -0.6689708  -0.38328302 -0.12344702 -0.67645271\n",
            "   0.60109932 -0.44000363 -0.62906801 -0.36541884 -0.2422005  -0.11881819\n",
            "  -0.65428525 -0.07054683 -0.04896984 -0.60440735 -0.6689708  -0.38328302\n",
            "  -0.12344702 -0.67645271  0.60109932 -0.44000363 -0.62906801 -0.43133605\n",
            "   5.23773076 -0.94743586 -0.12659907 -0.19222805]]\n",
            "[[-0.62906801 -0.36541884 -0.2422005  -0.11881819 -0.65428525 -0.06355053\n",
            "  -0.04708692 -0.13040081 -0.22006099  0.19055402 -0.12344702 -0.85462167\n",
            "  -0.9345653  -0.66369856 -0.62906801 -0.36541884 -0.2422005  -0.11881819\n",
            "  -0.65428525 -0.06355053 -0.04708692 -0.13040081 -0.22006099  0.19055402\n",
            "  -0.12344702 -0.85462167 -0.9345653  -0.66369856 -0.62906801 -0.43133605\n",
            "  -0.19092238  1.05548042 -0.12659907 -0.19222805]\n",
            " [-0.62906801 -0.36541884 -0.2422005  -0.11881819 -0.65428525 -0.06215127\n",
            "  -0.04896984 -0.58268992 -0.6689708  -0.35479434 -0.12344702 -0.77336047\n",
            "  -0.74319615 -0.64163342 -0.62906801 -0.36541884 -0.2422005  -0.11881819\n",
            "  -0.65428525 -0.06215127 -0.04896984 -0.58268992 -0.6689708  -0.35479434\n",
            "  -0.12344702 -0.77336047 -0.74319615 -0.64163342 -0.62906801 -0.43133605\n",
            "  -0.19092238  1.05548042 -0.12659907 -0.19222805]\n",
            " [-0.62906801  3.33018222  0.73886374  0.12568476  1.85863259 -0.06075201\n",
            "  -0.03578935 -0.51996566 -0.6689708  -0.37824059  0.00935486 -0.3741207\n",
            "   2.12437817 -0.54522068 -0.62906801  3.33018222  0.73886374  0.12568476\n",
            "   1.85863259 -0.06075201 -0.03578935 -0.51996566 -0.6689708  -0.37824059\n",
            "   0.00935486 -0.3741207   2.12437817 -0.54522068 -0.62906801 -0.43133605\n",
            "  -0.19092238  1.05548042 -0.12659907 -0.19222805]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iP6L6GN22_bR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, y_train = torch.from_numpy(x_train), torch.from_numpy(y_train)\n",
        "x_val, y_val = torch.from_numpy(x_val), torch.from_numpy(y_val)\n",
        "x_test, y_test = torch.from_numpy(x_test), torch.from_numpy(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XnnrzRjL3IP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = data.TensorDataset(x_train, y_train)\n",
        "val_dataset = data.TensorDataset(x_val, y_val)\n",
        "test_dataset = data.TensorDataset(x_test, y_test)\n",
        "\n",
        "train_dataloader = data.DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
        "val_dataloader = data.DataLoader(val_dataset, batch_size=32)\n",
        "test_dataloader = data.DataLoader(test_dataset, batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHoY9-JcORo-",
        "colab_type": "code",
        "outputId": "73cf48d3-a9cf-4f4c-f2e0-3bc36fcc6d28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "source": [
        "inputs, targets = next(iter(train_dataloader))\n",
        "print(inputs[:3])\n",
        "print(targets[:3])\n",
        "\n",
        "inputs, targets = next(iter(val_dataloader))\n",
        "print(inputs[:3])\n",
        "print(targets[:3])\n",
        "\n",
        "inputs, targets = next(iter(test_dataloader))\n",
        "print(inputs[:3])\n",
        "print(targets[:3])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.6291, -0.3654, -0.2422, -0.0781, -0.2886, -0.0342, -0.0301, -0.6393,\n",
            "         -0.6316, -0.0083,  0.0094,  1.9244,  1.6085,  2.8477, -0.6291, -0.3654,\n",
            "         -0.2422, -0.0781, -0.2886, -0.0342, -0.0301, -0.6393, -0.6316, -0.0083,\n",
            "          0.0094,  1.9244,  1.6085,  2.8477, -0.6291, -0.4313, -0.1909,  1.0555,\n",
            "         -0.1266, -0.1922],\n",
            "        [-0.6291, -0.3654, -0.2422, -0.0373,  0.6892, -0.0132, -0.0094,  0.1075,\n",
            "         -0.5942, -0.3362,  0.1422, -0.0415,  1.2146,  0.4061, -0.6291, -0.3654,\n",
            "         -0.2422, -0.0373,  0.6892, -0.0132, -0.0094,  0.1075, -0.5942, -0.3362,\n",
            "          0.1422, -0.0415,  1.2146,  0.4061, -0.6291, -0.4313, -0.1909,  1.0555,\n",
            "         -0.1266, -0.1922],\n",
            "        [-0.6291, -0.9814, -0.4057, -0.1596, -0.6543, -0.0719, -0.0490, -0.6667,\n",
            "         -0.7064, -0.3833, -0.2562, -0.8829, -1.0468, -0.6903, -0.6291, -0.9814,\n",
            "         -0.4057, -0.1596, -0.6543, -0.0719, -0.0490, -0.6667, -0.7064, -0.3833,\n",
            "         -0.2562, -0.8829, -1.0468, -0.6903, -0.6291,  2.3184, -0.1909, -0.9474,\n",
            "         -0.1266, -0.1922]], dtype=torch.float64)\n",
            "tensor([0, 1, 0], dtype=torch.int8)\n",
            "tensor([[-0.6291,  0.2505, -0.0787, -0.0781,  0.1728, -0.0677, -0.0433, -0.2083,\n",
            "          0.3411, -0.3597,  0.0094, -0.8052, -0.5832, -0.6325, -0.6291,  0.2505,\n",
            "         -0.0787, -0.0781,  0.1728, -0.0677, -0.0433, -0.2083,  0.3411, -0.3597,\n",
            "          0.0094, -0.8052, -0.5832, -0.6325, -0.6291, -0.4313, -0.1909,  1.0555,\n",
            "         -0.1266, -0.1922],\n",
            "        [-0.6291, -0.3654, -0.2422, -0.1188, -0.6543, -0.0705, -0.0471, -0.6337,\n",
            "         -0.3323,  1.6124, -0.1234, -0.8824, -1.0465, -0.6903, -0.6291, -0.3654,\n",
            "         -0.2422, -0.1188, -0.6543, -0.0705, -0.0471, -0.6337, -0.3323,  1.6124,\n",
            "         -0.1234, -0.8824, -1.0465, -0.6903, -0.6291, -0.4313, -0.1909,  1.0555,\n",
            "         -0.1266, -0.1922],\n",
            "        [-0.6291, -0.3654, -0.2422, -0.1188, -0.6543, -0.0705, -0.0490, -0.6044,\n",
            "         -0.6690, -0.3833, -0.1234, -0.6765,  0.6011, -0.4400, -0.6291, -0.3654,\n",
            "         -0.2422, -0.1188, -0.6543, -0.0705, -0.0490, -0.6044, -0.6690, -0.3833,\n",
            "         -0.1234, -0.6765,  0.6011, -0.4400, -0.6291, -0.4313,  5.2377, -0.9474,\n",
            "         -0.1266, -0.1922]], dtype=torch.float64)\n",
            "tensor([0, 0, 0], dtype=torch.int8)\n",
            "tensor([[-0.6291, -0.3654, -0.2422, -0.1188, -0.6543, -0.0636, -0.0471, -0.1304,\n",
            "         -0.2201,  0.1906, -0.1234, -0.8546, -0.9346, -0.6637, -0.6291, -0.3654,\n",
            "         -0.2422, -0.1188, -0.6543, -0.0636, -0.0471, -0.1304, -0.2201,  0.1906,\n",
            "         -0.1234, -0.8546, -0.9346, -0.6637, -0.6291, -0.4313, -0.1909,  1.0555,\n",
            "         -0.1266, -0.1922],\n",
            "        [-0.6291, -0.3654, -0.2422, -0.1188, -0.6543, -0.0622, -0.0490, -0.5827,\n",
            "         -0.6690, -0.3548, -0.1234, -0.7734, -0.7432, -0.6416, -0.6291, -0.3654,\n",
            "         -0.2422, -0.1188, -0.6543, -0.0622, -0.0490, -0.5827, -0.6690, -0.3548,\n",
            "         -0.1234, -0.7734, -0.7432, -0.6416, -0.6291, -0.4313, -0.1909,  1.0555,\n",
            "         -0.1266, -0.1922],\n",
            "        [-0.6291,  3.3302,  0.7389,  0.1257,  1.8586, -0.0608, -0.0358, -0.5200,\n",
            "         -0.6690, -0.3782,  0.0094, -0.3741,  2.1244, -0.5452, -0.6291,  3.3302,\n",
            "          0.7389,  0.1257,  1.8586, -0.0608, -0.0358, -0.5200, -0.6690, -0.3782,\n",
            "          0.0094, -0.3741,  2.1244, -0.5452, -0.6291, -0.4313, -0.1909,  1.0555,\n",
            "         -0.1266, -0.1922]], dtype=torch.float64)\n",
            "tensor([0, 0, 0], dtype=torch.int8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-GctbL8E1oA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Classifier(nn.Module):\n",
        "  def __init__(self, epochs, input_size, hidden_size, val_loss=None):\n",
        "    super(Classifier, self).__init__()\n",
        "    self.epochs = epochs\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.val_loss = val_loss\n",
        "    self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "    self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "    self.fcout = nn.Linear(hidden_size, 1)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = torch.relu(self.fc1(x))\n",
        "    x = torch.relu(self.fc2(x))\n",
        "    x = torch.sigmoid(self.fcout(x))\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inpSePkGLTIs",
        "colab_type": "code",
        "outputId": "b35d4651-ba74-4ccc-8f37-961cf1894814",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "classifier = Classifier(epochs=100, input_size=x.shape[1], hidden_size=512)\n",
        "classifier"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Classifier(\n",
              "  (fc1): Linear(in_features=34, out_features=512, bias=True)\n",
              "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
              "  (fcout): Linear(in_features=512, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsT5eIrNoXAh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = 'classifier.cpt'\n",
        "def save_classifier(classifier):\n",
        "  checkpoint = {\n",
        "      'epochs': classifier.epochs,\n",
        "      'input_size': classifier.input_size,\n",
        "      'hidden_size': classifier.hidden_size,\n",
        "      'val_loss': classifier.val_loss,\n",
        "      'state_dict': classifier.state_dict()\n",
        "  }\n",
        "  with open(filename, 'wb') as f:\n",
        "    torch.save(checkpoint, f)\n",
        "\n",
        "def load_classifier():\n",
        "  with open(filename, 'rb') as f:\n",
        "    checkpoint = torch.load(f)\n",
        "  classifier = Classifier(checkpoint['epochs'], checkpoint['input_size'], checkpoint['hidden_size'], checkpoint['val_loss'])\n",
        "  classifier.load_state_dict(checkpoint['state_dict'])\n",
        "  return classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LX_eofKUq0ok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_classifier(classifier)\n",
        "classifier = load_classifier()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PR8s5ZA5rX-V",
        "colab_type": "code",
        "outputId": "4e721c6a-7962-4434-f937-fd5cfa4b3ee9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  classifier = classifier.cuda()\n",
        "  inputs = inputs.cuda()\n",
        "\n",
        "classifier(inputs[:3].float())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.5089],\n",
              "        [0.5105],\n",
              "        [0.4651]], device='cuda:0', grad_fn=<SigmoidBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arYOr5lMLYy7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(params=classifier.parameters(), lr=0.003)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFy03p96Lza1",
        "colab_type": "code",
        "outputId": "67f6eb30-2d70-483e-c644-514d3cd62485",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_loss = 0\n",
        "epochs = classifier.epochs\n",
        "for epoch in range(epochs):\n",
        "  classifier.train()\n",
        "  for inputs, targets in train_dataloader:\n",
        "    if torch.cuda.is_available():\n",
        "      inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "    outputs = classifier(inputs.float())\n",
        "    loss = criterion(outputs.squeeze(), targets.float())\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    classifier.eval()\n",
        "    val_loss = 0\n",
        "    for inputs, targets in val_dataloader:\n",
        "      if torch.cuda.is_available():\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "      outputs = classifier(inputs.float())\n",
        "      loss = criterion(outputs.squeeze(), targets.float())\n",
        "      val_loss += loss.item()\n",
        "\n",
        "    train_loss = train_loss / len(train_dataloader)\n",
        "    val_loss = val_loss / len(val_dataloader)\n",
        "    print('Epoch: {}, Train loss: {}, Val loss: {}'.format(epoch, train_loss, val_loss))\n",
        "\n",
        "    if classifier.val_loss is None or val_loss < classifier.val_loss:\n",
        "      classifier.val_loss = val_loss\n",
        "      classifier.epochs = epoch\n",
        "      save_classifier(classifier)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Train loss: 0.4518454851288545, Val loss: 0.4002617338770314\n",
            "Epoch: 1, Train loss: 0.41929472559789543, Val loss: 0.3944960748286624\n",
            "Epoch: 2, Train loss: 0.40892091186268803, Val loss: 0.38618648601205724\n",
            "Epoch: 3, Train loss: 0.39733148086265085, Val loss: 0.3878860126592611\n",
            "Epoch: 4, Train loss: 0.39061833302917065, Val loss: 0.393912924747718\n",
            "Epoch: 5, Train loss: 0.3892999176081706, Val loss: 0.3977085557815276\n",
            "Epoch: 6, Train loss: 0.3841679822559333, Val loss: 0.37690653651952744\n",
            "Epoch: 7, Train loss: 0.3832928031974591, Val loss: 0.3739520317237628\n",
            "Epoch: 8, Train loss: 0.3782837001124327, Val loss: 0.37238402468593496\n",
            "Epoch: 9, Train loss: 0.37417520166394824, Val loss: 0.3731482168169398\n",
            "Epoch: 10, Train loss: 0.3702915768563114, Val loss: 0.3742051793164329\n",
            "Epoch: 11, Train loss: 0.36937798523259624, Val loss: 0.36866230792120885\n",
            "Epoch: 12, Train loss: 0.37039320130707726, Val loss: 0.3644409716913575\n",
            "Epoch: 13, Train loss: 0.36286138585250677, Val loss: 0.36260179734151615\n",
            "Epoch: 14, Train loss: 0.3611179893714925, Val loss: 0.36808714015703453\n",
            "Epoch: 15, Train loss: 0.36098396308982256, Val loss: 0.366598516702652\n",
            "Epoch: 16, Train loss: 0.35981579739999453, Val loss: 0.36173346285757263\n",
            "Epoch: 17, Train loss: 0.35593338287691056, Val loss: 0.36331343513570336\n",
            "Epoch: 18, Train loss: 0.3531526080027013, Val loss: 0.3703500536319457\n",
            "Epoch: 19, Train loss: 0.35771549388832896, Val loss: 0.3572475431781066\n",
            "Epoch: 20, Train loss: 0.3512310410918549, Val loss: 0.3654071939618964\n",
            "Epoch: 21, Train loss: 0.351982328518916, Val loss: 0.3727959002319135\n",
            "Epoch: 22, Train loss: 0.35475810586425005, Val loss: 0.36349749388663394\n",
            "Epoch: 23, Train loss: 0.3499989915362085, Val loss: 0.3735440832219626\n",
            "Epoch: 24, Train loss: 0.3511484715197983, Val loss: 0.3704671165660808\n",
            "Epoch: 25, Train loss: 0.35018136431474045, Val loss: 0.3600645996630192\n",
            "Epoch: 26, Train loss: 0.34633480387029375, Val loss: 0.36996579209440633\n",
            "Epoch: 27, Train loss: 0.3458495615545849, Val loss: 0.35776608084377487\n",
            "Epoch: 28, Train loss: 0.33957046160212717, Val loss: 0.35834003592792313\n",
            "Epoch: 29, Train loss: 0.3398287648927516, Val loss: 0.3659098746353074\n",
            "Epoch: 30, Train loss: 0.33672854946101793, Val loss: 0.35672627487464953\n",
            "Epoch: 31, Train loss: 0.3357823210447622, Val loss: 0.3648370833773362\n",
            "Epoch: 32, Train loss: 0.33512537421231103, Val loss: 0.3664695073507334\n",
            "Epoch: 33, Train loss: 0.33316676285743835, Val loss: 0.3778008228461993\n",
            "Epoch: 34, Train loss: 0.33127724221580906, Val loss: 0.37067543793665736\n",
            "Epoch: 35, Train loss: 0.3308667287207574, Val loss: 0.36620378337408366\n",
            "Epoch: 36, Train loss: 0.32698325354011676, Val loss: 0.36757212367496994\n",
            "Epoch: 37, Train loss: 0.3325037566815949, Val loss: 0.38322470160691363\n",
            "Epoch: 38, Train loss: 0.33235181138179576, Val loss: 0.3720909543335438\n",
            "Epoch: 39, Train loss: 0.3313100551736098, Val loss: 0.3747917333323705\n",
            "Epoch: 40, Train loss: 0.3248077142530673, Val loss: 0.3666201482870077\n",
            "Epoch: 41, Train loss: 0.32229434295781345, Val loss: 0.3692202485705677\n",
            "Epoch: 42, Train loss: 0.322683897155494, Val loss: 0.3770795578235074\n",
            "Epoch: 43, Train loss: 0.3224689912518381, Val loss: 0.3715927359697066\n",
            "Epoch: 44, Train loss: 0.3179898770538352, Val loss: 0.38246698657933037\n",
            "Epoch: 45, Train loss: 0.32291015881165963, Val loss: 0.37723851576447487\n",
            "Epoch: 46, Train loss: 0.3240697030691371, Val loss: 0.37705817328471886\n",
            "Epoch: 47, Train loss: 0.3187611485899228, Val loss: 0.3783005799509977\n",
            "Epoch: 48, Train loss: 0.31489595871583603, Val loss: 0.37894579532899353\n",
            "Epoch: 49, Train loss: 0.31217636876498817, Val loss: 0.375784397321312\n",
            "Epoch: 50, Train loss: 0.31258628065470934, Val loss: 0.38606100901961327\n",
            "Epoch: 51, Train loss: 0.311474809598265, Val loss: 0.39369724514453036\n",
            "Epoch: 52, Train loss: 0.31034056710442265, Val loss: 0.381389711836451\n",
            "Epoch: 53, Train loss: 0.3087484487230423, Val loss: 0.3889246896693581\n",
            "Epoch: 54, Train loss: 0.3035000522103614, Val loss: 0.38591632893995237\n",
            "Epoch: 55, Train loss: 0.30194833079756017, Val loss: 0.39690429442807246\n",
            "Epoch: 56, Train loss: 0.3032028751414546, Val loss: 0.38092268199512835\n",
            "Epoch: 57, Train loss: 0.29841084170979765, Val loss: 0.3854085634413518\n",
            "Epoch: 58, Train loss: 0.3008451997419832, Val loss: 0.3903502572916056\n",
            "Epoch: 59, Train loss: 0.29380045187886517, Val loss: 0.38341284798164116\n",
            "Epoch: 60, Train loss: 0.29623300996590496, Val loss: 0.39322723546310473\n",
            "Epoch: 61, Train loss: 0.29183427902354714, Val loss: 0.39775313966368375\n",
            "Epoch: 62, Train loss: 0.2915067758770088, Val loss: 0.39289714690101774\n",
            "Epoch: 63, Train loss: 0.2920900573767364, Val loss: 0.4159725933874908\n",
            "Epoch: 64, Train loss: 0.2925914568870662, Val loss: 0.3894804878846595\n",
            "Epoch: 65, Train loss: 0.2886437547394304, Val loss: 0.40702945327288226\n",
            "Epoch: 66, Train loss: 0.28847713238363903, Val loss: 0.42073306029564456\n",
            "Epoch: 67, Train loss: 0.2865569052901094, Val loss: 0.4077034084812591\n",
            "Epoch: 68, Train loss: 0.29296452841687154, Val loss: 0.42691299691796303\n",
            "Epoch: 69, Train loss: 0.2911966516369079, Val loss: 0.4249761853563158\n",
            "Epoch: 70, Train loss: 0.2899683927595445, Val loss: 0.4247160740196705\n",
            "Epoch: 71, Train loss: 0.28180636325713787, Val loss: 0.41075938743980306\n",
            "Epoch: 72, Train loss: 0.2764998626968413, Val loss: 0.41072995784251315\n",
            "Epoch: 73, Train loss: 0.2762317193502427, Val loss: 0.42158144575200585\n",
            "Epoch: 74, Train loss: 0.2746936352968065, Val loss: 0.44170410244872693\n",
            "Epoch: 75, Train loss: 0.276396579182775, Val loss: 0.42521470883174944\n",
            "Epoch: 76, Train loss: 0.2729391210339736, Val loss: 0.42813831999113683\n",
            "Epoch: 77, Train loss: 0.2694659430548347, Val loss: 0.4299578109854146\n",
            "Epoch: 78, Train loss: 0.27294313959585154, Val loss: 0.4364161044359207\n",
            "Epoch: 79, Train loss: 0.2706820451252049, Val loss: 0.45299406526120084\n",
            "Epoch: 80, Train loss: 0.26745887122403217, Val loss: 0.4412576999319227\n",
            "Epoch: 81, Train loss: 0.2628791638167936, Val loss: 0.4473021749995257\n",
            "Epoch: 82, Train loss: 0.2631848129210562, Val loss: 0.4508393502941257\n",
            "Epoch: 83, Train loss: 0.25770028869462036, Val loss: 0.463895966740031\n",
            "Epoch: 84, Train loss: 0.27315727920389804, Val loss: 0.46365678996631976\n",
            "Epoch: 85, Train loss: 0.2723923802770692, Val loss: 0.47038689794901173\n",
            "Epoch: 86, Train loss: 0.27120069343896686, Val loss: 0.4664683806660928\n",
            "Epoch: 87, Train loss: 0.2583852310136358, Val loss: 0.4733238131984284\n",
            "Epoch: 88, Train loss: 0.252762295042097, Val loss: 0.4773181406290908\n",
            "Epoch: 89, Train loss: 0.2507393667895182, Val loss: 0.46549540777739723\n",
            "Epoch: 90, Train loss: 0.2537862462688919, Val loss: 0.4940846689829701\n",
            "Epoch: 91, Train loss: 0.25837379054251847, Val loss: 0.46603059709856387\n",
            "Epoch: 92, Train loss: 0.25408166630122314, Val loss: 0.4852884723559806\n",
            "Epoch: 93, Train loss: 0.25134838019103684, Val loss: 0.4809782854036281\n",
            "Epoch: 94, Train loss: 0.27717988399410737, Val loss: 0.48595515913084936\n",
            "Epoch: 95, Train loss: 0.2608500354927215, Val loss: 0.47829484174910347\n",
            "Epoch: 96, Train loss: 0.2465348583131543, Val loss: 0.48660472309903097\n",
            "Epoch: 97, Train loss: 0.24152958168708838, Val loss: 0.495094002077454\n",
            "Epoch: 98, Train loss: 0.23894631817521853, Val loss: 0.4998186803177783\n",
            "Epoch: 99, Train loss: 0.23510798457802967, Val loss: 0.5068905345703426\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXw0R-Tkucgw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_recalls(targets, predictions):\n",
        "  n_classes = 2\n",
        "  confusion_matrix, _, _ = np.histogram2d(targets.detach().cpu().numpy(), predictions.squeeze().detach().cpu().numpy(), bins=n_classes)\n",
        "  return np.diag(confusion_matrix) / (np.sum(confusion_matrix, axis=1))\n",
        "\n",
        "def gmean(classifier, dataloader):\n",
        "  with torch.no_grad():\n",
        "    classifier.eval()\n",
        "    recalls = np.zeros((2))\n",
        "    for inputs, targets in dataloader:\n",
        "      if torch.cuda.is_available():\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "      \n",
        "      outputs = classifier(inputs.float())\n",
        "      predictions = torch.round(outputs).int()\n",
        "      recalls += calculate_recalls(targets, predictions)\n",
        "\n",
        "    recalls = recalls / len(dataloader)\n",
        "    return mstats.gmean(recalls)    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlrNw0bEqAeh",
        "colab_type": "code",
        "outputId": "7b637c73-3985-47ec-8c06-47fe54901cd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "classifier = load_classifier()\n",
        "if torch.cuda.is_available():\n",
        "  classifier = classifier.cuda()\n",
        "train_accuracy = gmean(classifier, train_dataloader)\n",
        "test_accuracy = gmean(classifier, test_dataloader)\n",
        "print('Epochs: {}'.format(classifier.epochs))\n",
        "print('Train g-mean: {}'.format(train_accuracy.item()))\n",
        "print('Test g-mean: {}'.format(test_accuracy.item()))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epochs: 30\n",
            "Train g-mean: 0.63715097011243\n",
            "Test g-mean: 0.6254205116801312\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "918KM5F07hFl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}